<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Centos7使用gnome美化主题]]></title>
    <url>%2F2018%2F10%2F21%2FCentos7%E4%BD%BF%E7%94%A8gnome%E7%BE%8E%E5%8C%96%E4%B8%BB%E9%A2%98%2F</url>
    <content type="text"><![CDATA[先上效果图 1.安装必要工具、依赖 主题配置工具1sudo apt install gnome-tweak-tool 依赖1sudo apt install chrome-gnome-shell 2.浏览器安装扩展 浏览器打开https://extensions.gnome.org/ 3.安装User Themes 更多请点击这里 4.创建主题和图标文件夹 12mkdir ~/.themesmkdir ~/.icons 5.下载Github资源地址 只需关注thmes和icons两个文件中的压缩包即可 资源下载。也可以在gnome-look.org 自行提取原始资源地址 6.开始美化，放置资源 配置应用主题 配置gnome桌面效果 themes文件夹中的压缩包Sierra-light-solid.tar.xz打开后包含gnome-shell和gtk3，即gnome桌面效果与应用效果。 12tar -xf Sierra-light-solid.tar.xzmv Sierra-light-solid ~/.themes MacOSX-icon-theme.tar.xz 配置图标效果 配置光标（鼠标）效果 icons文件夹中的MacOSX-cursors.tar.xz和MacOSX-icon-theme.tar.xz解压，放置在~/.icons中 123456tar -xf MacOSX-icon-theme.tar.xzmv MacOSX ~/.iconsmv MacOSX-dark ~/.icons tar -xf MacOSX-cursors.tar.xz.xzmv capitaine-cursors ~/.icons 另外也可以配置字体，字体文件解压放置在~/.local/share/fonts/ 字体不建议配置，配置不好影响正常内容显示 7.放置好资源后，配置tweaks如下图。 8.安装其余扩展插件 我推荐几款我用的Dash to Dock：底部任务栏GPaste：记录历史复制黏贴记录OpenWeather：天气插件…更多就去插件中心自己找去吧。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Centos</tag>
        <tag>gnome</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Centos7安装Google Chrome]]></title>
    <url>%2F2018%2F10%2F21%2FCentos7%E5%AE%89%E8%A3%85Google%20Chrome%2F</url>
    <content type="text"><![CDATA[1.首先进入根目录，然后进入etc/yum.repos.d目录下，创建google-chrome.repo文件123cd /cd etc/yum.repos.dvi google-chrome.repo 2.在文件中添加：123456[google-chrome] name=google-chrome baseurl=http://dl.google.com/linux/chrome/rpm/stable/$basearch enabled=1gpgcheck=1 gpgkey=https://dl-ssl.google.com/linux/linux_signing_key.pub ESC 退出到命令模式，shift+q ， x保存退出 3.加入谷歌的源之后键入：1yum -y install google-chrome-stable --nogpgcheck 4.安装完成。在应用程序的互联网中就可以找到谷歌浏览器了。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Centos</tag>
        <tag>Google Chrome</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Centos7安装jq]]></title>
    <url>%2F2018%2F10%2F21%2FCentos7%E5%AE%89%E8%A3%85jq%2F</url>
    <content type="text"><![CDATA[1234wget http://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpmrpm -ivh epel-release-latest-7.noarch.rpmyum repolistyum install jq]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Centos</tag>
        <tag>jq</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[10张图带你了解后台服务架构演变]]></title>
    <url>%2F2018%2F10%2F16%2F10%E5%BC%A0%E5%9B%BE%E5%B8%A6%E4%BD%A0%E4%BA%86%E8%A7%A3%E5%90%8E%E5%8F%B0%E6%9C%8D%E5%8A%A1%E6%9E%B6%E6%9E%84%E6%BC%94%E5%8F%98%2F</url>
    <content type="text"><![CDATA[上篇文章讲了一些高并发相关的知识，相信大家对高并发有些简单的认识。说到高并发，往往离不开分布式系统。人们经常将两者拿来一起讨论，因为高并发（High Concurrency）是互联网分布式系统架构设计中必须考虑的因素之一。可以这么说，目前应用商城上很多社交应用，网络游戏的后台服务都是分布式服务。那具体什么催生出今天的分布式系统呢？文章的主要内容是讲讲大型网站的服务架构演变。 1.初始阶段的网站架构在互联网展露出萌芽的网络时代，网站基本都是小型网站。网站的访客也不是很多，通常会将应用程序、数据库、文件等所有资源都在一台服务器上。这里为 Java Web 服务为例。网站开发者可以使用 Tomcat 等 Web 容器直接运行 JSP 程序，然后将数据存储到数据库，文件直接存放到服务器的磁盘中。就像这样子： 2.应用服务和数据分离随着网站业务的发展和用户量的增加，一台服务器就无法再满足需求了。大量用户访问导致访问速度越来越慢，而逐渐增加的数据也会导致存储空间不足。这时需要将 Web 应用和数据分离，分别将存放到不同的服务器：应用服务器、文件服务器和数据库服务器。这样不仅提高了单台机器的负载能力，也提高了容灾能力。 3.使用缓存改善网站性能随着用户再增加，网站又会一次面临挑战：数据库压力太大导致整站访问效率再此下降，用户体验受到影响。 一个网站往往 80% 的业务访问集中在 20% 的数据上。那么将这一小部分频繁读取的数据先提前缓存在内存中，而不是每次都去数据库读取。这样就可以减少数据库的访问压力，从而提高整个网站的访问速度。 缓存分为本地缓存和分布式缓存服务器，前者更快但容量有限，后者理论上容量可以无限伸缩。 4.使用集群改善并发处理能力使用缓存后，数据访问压力得到了缓解.但是单一应用服务器能够处理的请求连接有限，在网站访问高峰期，应用服务器就成了整个网站的效率瓶颈。因此使用负载均衡处理器势在必然。通过负载均衡调度服务器，可将来自浏览器的访问请求分发到应用的集群中的任何一台服务器上。使用服务器集群也有个好处，Web 应用程序更新可以做到用户无感知。 大部分应用使用软件来实现负载均衡。常见的软件有 Nginx 等。 5.数据库读写分离当用户达到一定规模后，数据库因为负载压力过高而成为网站的瓶颈。虽然前面使用缓存能满足查询的需求，但是大部分数据操作还是需要通过数据库来完成。而目前主流的数据库都提供主从热备功能，通过配置两台数据库主从关系，可以将一台数据库的数据更新同步到另一台服务器上。网站利用数据库这一功能实现数据库读写分离，从而改善数据库负载压力。 应用服务器在写数据的时候，访问主数据库，主数据库通过主从复制机制将数据更新同步到从数据库，这样当应用服务器读数据的时候，就可以通过从数据库获得数据。为了便于应用程序访问读写分离后的数据库，通常在应用服务器端使用专门的数据访问模块，使数据库读写分离对应用透明。 6.反向代理和 CDN 加速随着网站名气越多越大，用户规模越来越大，网站业务也随着继续壮大。为了满足不同地区的用户快速访问网站的需求，需要提高网站的访问速度。主要手段有使用 CDN 和反向代理。 同时 Ajax 技术的出现，Web 应用会将数据（内容和图片）和页面框架（指 HTML 文件以及其中的标签）。页面框架内容存放到 CDN 服务器上，数据存放到数据库服务器上。当用户使用浏览器访问网站，会显示页面框架，然后页面框架发起 HTTP 请求加载数据。 而反向代理是部署在网站的中心机房，当用户请求到达中心机房后，首先访问的反向代理，如果反向代理缓存着用户请求的资源，则直接返回给用户。 因此，CDN 和反向代理的基本原理都是缓存。 7.使用分布式文件系统和分布式数据库系统任何强大的单一服务器都满足不了大型网站持续增长的业务需求。 分布式数据库时网站数据库拆分的最后手段，只用在单表数据规模非常大的时候才使用。不到不得已时，网站更常用的数据库拆分手段是业务拆分，将不同业务的数据部署在不同的物理服务器上。 8.使用NoSQL和搜索引擎随着网站业务越来越复杂，对数据存储和检索的需求也越来越复杂。网站需要采用一些非关系数据库技术如 NoSQL 数据库和非数据库查询技术如搜索引擎。而常见的 NoSQL 数据库有 Mongodb、HBase等。 9.业务拆分大型网站为了应对日益复杂的业务场景，通过使用分而治之的手段将真个网站业务拆分成不同的产品线。如大型购物交易网站都会将首页、商铺、订单、买家、卖家等拆分成不同的产品线，分归不同的业务团队负责。 10.啥分布式服务随着业务拆分越来越小，存储系统越来越庞大，应用系统的整体复杂度呈指数级增加，部署维护越来越困难。 既然每一个应用系统都需要执行许多相同的业务操作，比如用户管理、商品管理等，那么可以将这些共用的业务提取出来，独立部署。由这些可复用的业务连接数据库，提供共用业务服务，而应用系统只需要管理用户界面，通过分布式服务调用共用业务服务完成具体业务操作。 大型网站的架构演化到这里，基本上大多数的技术问题都可以得以解决了。 本文本分内容以及图片参考书籍《大型网站技术架构：核心原理与案例分析》 作者: 李智慧。]]></content>
      <categories>
        <category>架构设计</category>
      </categories>
      <tags>
        <tag>架构设计</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Idea开发环境中搭建Maven并且使用Maven打包部署程序]]></title>
    <url>%2F2018%2F10%2F16%2FIdea%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E4%B8%AD%E6%90%AD%E5%BB%BAMaven%E5%B9%B6%E4%B8%94%E4%BD%BF%E7%94%A8Maven%E6%89%93%E5%8C%85%E9%83%A8%E7%BD%B2%E7%A8%8B%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[作者：怪才来源：www.cnblogs.com/hanyinglong 1.配置Maven的环境变量a.首先我们去maven官网下载Maven程序，解压到安装目录，如图所示: b.配置M2_HOME的环境变量，然后将该变量添加到Path中 备注：必须要有JAVA_HOME的M2_HOME环境变量，不然Maven会提示错误。配置环境变量如图所示： c.如果想要修改Maven的本地仓库位置，则可以直接在Maven的安装目录下找到conf文件下的setting配置文件中，设置localRepository为本地仓库位置&lt;localRepository&gt;E:\java\repo&lt;/localRepository&gt; d.重新打开命令提示符cmd(管理员)，输入mvn –version ，如图所示，则说明安装成功 2.Idea开发环境中搭建Mavena.当配置完Maven之后，我们需要给Idea配置Maven，那么首先必须先要安装Idea，Idea的安装在这里就不累赘了，请自行百度，非常简单，下一步下一步即可，安装完成之后打开Idea设置Maven，如图所示： b.单击Setting之后，设置Maven节点下的Maven home directory和user settings file和local repository 如图所示： 到这里我们整个Idea配置Maven就完成了，下面来说使用Maven开发JavaWeb项目以及使用Maven打包。 3.使用Maven开发JavaWeb项目(Idea14)a.通过上面的步骤我们便给Idea配置好了Maven环境，那么这时候我们更愿意创建Maven管理的Java Web项目，如何创建呢？ b.单击File-&gt;New Project-&gt;选择Maven,如图所示：选中Createfrom archetype，选择maven-archetype-webapp c.单击Next，如图所示：填写GroupId和ArtifactId和Version d.单击Next，如图所示：此页面获取的是maven的安装信息 e.单击Next，如图所示：填写项目名称和项目存放的路劲 f.单击 Finish完成，即创建Maven项目成功，如图所示： g.如果单击完成在下面的提示中报错，出现问题的可能性是Maven和Idea的兼容性问题，建议将Maven换成低版本的即可。报错如图所示： 备注：当改变Maven版本的时候，必须改变环境变量和Idea中的设置才可以。 4.使用Maven打包JavaWeb项目a.通过以上步骤即安装了Maven和开发了一个Maven的JavaWeb项目，那么接下来就需要将JavaWeb打包(war文件)发布到Tomcat下，如何打包呢？ b.在Idea中的最右边的导航栏中可以看到一个Maven Projects，单击打开，如图所示： 图一 图二 c.接下来我们就能够利用这个简单的工具对Maven进行打包(war)。 d.如图2所示，当单击Run Maven Build的时候，出现错误，如图所示： e.出现以上错误之后，按照下面的步骤解决，单击File-&gt;Setting-&gt;在搜索框中输入Maven回车直接定位到Maven节点-&gt;Runner,打开之后将这段内容 -Dmaven.multiModuleProjectDirectory=$M2_HOME） 复制到VM Options的文本框中，单击OK即可。 f.按照图2所示，我们再次运行，发现不会报错，并且输入了一些内容，证明已可以打包程序。 g.选择install右键选择Run运行即可，运行完成之后则会提示你打包的war包在哪里，如图所示 h.然后找到war包,复制到Tomcat的WebApps文件夹下面，然后直接访问网站即可访问。]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>Idea</tag>
        <tag>maven</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[拜托，面试别再问我时间复杂度了！！！]]></title>
    <url>%2F2018%2F10%2F11%2F%E6%8B%9C%E6%89%98%EF%BC%8C%E9%9D%A2%E8%AF%95%E5%88%AB%E5%86%8D%E9%97%AE%E6%88%91%E6%97%B6%E9%97%B4%E5%A4%8D%E6%9D%82%E5%BA%A6%E4%BA%86%EF%BC%81%EF%BC%81%EF%BC%81%2F</url>
    <content type="text"><![CDATA[此文摘自微信公众号【架构师之路】 微信扫一扫关注该公众号 快速排序分为这么几步： 第一步，先做一次partition； partition使用第一个元素t=arr[low]为哨兵，把数组分成了两个半区： 左半区比t大 右半区比t小 第二步，左半区递归； 第三步，右半区递归； 伪代码为： 123456void quick_sort(int[]arr, int low, int high)&#123; if(low== high) return; int i = partition(arr, low, high); quick_sort(arr, low, i-1); quick_sort(arr, i+1, high);&#125; 为啥，快速排序，时间复杂度是O(n*lg(n))呢？ 今天和大家聊聊时间复杂度。 画外音：往下看，第三类方法很牛逼。 第一大类，简单规则 为方便记忆，先总结几条简单规则，热热身。 规则一：“有限次操作”的时间复杂度往往是O(1)。 例子：交换两个数a和b的值。 123456789void swap(int&amp; a, int&amp; b)&#123; int t=a; a=b; b=t;&#125; 分析：通过了一个中间变量t，进行了3次操作，交换了a和b的值，swap的时间复杂度是O(1)。 画外音：这里的有限次操作，是指不随数据量的增加，操作次数增加。 规则二：“for循环”的时间复杂度往往是O(n)。 例子：n个数中找到最大值。 max(int[] arr, int n)&#123;12345678910 int temp = -MAX; for(int i=0;i&lt;n;++i) if(arr[i]&gt;temp) temp=arr[i]; return temp;&#125; 分析：通过一个for循环，将数据集遍历，每次遍历，都只执行“有限次操作”，计算的总次数，和输入数据量n呈线性关系。 规则三：“树的高度”的时间复杂度往往是O(lg(n))。 分析：树的总节点个数是n，则树的高度是lg(n)。 在一棵包含n个元素二分查找树上进行二分查找，其时间复杂度是O(lg(n))。 对一个包含n个元素的堆顶元素弹出后，调整成一个新的堆，其时间复杂度也是O(lg(n))。 第二大类：组合规则 通过简单规则的时间复杂度，来求解组合规则的时间复杂度。 例如：n个数冒泡排序。 1234567891011void bubble_sort(int[] arr, int n)&#123; for(int i=0;i&lt;n;i++) for(int j=0;j&lt;n-i-1;j++) if(arr[j]&gt;arr[j+1]) swap(arr[j], arr[j+1]);&#125; 分析：冒泡排序，可以看成三个规则的组合： 1. 外层for循环 2. 内层for循环 3. 最内层的swap 故，冒泡排序的时间复杂度为： O(n) O(n) O(1) = O(n^2) 又例如：TopK问题，通过建立k元素的堆，来从n个数中求解最大的k个数。 先用前k个元素生成一个小顶堆，这个小顶堆用于存储，当前最大的k个元素。 接着，从第k+1个元素开始扫描，和堆顶（堆中最小的元素）比较，如果被扫描的元素大于堆顶，则替换堆顶的元素，并调整堆，以保证堆内的k个元素，总是当前最大的k个元素。 直到，扫描完所有n-k个元素，最终堆中的k个元素，就是为所求的TopK。 伪代码： 123456789heap[k] = make_heap(arr[1, k]);for(i=k+1 to n)&#123; adjust_heap(heep[k],arr[i]);&#125;return heap[k]; 分析：可以看成三个规则的组合： 1. 新建堆 2. for循环 3. 调整堆 故，用堆求解TopK，时间复杂度为： O(k) + O(n) O(lg(k)) = O(nlg(k)) 画外音：注意哪些地方用加，哪些地方用乘；哪些地方是n，哪些地方是k。 第三大类，递归求解 简单规则和组合规则可以用来求解非递归的算法的时间复杂度。对于递归的算法，该怎么分析呢？ 接下来，通过几个案例，来说明如何通分析递归式，来分析递归算法的时间复杂度。 案例一：计算 1到n的和，时间复杂度分析。 如果用非递归的算法： 1234567891011int sum(int n)&#123; int result=0; for(int i=0;i&lt;n;i++) result += i; return result;&#125; 根据简单规则，for循环，sum的时间复杂度是O(n)。 但如果是递归算法，就没有这么直观了： 1234567int sum(int n)&#123; if (n==1) return 1; return n+sum(n-1);&#125; 如何来进行时间复杂度分析呢？ 用f(n)来表示数据量为n时，算法的计算次数，很容易知道： 当n=1时，sum函数只计算1次 画外音：if (n==1) return 1; 即： f(1)=1【式子A】 不难发现，当n不等于1时： f(n)的计算次数，等于f(n-1)的计算次数，再加1次计算 画外音：return n+sum(n-1); 即： f(n)=f(n-1)+1【式子B】 【式子B】不断的展开，再配合【式子A】： 画外音：这一句话，是分析这个算法的关键。 f(n)=f(n-1)+1 f(n-1)=f(n-2)+1 … f(2)=f(1)+1 f(1)=1 上面共n个等式，左侧和右侧分别相加： f(n)+f(n-1)+…+f(2)+f(1) = [f(n-1)+1]+[f(n-2)+1]+…+[f(1)+1]+[1] 即得到： f(n)=n 已经有那么点意思了哈，再来个复杂点的算法。 案例二：二分查找binary_search，时间复杂度分析。 1234567891011121314151617int BS(int[] arr, int low, int high, int target)&#123; if (low&gt;high) return -1; mid = (low+high)/2; if (arr[mid]== target) return mid; if (arr[mid]&gt; target) return BS(arr, low, mid-1, target); else return BS(arr, mid+1, high, target);&#125; 二分查找，单纯从递归算法来分析，怎能知道其时间复杂度是O(lg(n))呢？ 仍用f(n)来表示数据量为n时，算法的计算次数，很容易知道： 当n=1时，bs函数只计算1次 画外音：不用纠结是1次还是1.5次，还是2.7次，是一个常数次。 即： f(1)=1【式子A】 在n很大时，二分会进行一次比较，然后进行左侧或者右侧的递归，以减少一半的数据量： f(n)的计算次数，等于f(n/2)的计算次数，再加1次计算 画外音：计算arr[mid]&gt;target，再减少一半数据量迭代 即： f(n)=f(n/2)+1【式子B】 【式子B】不断的展开， f(n)=f(n/2)+1 f(n/2)=f(n/4)+1 f(n/4)=f(n/8)+1 … f(n/2^(m-1))=f(n/2^m)+1 上面共m个等式，左侧和右侧分别相加： f(n)+f(n/2)+…+f(n/2^(m-1))=[f(n/2)+1]+[f(n/4)+1]+…+[f(n/2^m)]+[1] 即得到： f(n)=f(n/2^m)+m 再配合【式子A】： f(1)=1 即，n/2^m=1时, f(n/2^m)=1, 此时m=lg(n), 这一步，这是分析这个算法的关键。 将m=lg(n)带入，得到： f(n)=1+lg(n) 神奇不神奇？ 最后，大boss，快速排序递归算法，时间复杂度的分析过程。 案例三：快速排序quick_sort，时间复杂度分析。 1234567891011void quick_sort(int[]arr, int low, inthigh)&#123; if (low==high) return; int i = partition(arr, low, high); quick_sort(arr, low, i-1); quick_sort(arr, i+1, high);&#125; 仍用f(n)来表示数据量为n时，算法的计算次数，很容易知道： 当n=1时，quick_sort函数只计算1次 f(1)=1【式子A】 在n很大时： 第一步，先做一次partition； 第二步，左半区递归； 第三步，右半区递归； 即： f(n)=n+f(n/2)+f(n/2)=n+2*f(n/2)【式子B】 画外音： (1)partition本质是一个for，计算次数是n； (2)二分查找只需要递归一个半区，而快速排序左半区和右半区都要递归，这一点在分治法与减治法一章节已经详细讲述过； 【式子B】不断的展开， f(n)=n+2*f(n/2) f(n/2)=n/2+2*f(n/4) f(n/4)=n/4+2*f(n/8) … f(n/2^(m-1))=n/2^(m-1)+2f(n/2^m) 上面共m个等式，逐步带入，于是得到： f(n)=n+2*f(n/2) =n+2[n/2+2\f(n/4)]=2n+4*f(n/4) =2n+4[n/4+2f(n/8)]=3n+8f(n/8) =… =m*n+2^m*f(n/2^m) 再配合【式子A】： f(1)=1 即，n/2^m=1时, f(n/2^m)=1, 此时m=lg(n), 这一步，这是分析这个算法的关键。 将m=lg(n)带入，得到： f(n)=lg(n)*n+2^(lg(n))*f(1)=n*lg(n)+n 故，快速排序的时间复杂度是n*lg(n)。 wacalei，有点意思哈！ 画外音：额，估计83%的同学没有细究看，花5分钟细思上述过程，一定有收获。 总结 for循环的时间复杂度往往是O(n) 树的高度的时间复杂度往往是O(lg(n)) 二分查找的时间复杂度是O(lg(n))，快速排序的时间复杂度n*(lg(n)) 递归求解，未来再问时间复杂度，通杀 知其然，知其所以然。 思路比结论重要。]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>时间复杂度</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何对JPA或者MyBatis进行技术选型]]></title>
    <url>%2F2018%2F10%2F10%2F%E5%A6%82%E4%BD%95%E5%AF%B9JPA%E6%88%96%E8%80%85MyBatis%E8%BF%9B%E8%A1%8C%E6%8A%80%E6%9C%AF%E9%80%89%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[作者：springforall来源：http://www.spring4all.com/common/user/532 在我们平时的项目中，大家都知道可以使用 JPA 或者 Mybatis 作为 ORM 层。对 JPA 和 Mybatis 如何进行技术选型？ 下面看看大精华总结如下： 最佳回答首先表达个人观点，JPA必然是首选的。 个人认为仅仅讨论两者使用起来有何区别，何者更加方便，不足以真正的比较这两个框架。要评判出更加优秀的方案，我觉得可以从软件设计的角度来评判。个人对 mybatis 并不熟悉，但 JPA 规范和 springdata 的实现，设计理念绝对是超前的。软件开发复杂性的一个解决手段是遵循 DDD（DDD 只是一种手段，但不是唯一手段），而我着重几点来聊聊 JPA 的设计中是如何体现领域驱动设计思想的，抛砖引玉。 聚合根和值对象领域驱动设计中有两个广为大家熟知的概念，entity（实体）和 value object（值对象）。entity 的特点是具有生命周期的，有标识的，而值对象是起到一个修饰的作用，其具有不可变性，无标识。在 JPA中 ，需要为数据库的实体类添加 @Entity 注解，相信大家也注意到了，这并不是巧合。 1234567891011@Entity@Table(name = "t_order")public class Order &#123; @Id private String oid; @Embedded private CustomerVo customer; @OneToMany(cascade = &#123;CascadeType.ALL&#125;, orphanRemoval = true, fetch = FetchType.LAZY, mappedBy = "order") private List&lt;OrderItem&gt; orderItems;&#125; 如上述的代码，Order 便是 DDD 中的实体，而 CustomerVo，OrderItem 则是值对象。程序设计者无需关心数据库如何映射这些字段，因为在 DDD 中，需要做的工作是领域建模，而不是数据建模。实体和值对象的意义不在此展开讨论，但通过此可以初见端倪，JPA 的内涵绝不仅仅是一个 ORM 框架。 仓储Repository 模式是领域驱动设计中另一个经典的模式。在早期，我们常常将数据访问层命名为：DAO，而在 SpringData JPA 中，其称之为 Repository（仓储），这也不是巧合，而是设计者有意为之。 熟悉 SpringData JPA 的朋友都知道当一个接口继承 JpaRepository 接口之后便自动具备了 一系列常用的数据操作方法，findAll， findOne ，save等。 12public interface OrderRepository extends JpaRepository&lt;Order, String&gt;&#123;&#125; 那么仓储和DAO到底有什么区别呢？这就要提到一些遗留问题，以及一些软件设计方面的因素。在这次SpringForAll 的议题中我能够预想到有很多会强调 SpringData JPA 具有方便可扩展的 API，像下面这样： 123456789public interface OrderRepository extends JpaRepository&lt;Order, String&gt;&#123; findByOrderNoAndXxxx(String orderNo,Xxx xx); @Transactional @Modifying(clearAutomatically = true) @Query("update t_order set order_status =?1 where id=?2") int updateOrderStatusById(String orderStatus, String id);&#125; 但我要强调的是，这是 SpringData JPA 的妥协，其支持这一特性，并不代表其建议使用。因为这并不符合领域驱动设计的理念。注意对比，SpringData JPA 的设计理念是将 Repository 作为数据仓库，而不是一系列数据库脚本的集合，findByOrderNoAndXxxx 方法可以由下面一节要提到的JpaSpecificationExecutor代替，而 updateOrderStatusById 方法则可以由 findOne + save 代替，不要觉得这变得复杂了，试想一下真正的业务场景，修改操作一般不会只涉及一个字段的修改， findOne + save 可以帮助你完成更加复杂业务操作，而不必关心我们该如何编写 SQL 语句，真正做到了面向领域开发，而不是面向数据库 SQL 开发，面向对象的拥趸者也必然会觉得，这更加的 OO。 Specification上面提到 SpringData JPA 可以借助 Specification 模式代替复杂的 findByOrderNoAndXxxx 一类 SQL 脚本的查询。试想一下，业务不停在变，你怎么知道将来的查询会不会多一个条件 变成 findByOrderNoAndXxxxAndXxxxAndXxxx.... 。SpringData JPA 为了实现领域驱动设计中的 Specification 模式，提供了一些列的 Specification 接口，其中最常用的便是 ：JpaSpecificationExecutor 12public interface OrderRepository extends JpaRepository&lt;Order,String&gt;,JpaSpecificationExecutor&lt;Order&gt;&#123;&#125; 使用 SpringData JPA 构建复杂查询（join操作，聚集操作等等）都是依赖于 JpaSpecificationExecutor 构建的 Specification 。例子就不介绍，有点长。 请注意，上述代码并不是一个例子，在真正遵循 DDD 设计规范的系统中，OrderRepository 接口中就应该是干干净净的，没有任何代码，只需要继承 JpaRepository （负责基础CRUD）以及 JpaSpecificationExecutor （负责Specification 查询）即可。当然， SpringData JPA 也提供了其他一系列的接口，根据特定业务场景继承即可。 乐观锁为了解决数据并发问题，JPA 中提供了 [@Version](https://github.com/Version &quot;@Version&quot;) ，一般在 Entity 中 添加一个 Long version 字段，配合 [@Version](https://github.com/Version &quot;@Version&quot;) 注解，SpringData JPA 也考虑到了这一点。这一点侧面体现出，JPA 设计的理念和 SpringData 作为一个工程解决方案的双剑合璧，造就出了一个伟大的设计方案。 复杂的多表查询很多人青睐 Mybatis ，原因是其提供了便利的 SQL 操作，自由度高，封装性好……SpringData JPA对复杂 SQL 的支持不好，没有实体关联的两个表要做 join ，的确要花不少功夫。但 SpringData JPA 并不把这个当做一个问题。为什么？因为现代微服务的架构，各个服务之间的数据库是隔离的，跨越很多张表的 join 操作本就不应该交给单一的业务数据库去完成。解决方案是：使用 elasticSearch做视图查询 或者 mongodb 一类的Nosql 去完成。问题本不是问题。 总结真正走进 JPA，真正走进 SpringData 会发现，我们并不是在解决一个数据库查询问题，并不是在使用一个 ORM 框架，而是真正地在实践领域驱动设计。 （再次补充：DDD 只是一种手段，但不是唯一手段） 第二名回答lexburne 兄说的也很不错了，不过我还想在补充2点，来消除大家对使用spring data jpa 的误解spring data jpa 的好处我相信大家都了解，就是开发速度很快，很方便，大家不愿意使用spring data jpa 的地方通常是因为sql 不是自己写的，不可控，复杂查询不好搞，那么下面我要说的就是其实对于这种需求，spring data jpa 是完全支持的！！ 第一种方式:@query 注解指定nativeQuery,这样就可以使用原生sql查询了,示例代码来自官方文档: 12345public interface UserRepository extends JpaRepository&lt;User, Long&gt; &#123;@Query(value = "SELECT * FROM USERS WHERE EMAIL_ADDRESS = ?1", nativeQuery = true)User findByEmailAddress(String emailAddress);&#125; 如果单靠sql搞不定怎么办？必须要写逻辑怎么办?可以使用官方文档3.6.1 节：Customizing individual repositories 提供的功能去实现，先看官方文档的代码: 12345678910111213interface CustomizedUserRepository &#123;void someCustomMethod(User user);&#125;class CustomizedUserRepositoryImpl implements CustomizedUserRepository &#123;public void someCustomMethod(User user) &#123; // Your custom implementation&#125;&#125;interface UserRepository extends CrudRepository&lt;User, Long&gt;, CustomizedUserRepository &#123;// Declare query methods here&#125; 我来解释下上面的代码，如果搞不定的查询方法，可以自定义接口，例如CustomizedUserRepository ，和他的实现了类，然后在这个实现类里用你自己喜欢的dao 框架，比如说mybatis ,jdbcTemplate ,都随意，最后在用UserRepository 去继承CustomizedUserRepository接口，就实现了和其他dao 框架的组合使用！！ 那么下面我在总结1下，有了上面介绍的2种功能，你还在担心，使用spring data jpa 会有局限么，他只会加速你的开发速度，并允许你组合使用其他框架，只有好处，没有坏处。。最后再说1点，我最近在看es ,然后看了下spring data es 的文档，大概扫了1下，我发现，学会spring data 其中某1个系列以后，在看其他的，我发现我都不用花时间学。。直接就可以用，对就是这么神奇～～ 第三工作以来一直是使用 hibernate 和 mybatis，总结下来一般传统公司、个人开发（可能只是我）喜欢用jpa，互联网公司更青睐于 mybatis原因：，而mybatis 更加灵活。开发迭代模式决定的，传统公司需求迭代速度慢，项目改动小，hibernate可以帮他们做到一劳永逸；而互联网公司追求快速迭代，需求快速变更，灵活的 mybatis 修改起来更加方便，而且一般每一次的改动不会带来性能上的下降，hibernate经常因为添加关联关系或者开发者不了解优化导致项目越来越糟糕（本来开始也是性能很好的） 1、mybatis官方文档就说了他是一个半自动化的持久层框架，相对于全自动化的 hibernate 他更加的灵活、可控2、mybatis 的学习成本低于 hibernate。hibernate 使用需要对他有深入的理解，尤其是缓存方面，作为一个持久层框架，性能依然是第一位的。 hibernate 它有着三级缓存，一级缓存是默认开启的，二级缓存需要手动开启以及配置优化，三级缓存可以整合业界流行的缓存技术 redis，ecache 等等一起去实现hibernate 使用中的优化点：1、缓存的优化2、关联查询的懒加载（在开发中，还是不建议过多使用外键去关联操作） jpa（Java Persistence API） 与 hibernate 的关系：Jpa是一种规范，hibernate 也是遵从他的规范的。springDataJpa 是对 repository 的封装,简化了 repository 的操作 第四 数据分析型的OLAP应用适合用MyBatis，事务处理型OLTP应用适合用JPA。 越是复杂的业务，越需要领域建模，建模用JPA实现最方便灵活。但是JPA想用好，门槛比较高，不懂DDD的话，就会沦为增删改查了。 复杂的查询应该是通过CQRS模式，通过异步队列建立合适查询的视图，通过视图避免复杂的Join，而不是直接查询领域模型。 从目前的趋势来看OLAP交给NoSQL数据库可能更合适 第五使用了一段时间jpa，而mybatis是之前一直在用的，不说区别是啥，因为有很多人比较这两个框架了！从国内开源的应用框架来看，国内使用jpa做orm的人还是比较少，如果换成hibernate还会多一些，所以面临的风险可能就是你会用，和你合作的人不一定会用，如果要多方协作，肯定要考虑这个问题！灵活性方面，jpa更灵活，包括基本的增删改查、数据关系以及数据库的切换上都比mybatis灵活，但是jpa门槛较高，另外就是更新数据需要先将数据查出来才能进行更新，数据量大的时候，jpa效率会低一些，这时候需要做一些额外的工作去处理！现在结合Springboot有Springdata jpa给到，很多东西都简化了，感兴趣并且有能力可以考虑在公司内部和圈子里推广！我的博客里有一些简单使用jpa的示例，https://github.com/icnws/spring-data-jpa-demo 第六1.相对来说，jpa的学习成本比mybatis略高2.公司业务需求频繁变更导致表结构复杂，此处使用mybatis比jpa更灵活3.就方言来讲，一般公司选定数据库后再变更微乎其微，所以此处方言的优势可以忽略]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>mybatis</tag>
        <tag>jpa</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MyBatis拦截器原理探究]]></title>
    <url>%2F2018%2F10%2F08%2FMyBatis%E6%8B%A6%E6%88%AA%E5%99%A8%E5%8E%9F%E7%90%86%E6%8E%A2%E7%A9%B6%2F</url>
    <content type="text"><![CDATA[作者：丶Format来源：http://fangjian0423.github.io/ MyBatis拦截器介绍MyBatis提供了一种插件(plugin)的功能，虽然叫做插件，但其实这是拦截器功能。那么拦截器拦截MyBatis中的哪些内容呢？ 我们进入官网看一看： MyBatis 允许你在已映射语句执行过程中的某一点进行拦截调用。默认情况下，MyBatis 允许使用插件来拦截的方法调用包括： Executor (update, query, flushStatements, commit, rollback, getTransaction, close, isClosed) ParameterHandler (getParameterObject, setParameters) ResultSetHandler (handleResultSets, handleOutputParameters) StatementHandler (prepare, parameterize, batch, update, query) 我们看到了可以拦截Executor接口的部分方法，比如update，query，commit，rollback等方法，还有其他接口的一些方法等。 总体概括为： 拦截执行器的方法 拦截参数的处理 拦截结果集的处理 拦截Sql语法构建的处理 拦截器的使用拦截器介绍及配置首先我们看下MyBatis拦截器的接口定义：12345public interface Interceptor &#123; Object intercept(Invocation invocation) throws Throwable; Object plugin(Object target); void setProperties(Properties properties); &#125; 比较简单，只有3个方法。 MyBatis默认没有一个拦截器接口的实现类，开发者们可以实现符合自己需求的拦截器。 下面的MyBatis官网的一个拦截器实例：1234567891011121314@Intercepts(&#123;@Signature( type= Executor.class, method = "update", args = &#123;MappedStatement.class,Object.class&#125;)&#125;) public class ExamplePlugin implements Interceptor &#123; public Object intercept(Invocation invocation) throws Throwable &#123; return invocation.proceed(); &#125; public Object plugin(Object target) &#123; return Plugin.wrap(target, this); &#125; public void setProperties(Properties properties) &#123; &#125; &#125; 全局xml配置：123&lt;plugins&gt; &lt;plugin interceptor="org.format.mybatis.cache.interceptor.ExamplePlugin"&gt;&lt;/plugin&gt; &lt;/plugins&gt; 这个拦截器拦截Executor接口的update方法（其实也就是SqlSession的新增，删除，修改操作），所有执行executor的update方法都会被该拦截器拦截到。 源码分析下面我们分析一下这段代码背后的源码。 首先从源头-&gt;配置文件开始分析： XMLConfigBuilder解析MyBatis全局配置文件的pluginElement私有方法：1234567891011private void pluginElement(XNode parent) throws Exception &#123; if (parent != null) &#123; for (XNode child : parent.getChildren()) &#123; String interceptor = child.getStringAttribute("interceptor"); Properties properties = child.getChildrenAsProperties(); Interceptor interceptorInstance = (Interceptor) resolveClass(interceptor).newInstance(); interceptorInstance.setProperties(properties); configuration.addInterceptor(interceptorInstance); &#125; &#125;&#125; 具体的解析代码其实比较简单，就不贴了，主要就是通过反射实例化plugin节点中的interceptor属性表示的类。然后调用全局配置类Configuration的addInterceptor方法。123public void addInterceptor(Interceptor interceptor) &#123; interceptorChain.addInterceptor(interceptor); &#125; 这个interceptorChain是Configuration的内部属性，类型为InterceptorChain，也就是一个拦截器链，我们来看下它的定义：1234567891011121314151617181920public class InterceptorChain &#123; private final List&lt;Interceptor&gt; interceptors = new ArrayList&lt;Interceptor&gt;(); public Object pluginAll(Object target) &#123; for (Interceptor interceptor : interceptors) &#123; target = interceptor.plugin(target); &#125; return target; &#125; public void addInterceptor(Interceptor interceptor) &#123; interceptors.add(interceptor); &#125; public List&lt;Interceptor&gt; getInterceptors() &#123; return Collections.unmodifiableList(interceptors); &#125;&#125; 现在我们理解了拦截器配置的解析以及拦截器的归属，现在我们回过头看下为何拦截器会拦截这些方法（Executor，ParameterHandler，ResultSetHandler，StatementHandler的部分方法）：123456789101112131415161718192021222324252627282930313233343536public ParameterHandler newParameterHandler(MappedStatement mappedStatement, Object parameterObject, BoundSql boundSql) &#123; ParameterHandler parameterHandler = mappedStatement.getLang().createParameterHandler(mappedStatement, parameterObject, boundSql); parameterHandler = (ParameterHandler) interceptorChain.pluginAll(parameterHandler); return parameterHandler; &#125; public ResultSetHandler newResultSetHandler(Executor executor, MappedStatement mappedStatement, RowBounds rowBounds, ParameterHandler parameterHandler, ResultHandler resultHandler, BoundSql boundSql) &#123; ResultSetHandler resultSetHandler = new DefaultResultSetHandler(executor, mappedStatement, parameterHandler, resultHandler, boundSql, rowBounds); resultSetHandler = (ResultSetHandler) interceptorChain.pluginAll(resultSetHandler); return resultSetHandler; &#125; public StatementHandler newStatementHandler(Executor executor, MappedStatement mappedStatement, Object parameterObject, RowBounds rowBounds, ResultHandler resultHandler, BoundSql boundSql) &#123; StatementHandler statementHandler = new RoutingStatementHandler(executor, mappedStatement, parameterObject, rowBounds, resultHandler, boundSql); statementHandler = (StatementHandler) interceptorChain.pluginAll(statementHandler); return statementHandler; &#125;public Executor newExecutor(Transaction transaction, ExecutorType executorType, boolean autoCommit) &#123; executorType = executorType == null ? defaultExecutorType : executorType; executorType = executorType == null ? ExecutorType.SIMPLE : executorType; Executor executor; if (ExecutorType.BATCH == executorType) &#123; executor = new BatchExecutor(this, transaction); &#125; else if (ExecutorType.REUSE == executorType) &#123; executor = new ReuseExecutor(this, transaction); &#125; else &#123; executor = new SimpleExecutor(this, transaction); &#125; if (cacheEnabled) &#123; executor = new CachingExecutor(executor, autoCommit); &#125; executor = (Executor) interceptorChain.pluginAll(executor); return executor; &#125; 以上4个方法都是Configuration的方法。这些方法在MyBatis的一个操作(新增，删除，修改，查询)中都会被执行到，执行的先后顺序是Executor，ParameterHandler，ResultSetHandler，StatementHandler(其中ParameterHandler和ResultSetHandler的创建是在创建StatementHandler[3个可用的实现类CallableStatementHandler,PreparedStatementHandler,SimpleStatementHandler]的时候，其构造函数调用的[这3个实现类的构造函数其实都调用了父类BaseStatementHandler的构造函数])。 这4个方法实例化了对应的对象之后，都会调用interceptorChain的pluginAll方法，InterceptorChain的pluginAll刚才已经介绍过了，就是遍历所有的拦截器，然后调用各个拦截器的plugin方法。注意：拦截器的plugin方法的返回值会直接被赋值给原先的对象 由于可以拦截StatementHandler，这个接口主要处理sql语法的构建，因此比如分页的功能，可以用拦截器实现，只需要在拦截器的plugin方法中处理StatementHandler接口实现类中的sql即可，可使用反射实现。 MyBatis还提供了@Intercepts和@Signature关于拦截器的注解。官网的例子就是使用了这2个注解，还包括了Plugin类的使用：1234@Override public Object plugin(Object target) &#123; return Plugin.wrap(target, this); &#125; 下面我们就分析这3个 “新组合” 的源码，首先先看Plugin类的wrap方法：123456789101112public static Object wrap(Object target, Interceptor interceptor) &#123; Map&lt;Class&lt;?&gt;, Set&lt;Method&gt;&gt; signatureMap = getSignatureMap(interceptor); Class&lt;?&gt; type = target.getClass(); Class&lt;?&gt;[] interfaces = getAllInterfaces(type, signatureMap); if (interfaces.length &gt; 0) &#123; return Proxy.newProxyInstance( type.getClassLoader(), interfaces, new Plugin(target, interceptor, signatureMap)); &#125; return target; &#125; Plugin类实现了InvocationHandler接口，很明显，我们看到这里返回了一个JDK自身提供的动态代理类。我们解剖一下这个方法调用的其他方法： getSignatureMap方法：12345678910111213141516171819202122private static Map&lt;Class&lt;?&gt;, Set&lt;Method&gt;&gt; getSignatureMap(Interceptor interceptor) &#123; Intercepts interceptsAnnotation = interceptor.getClass().getAnnotation(Intercepts.class); if (interceptsAnnotation == null) &#123; // issue #251 throw new PluginException("No @Intercepts annotation was found in interceptor " + interceptor.getClass().getName()); &#125; Signature[] sigs = interceptsAnnotation.value(); Map&lt;Class&lt;?&gt;, Set&lt;Method&gt;&gt; signatureMap = new HashMap&lt;Class&lt;?&gt;, Set&lt;Method&gt;&gt;(); for (Signature sig : sigs) &#123; Set&lt;Method&gt; methods = signatureMap.get(sig.type()); if (methods == null) &#123; methods = new HashSet&lt;Method&gt;(); signatureMap.put(sig.type(), methods); &#125; try &#123; Method method = sig.type().getMethod(sig.method(), sig.args()); methods.add(method); &#125; catch (NoSuchMethodException e) &#123; throw new PluginException("Could not find method on " + sig.type() + " named " + sig.method() + ". Cause: " + e, e); &#125; &#125; return signatureMap; &#125; getSignatureMap方法解释：首先会拿到拦截器这个类的@Interceptors注解，然后拿到这个注解的属性@Signature注解集合，然后遍历这个集合，遍历的时候拿出@Signature注解的type属性(Class类型)，然后根据这个type得到带有method属性和args属性的Method。由于@Interceptors注解的@Signature属性是一个属性，所以最终会返回一个以type为key，value为Set&lt;Method&gt;的Map。1234@Intercepts(&#123;@Signature( type= Executor.class, method = "update", args = &#123;MappedStatement.class,Object.class&#125;)&#125;) 比如这个@Interceptors注解会返回一个key为Executor，value为集合(这个集合只有一个元素，也就是Method实例，这个Method实例就是Executor接口的update方法，且这个方法带有MappedStatement和Object类型的参数)。这个Method实例是根据@Signature的method和args属性得到的。如果args参数跟type类型的method方法对应不上，那么将会抛出异常。 getAllInterfaces方法：123456789101112private static Class&lt;?&gt;[] getAllInterfaces(Class&lt;?&gt; type, Map&lt;Class&lt;?&gt;, Set&lt;Method&gt;&gt; signatureMap) &#123; Set&lt;Class&lt;?&gt;&gt; interfaces = new HashSet&lt;Class&lt;?&gt;&gt;(); while (type != null) &#123; for (Class&lt;?&gt; c : type.getInterfaces()) &#123; if (signatureMap.containsKey(c)) &#123; interfaces.add(c); &#125; &#125; type = type.getSuperclass(); &#125; return interfaces.toArray(new Class&lt;?&gt;[interfaces.size()]); &#125; getAllInterfaces方法解释：根据目标实例target(这个target就是之前所说的MyBatis拦截器可以拦截的类，Executor,ParameterHandler,ResultSetHandler,StatementHandler)和它的父类们，返回signatureMap中含有target实现的接口数组。 所以Plugin这个类的作用就是根据@Interceptors注解，得到这个注解的属性@Signature数组，然后根据每个@Signature注解的type，method，args属性使用反射找到对应的Method。最终根据调用的target对象实现的接口决定是否返回一个代理对象替代原先的target对象。 比如MyBatis官网的例子，当Configuration调用newExecutor方法的时候，由于Executor接口的update(MappedStatement ms, Object parameter)方法被拦截器被截获。因此最终返回的是一个代理类Plugin，而不是Executor。这样调用方法的时候，如果是个代理类，那么会执行：1234567891011public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; try &#123; Set&lt;Method&gt; methods = signatureMap.get(method.getDeclaringClass()); if (methods != null &amp;&amp; methods.contains(method)) &#123; return interceptor.intercept(new Invocation(target, method, args)); &#125; return method.invoke(target, args); &#125; catch (Exception e) &#123; throw ExceptionUtil.unwrapThrowable(e); &#125; &#125; 没错，如果找到对应的方法被代理之后，那么会执行Interceptor接口的interceptor方法。 这个Invocation类如下：1234567891011121314151617181920212223242526272829public class Invocation &#123; private Object target; private Method method; private Object[] args; public Invocation(Object target, Method method, Object[] args) &#123; this.target = target; this.method = method; this.args = args; &#125; public Object getTarget() &#123; return target; &#125; public Method getMethod() &#123; return method; &#125; public Object[] getArgs() &#123; return args; &#125; public Object proceed() throws InvocationTargetException, IllegalAccessException &#123; return method.invoke(target, args); &#125;&#125; 它的proceed方法也就是调用原先方法(不走代理)。 总结MyBatis拦截器接口提供的3个方法中，plugin方法用于某些处理器(Handler)的构建过程。interceptor方法用于处理代理类的执行。setProperties方法用于拦截器属性的设置。 其实MyBatis官网提供的使用@Interceptors和@Signature注解以及Plugin类这样处理拦截器的方法，我们不一定要直接这样使用。我们也可以抛弃这3个类，直接在plugin方法内部根据target实例的类型做相应的操作。 总体来说MyBatis拦截器还是很简单的，拦截器本身不需要太多的知识点，但是学习拦截器需要对MyBatis中的各个接口很熟悉，因为拦截器涉及到了各个接口的知识点。]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>mybatis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MyBatis动态SQL底层原理分析]]></title>
    <url>%2F2018%2F10%2F08%2FMyBatis%E5%8A%A8%E6%80%81SQL%E5%BA%95%E5%B1%82%E5%8E%9F%E7%90%86%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[作者：丶Format来源：http://fangjian0423.github.io/ 前言废话不多说，直接进入文章。我们在使用mybatis的时候，会在xml中编写sql语句。比如这段动态sql代码：123456789101112131415&lt;update id="update" parameterType="org.format.dynamicproxy.mybatis.bean.User"&gt; UPDATE users &lt;trim prefix="SET" prefixOverrides=","&gt; &lt;if test="name != null and name != ''"&gt; name = #&#123;name&#125; &lt;/if&gt; &lt;if test="age != null and age != ''"&gt; , age = #&#123;age&#125; &lt;/if&gt; &lt;if test="birthday != null and birthday != ''"&gt; , birthday = #&#123;birthday&#125; &lt;/if&gt; &lt;/trim&gt; where id = $&#123;id&#125; &lt;/update&gt; mybatis底层是如何构造这段sql的？这方面的知识网上资料不多，于是就写了这么一篇文章。下面带着这个疑问，我们一步一步分析。 介绍MyBatis中一些关于动态SQL的接口和类SqlNode接口，简单理解就是xml中的每个标签，比如上述sql的update,trim,if标签：123public interface SqlNode &#123; boolean apply(DynamicContext context);&#125; SqlSource Sql源接口，代表从xml文件或注解映射的sql内容，主要就是用于创建BoundSql，有实现类DynamicSqlSource(动态Sql源)，StaticSqlSource(静态Sql源)等：123public interface SqlSource &#123; BoundSql getBoundSql(Object parameterObject);&#125; BoundSql类，封装mybatis最终产生sql的类，包括sql语句，参数，参数源数据等参数： XNode，一个Dom API中的Node接口的扩展类。 BaseBuilder接口及其实现类(属性，方法省略了，大家有兴趣的自己看),这些Builder的作用就是用于构造sql: 下面我们简单分析下其中4个Builder： 1 XMLConfigBuilder 解析mybatis中configLocation属性中的全局xml文件，内部会使用XMLMapperBuilder解析各个xml文件。 2 XMLMapperBuilder 遍历mybatis中mapperLocations属性中的xml文件中每个节点的Builder，比如user.xml，内部会使用XMLStatementBuilder处理xml中的每个节点。 3 XMLStatementBuilder 解析xml文件中各个节点，比如select,insert,update,delete节点，内部会使用XMLScriptBuilder处理节点的sql部分，遍历产生的数据会丢到Configuration的mappedStatements中。 4 XMLScriptBuilder 解析xml中各个节点sql部分的Builder。 LanguageDriver接口及其实现类(属性，方法省略了，大家有兴趣的自己看)，该接口主要的作用就是构造sql: 简单分析下XMLLanguageDriver(处理xml中的sql，RawLanguageDriver处理静态sql)： XMLLanguageDriver内部会使用XMLScriptBuilder解析xml中的sql部分。 ok， 大部分比较重要的类我们都已经介绍了，下面源码分析走起。 源码分析走起Spring与Mybatis整合的时候需要配置SqlSessionFactoryBean，该配置会加入数据源和mybatis xml配置文件路径等信息：12345&lt;bean id="sqlSessionFactory" class="org.mybatis.spring.SqlSessionFactoryBean"&gt; &lt;property name="dataSource" ref="dataSource"/&gt; &lt;property name="configLocation" value="classpath:mybatisConfig.xml"/&gt; &lt;property name="mapperLocations" value="classpath*:org/format/dao/*.xml"/&gt; &lt;/bean&gt; 我们就分析这一段配置背后的细节： SqlSessionFactoryBean实现了Spring的InitializingBean接口，InitializingBean接口的afterPropertiesSet方法中会调用buildSqlSessionFactory方法 buildSqlSessionFactory方法内部会使用XMLConfigBuilder解析属性configLocation中配置的路径，还会使用XMLMapperBuilder属性解析mapperLocations属性中的各个xml文件。 部分源码如下： 由于XMLConfigBuilder内部也是使用XMLMapperBuilder，我们就看看XMLMapperBuilder的解析细节。 我们关注一下，增删改查节点的解析。 XMLStatementBuilder的解析： 默认会使用XMLLanguageDriver创建SqlSource（Configuration构造函数中设置）。 XMLLanguageDriver创建SqlSource： XMLScriptBuilder解析sql： 得到SqlSource之后，会放到Configuration中，有了SqlSource，就能拿BoundSql了，BoundSql可以得到最终的sql。 实例分析我以以下xml的解析大概说下parseDynamicTags的解析过程：123456789101112131415&lt;update id="update" parameterType="org.format.dynamicproxy.mybatis.bean.User"&gt; UPDATE users &lt;trim prefix="SET" prefixOverrides=","&gt; &lt;if test="name != null and name != ''"&gt; name = #&#123;name&#125; &lt;/if&gt; &lt;if test="age != null and age != ''"&gt; , age = #&#123;age&#125; &lt;/if&gt; &lt;if test="birthday != null and birthday != ''"&gt; , birthday = #&#123;birthday&#125; &lt;/if&gt; &lt;/trim&gt; where id = $&#123;id&#125; &lt;/update&gt; 在看这段解析之前，请先了解dom相关的知识，xml dom知识, dom博文 parseDynamicTags方法的返回值是一个List，也就是一个Sql节点集合。SqlNode本文一开始已经介绍，分析完解析过程之后会说一下各个SqlNode类型的作用。 1 首先根据update节点(Node)得到所有的子节点，分别是3个子节点 (1)文本节点 \n UPDATE users (2)trim子节点 … (3)文本节点 \n where id = #{id} 2 遍历各个子节点 (1) 如果节点类型是文本或者CDATA，构造一个TextSqlNode或StaticTextSqlNode (2) 如果节点类型是元素，说明该update节点是个动态sql，然后会使用NodeHandler处理各个类型的子节点。这里的NodeHandler是XMLScriptBuilder的一个内部接口，其实现类包括TrimHandler、WhereHandler、SetHandler、IfHandler、ChooseHandler等。看类名也就明白了这个Handler的作用，比如我们分析的trim节点，对应的是TrimHandler；if节点，对应的是IfHandler… 这里子节点trim被TrimHandler处理，TrimHandler内部也使用parseDynamicTags方法解析节点 3 遇到子节点是元素的话，重复以上步骤 trim子节点内部有7个子节点，分别是文本节点、if节点、是文本节点、if节点、是文本节点、if节点、文本节点。文本节点跟之前一样处理，if节点使用IfHandler处理 遍历步骤如上所示，下面我们看下几个Handler的实现细节。 IfHandler处理方法也是使用parseDynamicTags方法，然后加上if标签必要的属性。123456789private class IfHandler implements NodeHandler &#123; public void handleNode(XNode nodeToHandle, List&lt;SqlNode&gt; targetContents) &#123; List&lt;SqlNode&gt; contents = parseDynamicTags(nodeToHandle); MixedSqlNode mixedSqlNode = new MixedSqlNode(contents); String test = nodeToHandle.getStringAttribute("test"); IfSqlNode ifSqlNode = new IfSqlNode(mixedSqlNode, test); targetContents.add(ifSqlNode); &#125;&#125; TrimHandler处理方法也是使用parseDynamicTags方法，然后加上trim标签必要的属性。123456789101112private class TrimHandler implements NodeHandler &#123; public void handleNode(XNode nodeToHandle, List&lt;SqlNode&gt; targetContents) &#123; List&lt;SqlNode&gt; contents = parseDynamicTags(nodeToHandle); MixedSqlNode mixedSqlNode = new MixedSqlNode(contents); String prefix = nodeToHandle.getStringAttribute("prefix"); String prefixOverrides = nodeToHandle.getStringAttribute("prefixOverrides"); String suffix = nodeToHandle.getStringAttribute("suffix"); String suffixOverrides = nodeToHandle.getStringAttribute("suffixOverrides"); TrimSqlNode trim = new TrimSqlNode(configuration, mixedSqlNode, prefix, prefixOverrides, suffix, suffixOverrides); targetContents.add(trim); &#125;&#125; 以上update方法最终通过parseDynamicTags方法得到的SqlNode集合如下： trim节点： 由于这个update方法是个动态节点，因此构造出了DynamicSqlSource。 DynamicSqlSource内部就可以构造sql了: DynamicSqlSource内部的SqlNode属性是一个MixedSqlNode。 然后我们看看各个SqlNode实现类的apply方法 下面分析一下两个SqlNode实现类的apply方法实现： MixedSqlNode：123456public boolean apply(DynamicContext context) &#123; for (SqlNode sqlNode : contents) &#123; sqlNode.apply(context); &#125; return true;&#125; MixedSqlNode会遍历调用内部各个sqlNode的apply方法。 StaticTextSqlNode:1234public boolean apply(DynamicContext context) &#123; context.appendSql(text); return true;&#125; 直接append sql文本。 IfSqlNode:1234567public boolean apply(DynamicContext context) &#123; if (evaluator.evaluateBoolean(test, context.getBindings())) &#123; contents.apply(context); return true; &#125; return false;&#125; 这里的evaluator是一个ExpressionEvaluator类型的实例，内部使用了OGNL处理表达式逻辑。 TrimSqlNode:12345678910111213141516171819202122232425262728293031323334public boolean apply(DynamicContext context) &#123; FilteredDynamicContext filteredDynamicContext = new FilteredDynamicContext(context); boolean result = contents.apply(filteredDynamicContext); filteredDynamicContext.applyAll(); return result;&#125;public void applyAll() &#123; sqlBuffer = new StringBuilder(sqlBuffer.toString().trim()); String trimmedUppercaseSql = sqlBuffer.toString().toUpperCase(Locale.ENGLISH); if (trimmedUppercaseSql.length() &gt; 0) &#123; applyPrefix(sqlBuffer, trimmedUppercaseSql); applySuffix(sqlBuffer, trimmedUppercaseSql); &#125; delegate.appendSql(sqlBuffer.toString()); &#125;private void applyPrefix(StringBuilder sql, String trimmedUppercaseSql) &#123; if (!prefixApplied) &#123; prefixApplied = true; if (prefixesToOverride != null) &#123; for (String toRemove : prefixesToOverride) &#123; if (trimmedUppercaseSql.startsWith(toRemove)) &#123; sql.delete(0, toRemove.trim().length()); break; &#125; &#125; &#125; if (prefix != null) &#123; sql.insert(0, " "); sql.insert(0, prefix); &#125; &#125; &#125; TrimSqlNode的apply方法也是调用属性contents(一般都是MixedSqlNode)的apply方法，按照实例也就是7个SqlNode，都是StaticTextSqlNode和IfSqlNode。 最后会使用FilteredDynamicContext过滤掉prefix和suffix。 总结大致讲解了一下mybatis对动态sql语句的解析过程，其实回过头来看看不算复杂，还算蛮简单的。 之前接触mybaits的时候遇到刚才分析的那一段动态sql的时候总是很费解。123456789101112131415&lt;update id="update" parameterType="org.format.dynamicproxy.mybatis.bean.User"&gt; UPDATE users &lt;trim prefix="SET" prefixOverrides=","&gt; &lt;if test="name != null and name != ''"&gt; name = #&#123;name&#125; &lt;/if&gt; &lt;if test="age != null and age != ''"&gt; , age = #&#123;age&#125; &lt;/if&gt; &lt;if test="birthday != null and birthday != ''"&gt; , birthday = #&#123;birthday&#125; &lt;/if&gt; &lt;/trim&gt; where id = $&#123;id&#125; &lt;/update&gt; 想搞明白这个trim节点的prefixOverrides到底是什么意思(从字面上理解就是前缀覆盖)，而且官方文档上也没这方面知识的说明。我将这段xml改成如下：123456789101112131415&lt;update id="update" parameterType="org.format.dynamicproxy.mybatis.bean.User"&gt; UPDATE users &lt;trim prefix="SET" prefixOverrides=","&gt; &lt;if test="name != null and name != ''"&gt; , name = #&#123;name&#125; &lt;/if&gt; &lt;if test="age != null and age != ''"&gt; , age = #&#123;age&#125; &lt;/if&gt; &lt;if test="birthday != null and birthday != ''"&gt; , birthday = #&#123;birthday&#125; &lt;/if&gt; &lt;/trim&gt; where id = $&#123;id&#125; &lt;/update&gt; (第二段第一个if节点多了个逗号) 结果我发现这2段xml解析的结果是一样的，非常迫切地想知道这到底是为什么，然后这也促使了我去看源码的决心。最终还是看下来了。 文章有点长，而且讲的也不是非常直观，希望对有些人有帮助。]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>mybatis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[通过源码分析MyBatis的缓存]]></title>
    <url>%2F2018%2F10%2F08%2F%E9%80%9A%E8%BF%87%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90MyBatis%E7%9A%84%E7%BC%93%E5%AD%98%2F</url>
    <content type="text"><![CDATA[作者：丶Format来源：http://fangjian0423.github.io/ 前方高能！ 本文内容有点多，通过实际测试例子+源码分析的方式解剖MyBatis缓存的概念，对这方面有兴趣的小伙伴请继续看下去~ MyBatis缓存介绍首先看一段wiki上关于MyBatis缓存的介绍： MyBatis支持声明式数据缓存（declarative data caching）。当一条SQL语句被标记为“可缓存”后，首次执行它时从数据库获取的所有数据会被存储在一段高速缓存中，今后执行这条语句时就会从高速缓存中读取结果，而不是再次命中数据库。MyBatis提供了默认下基于Java HashMap的缓存实现，以及用于与OSCache、Ehcache、Hazelcast和Memcached连接的默认连接器。MyBatis还提供API供其他缓存实现使用。 重点的那句话就是：MyBatis执行SQL语句之后，这条语句就是被缓存，以后再执行这条语句的时候，会直接从缓存中拿结果，而不是再次执行SQL 这也就是大家常说的MyBatis一级缓存，一级缓存的作用域scope是SqlSession。 MyBatis同时还提供了一种全局作用域global scope的缓存，这也叫做二级缓存，也称作全局缓存。 一级缓存测试同个session进行两次相同查询： 123456789101112@Testpublic void test() &#123; SqlSession sqlSession = sqlSessionFactory.openSession(); try &#123; User user = (User)sqlSession.selectOne("org.format.mybatis.cache.UserMapper.getById", 1); log.debug(user); User user2 = (User)sqlSession.selectOne("org.format.mybatis.cache.UserMapper.getById", 1); log.debug(user2); &#125; finally &#123; sqlSession.close(); &#125;&#125; MyBatis只进行1次数据库查询： 12345==&gt; Preparing: select * from USERS WHERE ID = ? ==&gt; Parameters: 1(Integer)&lt;== Total: 1User&#123;id=1, name='format', age=23, birthday=Sun Oct 12 23:20:13 CST 2014&#125;User&#123;id=1, name='format', age=23, birthday=Sun Oct 12 23:20:13 CST 2014&#125; 同个session进行两次不同的查询： 123456789101112@Testpublic void test() &#123; SqlSession sqlSession = sqlSessionFactory.openSession(); try &#123; User user = (User)sqlSession.selectOne("org.format.mybatis.cache.UserMapper.getById", 1); log.debug(user); User user2 = (User)sqlSession.selectOne("org.format.mybatis.cache.UserMapper.getById", 2); log.debug(user2); &#125; finally &#123; sqlSession.close(); &#125;&#125; MyBatis进行两次数据库查询： 12345678==&gt; Preparing: select * from USERS WHERE ID = ? ==&gt; Parameters: 1(Integer)&lt;== Total: 1User&#123;id=1, name='format', age=23, birthday=Sun Oct 12 23:20:13 CST 2014&#125;==&gt; Preparing: select * from USERS WHERE ID = ? ==&gt; Parameters: 2(Integer)&lt;== Total: 1User&#123;id=2, name='FFF', age=50, birthday=Sat Dec 06 17:12:01 CST 2014&#125; 不同session，进行相同查询： 1234567891011121314@Testpublic void test() &#123; SqlSession sqlSession = sqlSessionFactory.openSession(); SqlSession sqlSession2 = sqlSessionFactory.openSession(); try &#123; User user = (User)sqlSession.selectOne("org.format.mybatis.cache.UserMapper.getById", 1); log.debug(user); User user2 = (User)sqlSession2.selectOne("org.format.mybatis.cache.UserMapper.getById", 1); log.debug(user2); &#125; finally &#123; sqlSession.close(); sqlSession2.close(); &#125;&#125; MyBatis进行了两次数据库查询： 12345678==&gt; Preparing: select * from USERS WHERE ID = ? ==&gt; Parameters: 1(Integer)&lt;== Total: 1User&#123;id=1, name='format', age=23, birthday=Sun Oct 12 23:20:13 CST 2014&#125;==&gt; Preparing: select * from USERS WHERE ID = ? ==&gt; Parameters: 1(Integer)&lt;== Total: 1User&#123;id=1, name='format', age=23, birthday=Sun Oct 12 23:20:13 CST 2014&#125; 同个session,查询之后更新数据，再次查询相同的语句： 123456789101112131415@Testpublic void test() &#123; SqlSession sqlSession = sqlSessionFactory.openSession(); try &#123; User user = (User)sqlSession.selectOne("org.format.mybatis.cache.UserMapper.getById", 1); log.debug(user); user.setAge(100); sqlSession.update("org.format.mybatis.cache.UserMapper.update", user); User user2 = (User)sqlSession.selectOne("org.format.mybatis.cache.UserMapper.getById", 1); log.debug(user2); sqlSession.commit(); &#125; finally &#123; sqlSession.close(); &#125;&#125; 更新操作之后缓存会被清除： 1234567891011==&gt; Preparing: select * from USERS WHERE ID = ? ==&gt; Parameters: 1(Integer)&lt;== Total: 1User&#123;id=1, name='format', age=23, birthday=Sun Oct 12 23:20:13 CST 2014&#125;==&gt; Preparing: update USERS SET NAME = ? , AGE = ? , BIRTHDAY = ? where ID = ? ==&gt; Parameters: format(String), 23(Integer), 2014-10-12 23:20:13.0(Timestamp), 1(Integer)&lt;== Updates: 1==&gt; Preparing: select * from USERS WHERE ID = ? ==&gt; Parameters: 1(Integer)&lt;== Total: 1User&#123;id=1, name='format', age=23, birthday=Sun Oct 12 23:20:13 CST 2014&#125; 很明显，结果验证了一级缓存的概念，在同个SqlSession中，查询语句相同的sql会被缓存，但是一旦执行新增或更新或删除操作，缓存就会被清除 源码分析在分析MyBatis的一级缓存之前，我们先简单看下MyBatis中几个重要的类和接口： org.apache.ibatis.session.Configuration类：MyBatis全局配置信息类 org.apache.ibatis.session.SqlSessionFactory接口：操作SqlSession的工厂接口，具体的实现类是DefaultSqlSessionFactory org.apache.ibatis.session.SqlSession接口：执行sql，管理事务的接口，具体的实现类是DefaultSqlSession org.apache.ibatis.executor.Executor接口：sql执行器，SqlSession执行sql最终是通过该接口实现的，常用的实现类有SimpleExecutor和CachingExecutor,这些实现类都使用了装饰者设计模式 一级缓存的作用域是SqlSession，那么我们就先看一下SqlSession的select过程： 这是DefaultSqlSession（SqlSession接口实现类，MyBatis默认使用这个类）的selectList源码（我们例子上使用的是selectOne方法，调用selectOne方法最终会执行selectList方法）： 1234567891011public &lt;E&gt; List&lt;E&gt; selectList(String statement, Object parameter, RowBounds rowBounds) &#123; try &#123; MappedStatement ms = configuration.getMappedStatement(statement); List&lt;E&gt; result = executor.query(ms, wrapCollection(parameter), rowBounds, Executor.NO_RESULT_HANDLER); return result; &#125; catch (Exception e) &#123; throw ExceptionFactory.wrapException("Error querying database. Cause: " + e, e); &#125; finally &#123; ErrorContext.instance().reset(); &#125;&#125; 我们看到SqlSession最终会调用Executor接口的方法。 接下来我们看下DefaultSqlSession中的executor接口属性具体是哪个实现类。 DefaultSqlSession的构造过程（DefaultSqlSessionFactory内部）： 123456789101112131415private SqlSession openSessionFromDataSource(ExecutorType execType, TransactionIsolationLevel level, boolean autoCommit) &#123; Transaction tx = null; try &#123; final Environment environment = configuration.getEnvironment(); final TransactionFactory transactionFactory = getTransactionFactoryFromEnvironment(environment); tx = transactionFactory.newTransaction(environment.getDataSource(), level, autoCommit); final Executor executor = configuration.newExecutor(tx, execType, autoCommit); return new DefaultSqlSession(configuration, executor); &#125; catch (Exception e) &#123; closeTransaction(tx); // may have fetched a connection so lets call close() throw ExceptionFactory.wrapException("Error opening session. Cause: " + e, e); &#125; finally &#123; ErrorContext.instance().reset(); &#125;&#125; 我们看到DefaultSqlSessionFactory构造DefaultSqlSession的时候，Executor接口的实现类是由Configuration构造的： 1234567891011121314151617public Executor newExecutor(Transaction transaction, ExecutorType executorType, boolean autoCommit) &#123; executorType = executorType == null ? defaultExecutorType : executorType; executorType = executorType == null ? ExecutorType.SIMPLE : executorType; Executor executor; if (ExecutorType.BATCH == executorType) &#123; executor = new BatchExecutor(this, transaction); &#125; else if (ExecutorType.REUSE == executorType) &#123; executor = new ReuseExecutor(this, transaction); &#125; else &#123; executor = new SimpleExecutor(this, transaction); &#125; if (cacheEnabled) &#123; executor = new CachingExecutor(executor, autoCommit); &#125; executor = (Executor) interceptorChain.pluginAll(executor); return executor;&#125; Executor根据ExecutorType的不同而创建，最常用的是SimpleExecutor，本文的例子也是创建这个实现类。 最后我们发现如果cacheEnabled这个属性为true的话，那么executor会被包一层装饰器，这个装饰器是CachingExecutor。其中cacheEnabled这个属性是mybatis总配置文件中settings节点中cacheEnabled子节点的值，默认就是true，也就是说我们在mybatis总配置文件中不配cacheEnabled的话，它也是默认为打开的。 现在，问题就剩下一个了，CachingExecutor执行sql的时候到底做了什么？ 带着这个问题，我们继续走下去（CachingExecutor的query方法）： 1234567891011121314151617181920212223public &lt;E&gt; List&lt;E&gt; query(MappedStatement ms, Object parameterObject, RowBounds rowBounds, ResultHandler resultHandler, CacheKey key, BoundSql boundSql) throws SQLException &#123; Cache cache = ms.getCache(); if (cache != null) &#123; flushCacheIfRequired(ms); if (ms.isUseCache() &amp;&amp; resultHandler == null) &#123; ensureNoOutParams(ms, parameterObject, boundSql); if (!dirty) &#123; cache.getReadWriteLock().readLock().lock(); try &#123; @SuppressWarnings("unchecked") List&lt;E&gt; cachedList = (List&lt;E&gt;) cache.getObject(key); if (cachedList != null) return cachedList; &#125; finally &#123; cache.getReadWriteLock().readLock().unlock(); &#125; &#125; List&lt;E&gt; list = delegate.&lt;E&gt; query(ms, parameterObject, rowBounds, resultHandler, key, boundSql); tcm.putObject(cache, key, list); // issue #578. Query must be not synchronized to prevent deadlocks return list; &#125; &#125; return delegate.&lt;E&gt;query(ms, parameterObject, rowBounds, resultHandler, key, boundSql);&#125; 其中Cache cache = ms.getCache();这句代码中，这个cache实际上就是个二级缓存，由于我们没有开启二级缓存(二级缓存的内容下面会分析)，因此这里执行了最后一句话。这里的delegate也就是SimpleExecutor,SimpleExecutor没有Override父类的query方法，因此最终执行了SimpleExecutor的父类BaseExecutor的query方法。 所以一级缓存最重要的代码就是BaseExecutor的query方法! BaseExecutor的属性localCache是个PerpetualCache类型的实例，PerpetualCache类是实现了MyBatis的Cache缓存接口的实现类之一，内部有个Map&lt;Object, Object&gt;类型的属性用来存储缓存数据。 这个localCache的类型在BaseExecutor内部是写死的。 这个localCache就是一级缓存！ 接下来我们看下为何执行新增或更新或删除操作，一级缓存就会被清除这个问题。 首先MyBatis处理新增或删除的时候，最终都是调用update方法，也就是说新增或者删除操作在MyBatis眼里都是一个更新操作。 我们看下DefaultSqlSession的update方法： 1234567891011public int update(String statement, Object parameter) &#123; try &#123; dirty = true; MappedStatement ms = configuration.getMappedStatement(statement); return executor.update(ms, wrapCollection(parameter)); &#125; catch (Exception e) &#123; throw ExceptionFactory.wrapException("Error updating database. Cause: " + e, e); &#125; finally &#123; ErrorContext.instance().reset(); &#125;&#125; 很明显，这里调用了CachingExecutor的update方法： 1234public int update(MappedStatement ms, Object parameterObject) throws SQLException &#123; flushCacheIfRequired(ms); return delegate.update(ms, parameterObject);&#125; 这里的flushCacheIfRequired方法清除的是二级缓存，我们之后会分析。 CachingExecutor委托给了(之前已经分析过)SimpleExecutor的update方法，SimpleExecutor没有Override父类BaseExecutor的update方法，因此我们看BaseExecutor的update方法： 123456public int update(MappedStatement ms, Object parameter) throws SQLException &#123; ErrorContext.instance().resource(ms.getResource()).activity("executing an update").object(ms.getId()); if (closed) throw new ExecutorException("Executor was closed."); clearLocalCache(); return doUpdate(ms, parameter);&#125; 我们看到了关键的一句代码： clearLocalCache(); 进去看看： 123456public void clearLocalCache() &#123; if (!closed) &#123; localCache.clear(); localOutputParameterCache.clear(); &#125;&#125; 没错，就是这条，sqlsession没有关闭的话，进行新增、删除、修改操作的话就是清除一级缓存，也就是SqlSession的缓存。 二级缓存二级缓存的作用域是全局，换句话说，二级缓存已经脱离SqlSession的控制了。 在测试二级缓存之前，我先把结论说一下： 二级缓存的作用域是全局的，二级缓存在SqlSession关闭或提交之后才会生效。 在分析MyBatis的二级缓存之前，我们先简单看下MyBatis中一个关于二级缓存的类(其他相关的类和接口之前已经分析过)： org.apache.ibatis.mapping.MappedStatement： MappedStatement类在Mybatis框架中用于表示XML文件中一个sql语句节点，即一个&lt;select /&gt;、&lt;update /&gt;或者&lt;insert /&gt;标签。Mybatis框架在初始化阶段会对XML配置文件进行读取，将其中的sql语句节点对象化为一个个MappedStatement对象。 配置二级缓存跟一级缓存不同，一级缓存不需要配置任何东西，且默认打开。 二级缓存就需要配置一些东西。 本文就说下最简单的配置，在mapper文件上加上这句配置即可： &lt;cache/&gt; 其实二级缓存跟3个配置有关： mybatis全局配置文件中的setting中的cacheEnabled需要为true(默认为true，不设置也行) mapper配置文件中需要加入&lt;cache&gt;节点 mapper配置文件中的select节点需要加上属性useCache需要为true(默认为true，不设置也行) 测试不同SqlSession，查询相同语句，第一次查询之后commit SqlSession： 1234567891011121314151617@Testpublic void testCache2() &#123; SqlSession sqlSession = sqlSessionFactory.openSession(); SqlSession sqlSession2 = sqlSessionFactory.openSession(); try &#123; String sql = "org.format.mybatis.cache.UserMapper.getById"; User user = (User)sqlSession.selectOne(sql, 1); log.debug(user); // 注意，这里一定要提交。 不提交还是会查询两次数据库 sqlSession.commit(); User user2 = (User)sqlSession2.selectOne(sql, 1); log.debug(user2); &#125; finally &#123; sqlSession.close(); sqlSession2.close(); &#125;&#125; MyBatis仅进行了一次数据库查询： 12345==&gt; Preparing: select * from USERS WHERE ID = ? ==&gt; Parameters: 1(Integer)&lt;== Total: 1User&#123;id=1, name='format', age=23, birthday=Sun Oct 12 23:20:13 CST 2014&#125;User&#123;id=1, name='format', age=23, birthday=Sun Oct 12 23:20:13 CST 2014&#125; 不同SqlSession，查询相同语句，第一次查询之后close SqlSession： 123456789101112131415@Testpublic void testCache2() &#123; SqlSession sqlSession = sqlSessionFactory.openSession(); SqlSession sqlSession2 = sqlSessionFactory.openSession(); try &#123; String sql = "org.format.mybatis.cache.UserMapper.getById"; User user = (User)sqlSession.selectOne(sql, 1); log.debug(user); sqlSession.close(); User user2 = (User)sqlSession2.selectOne(sql, 1); log.debug(user2); &#125; finally &#123; sqlSession2.close(); &#125;&#125; MyBatis仅进行了一次数据库查询： 12345==&gt; Preparing: select * from USERS WHERE ID = ? ==&gt; Parameters: 1(Integer)&lt;== Total: 1User&#123;id=1, name='format', age=23, birthday=Sun Oct 12 23:20:13 CST 2014&#125;User&#123;id=1, name='format', age=23, birthday=Sun Oct 12 23:20:13 CST 2014&#125; 不同SqlSesson，查询相同语句。 第一次查询之后SqlSession不提交： 123456789101112131415@Testpublic void testCache2() &#123; SqlSession sqlSession = sqlSessionFactory.openSession(); SqlSession sqlSession2 = sqlSessionFactory.openSession(); try &#123; String sql = "org.format.mybatis.cache.UserMapper.getById"; User user = (User)sqlSession.selectOne(sql, 1); log.debug(user); User user2 = (User)sqlSession2.selectOne(sql, 1); log.debug(user2); &#125; finally &#123; sqlSession.close(); sqlSession2.close(); &#125;&#125; MyBatis执行了两次数据库查询： 12345678==&gt; Preparing: select * from USERS WHERE ID = ? ==&gt; Parameters: 1(Integer)&lt;== Total: 1User&#123;id=1, name='format', age=23, birthday=Sun Oct 12 23:20:13 CST 2014&#125;==&gt; Preparing: select * from USERS WHERE ID = ? ==&gt; Parameters: 1(Integer)&lt;== Total: 1User&#123;id=1, name='format', age=23, birthday=Sun Oct 12 23:20:13 CST 2014&#125; 源码分析我们从在mapper文件中加入的&lt;cache/&gt;中开始分析源码，关于MyBatis的SQL解析请参考另外一篇博客Mybatis解析动态sql原理分析。接下来我们看下这个cache的解析： XMLMappedBuilder（解析每个mapper配置文件的解析类，每一个mapper配置都会实例化一个XMLMapperBuilder类）的解析方法： 1234567891011121314151617private void configurationElement(XNode context) &#123; try &#123; String namespace = context.getStringAttribute("namespace"); if (namespace.equals("")) &#123; throw new BuilderException("Mapper's namespace cannot be empty"); &#125; builderAssistant.setCurrentNamespace(namespace); cacheRefElement(context.evalNode("cache-ref")); cacheElement(context.evalNode("cache")); parameterMapElement(context.evalNodes("/mapper/parameterMap")); resultMapElements(context.evalNodes("/mapper/resultMap")); sqlElement(context.evalNodes("/mapper/sql")); buildStatementFromContext(context.evalNodes("select|insert|update|delete")); &#125; catch (Exception e) &#123; throw new BuilderException("Error parsing Mapper XML. Cause: " + e, e); &#125;&#125; 我们看到了解析cache的那段代码： 12345678910111213private void cacheElement(XNode context) throws Exception &#123; if (context != null) &#123; String type = context.getStringAttribute("type", "PERPETUAL"); Class&lt;? extends Cache&gt; typeClass = typeAliasRegistry.resolveAlias(type); String eviction = context.getStringAttribute("eviction", "LRU"); Class&lt;? extends Cache&gt; evictionClass = typeAliasRegistry.resolveAlias(eviction); Long flushInterval = context.getLongAttribute("flushInterval"); Integer size = context.getIntAttribute("size"); boolean readWrite = !context.getBooleanAttribute("readOnly", false); Properties props = context.getChildrenAsProperties(); builderAssistant.useNewCache(typeClass, evictionClass, flushInterval, size, readWrite, props); &#125;&#125; 解析完cache标签之后会使用builderAssistant的userNewCache方法，这里的builderAssistant是一个MapperBuilderAssistant类型的帮助类，每个XMLMappedBuilder构造的时候都会实例化这个属性，MapperBuilderAssistant类内部有个Cache类型的currentCache属性，这个属性也就是mapper配置文件中cache节点所代表的值：1234567891011121314151617181920public Cache useNewCache(Class&lt;? extends Cache&gt; typeClass, Class&lt;? extends Cache&gt; evictionClass, Long flushInterval, Integer size, boolean readWrite, Properties props) &#123; typeClass = valueOrDefault(typeClass, PerpetualCache.class); evictionClass = valueOrDefault(evictionClass, LruCache.class); Cache cache = new CacheBuilder(currentNamespace) .implementation(typeClass) .addDecorator(evictionClass) .clearInterval(flushInterval) .size(size) .readWrite(readWrite) .properties(props) .build(); configuration.addCache(cache); currentCache = cache; return cache;&#125; ok，现在mapper配置文件中的cache节点被解析到了XMLMapperBuilder实例中的builderAssistant属性中的currentCache值里。 接下来XMLMapperBuilder会解析select节点，解析select节点的时候使用XMLStatementBuilder进行解析(也包括其他insert，update，delete节点)：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556public void parseStatementNode() &#123; String id = context.getStringAttribute("id"); String databaseId = context.getStringAttribute("databaseId"); if (!databaseIdMatchesCurrent(id, databaseId, this.requiredDatabaseId)) return; Integer fetchSize = context.getIntAttribute("fetchSize"); Integer timeout = context.getIntAttribute("timeout"); String parameterMap = context.getStringAttribute("parameterMap"); String parameterType = context.getStringAttribute("parameterType"); Class&lt;?&gt; parameterTypeClass = resolveClass(parameterType); String resultMap = context.getStringAttribute("resultMap"); String resultType = context.getStringAttribute("resultType"); String lang = context.getStringAttribute("lang"); LanguageDriver langDriver = getLanguageDriver(lang); Class&lt;?&gt; resultTypeClass = resolveClass(resultType); String resultSetType = context.getStringAttribute("resultSetType"); StatementType statementType = StatementType.valueOf(context.getStringAttribute("statementType", StatementType.PREPARED.toString())); ResultSetType resultSetTypeEnum = resolveResultSetType(resultSetType); String nodeName = context.getNode().getNodeName(); SqlCommandType sqlCommandType = SqlCommandType.valueOf(nodeName.toUpperCase(Locale.ENGLISH)); boolean isSelect = sqlCommandType == SqlCommandType.SELECT; boolean flushCache = context.getBooleanAttribute("flushCache", !isSelect); boolean useCache = context.getBooleanAttribute("useCache", isSelect); boolean resultOrdered = context.getBooleanAttribute("resultOrdered", false); // Include Fragments before parsing XMLIncludeTransformer includeParser = new XMLIncludeTransformer(configuration, builderAssistant); includeParser.applyIncludes(context.getNode()); // Parse selectKey after includes and remove them. processSelectKeyNodes(id, parameterTypeClass, langDriver); // Parse the SQL (pre: &lt;selectKey&gt; and &lt;include&gt; were parsed and removed) SqlSource sqlSource = langDriver.createSqlSource(configuration, context, parameterTypeClass); String resultSets = context.getStringAttribute("resultSets"); String keyProperty = context.getStringAttribute("keyProperty"); String keyColumn = context.getStringAttribute("keyColumn"); KeyGenerator keyGenerator; String keyStatementId = id + SelectKeyGenerator.SELECT_KEY_SUFFIX; keyStatementId = builderAssistant.applyCurrentNamespace(keyStatementId, true); if (configuration.hasKeyGenerator(keyStatementId)) &#123; keyGenerator = configuration.getKeyGenerator(keyStatementId); &#125; else &#123; keyGenerator = context.getBooleanAttribute("useGeneratedKeys", configuration.isUseGeneratedKeys() &amp;&amp; SqlCommandType.INSERT.equals(sqlCommandType)) ? new Jdbc3KeyGenerator() : new NoKeyGenerator(); &#125; builderAssistant.addMappedStatement(id, sqlSource, statementType, sqlCommandType, fetchSize, timeout, parameterMap, parameterTypeClass, resultMap, resultTypeClass, resultSetTypeEnum, flushCache, useCache, resultOrdered, keyGenerator, keyProperty, keyColumn, databaseId, langDriver, resultSets);&#125; 这段代码前面都是解析一些标签的属性，我们看到了最后一行使用builderAssistant添加MappedStatement，其中builderAssistant属性是构造XMLStatementBuilder的时候通过XMLMappedBuilder传入的，我们继续看builderAssistant的addMappedStatement方法： 进入setStatementCache： 123456789101112private void setStatementCache( boolean isSelect, boolean flushCache, boolean useCache, Cache cache, MappedStatement.Builder statementBuilder) &#123; flushCache = valueOrDefault(flushCache, !isSelect); useCache = valueOrDefault(useCache, isSelect); statementBuilder.flushCacheRequired(flushCache); statementBuilder.useCache(useCache); statementBuilder.cache(cache);&#125; 最终mapper配置文件中的&lt;cache/&gt;被设置到了XMLMapperBuilder的builderAssistant属性中，XMLMapperBuilder中使用XMLStatementBuilder遍历CRUD节点，遍历CRUD节点的时候将这个cache节点设置到这些CRUD节点中，这个cache就是所谓的二级缓存！ 接下来我们回过头来看查询的源码，CachingExecutor的query方法： 进入TransactionalCacheManager的putObject方法：12345678910111213public void putObject(Cache cache, CacheKey key, Object value) &#123; getTransactionalCache(cache).putObject(key, value);&#125;private TransactionalCache getTransactionalCache(Cache cache) &#123; TransactionalCache txCache = transactionalCaches.get(cache); if (txCache == null) &#123; txCache = new TransactionalCache(cache); transactionalCaches.put(cache, txCache); &#125; return txCache; &#125; TransactionalCache的putObject方法：1234public void putObject(Object key, Object object) &#123; entriesToRemoveOnCommit.remove(key); entriesToAddOnCommit.put(key, new AddEntry(delegate, key, object)); &#125; 我们看到，数据被加入到了entriesToAddOnCommit中，这个entriesToAddOnCommit是什么东西呢，它是TransactionalCache的一个Map属性：1private Map&lt;Object, AddEntry&gt; entriesToAddOnCommit; AddEntry是TransactionalCache内部的一个类：123456789101112131415private static class AddEntry &#123; private Cache cache; private Object key; private Object value; public AddEntry(Cache cache, Object key, Object value) &#123; this.cache = cache; this.key = key; this.value = value; &#125; public void commit() &#123; cache.putObject(key, value); &#125;&#125; 好了，现在我们发现使用二级缓存之后：查询数据的话，先从二级缓存中拿数据，如果没有的话，去一级缓存中拿，一级缓存也没有的话再查询数据库。有了数据之后在丢到TransactionalCache这个对象的entriesToAddOnCommit属性中。 接下来我们来验证为什么SqlSession commit或close之后，二级缓存才会生效这个问题。 DefaultSqlSession的commit方法：12345678910public void commit(boolean force) &#123; try &#123; executor.commit(isCommitOrRollbackRequired(force)); dirty = false; &#125; catch (Exception e) &#123; throw ExceptionFactory.wrapException("Error committing transaction. Cause: " + e, e); &#125; finally &#123; ErrorContext.instance().reset(); &#125; &#125; CachingExecutor的commit方法：12345public void commit(boolean required) throws SQLException &#123; delegate.commit(required); tcm.commit(); dirty = false; &#125; tcm.commit即 TransactionalCacheManager的commit方法：12345public void commit() &#123; for (TransactionalCache txCache : transactionalCaches.values()) &#123; txCache.commit(); &#125; &#125; TransactionalCache的commit方法：123456789101112131415161718public void commit() &#123; delegate.getReadWriteLock().writeLock().lock(); try &#123; if (clearOnCommit) &#123; delegate.clear(); &#125; else &#123; for (RemoveEntry entry : entriesToRemoveOnCommit.values()) &#123; entry.commit(); &#125; &#125; for (AddEntry entry : entriesToAddOnCommit.values()) &#123; entry.commit(); &#125; reset(); &#125; finally &#123; delegate.getReadWriteLock().writeLock().unlock(); &#125; &#125; 发现调用了AddEntry的commit方法：123public void commit() &#123; cache.putObject(key, value); &#125; 发现了！ AddEntry的commit方法会把数据丢到cache中，也就是丢到二级缓存中！ 关于为何调用close方法后，二级缓存才会生效，因为close方法内部会调用commit方法。本文就不具体说了。 读者有兴趣的话看一看源码就知道为什么了。 其他Cache接口简介org.apache.ibatis.cache.Cache是MyBatis的缓存接口，想要实现自定义的缓存需要实现这个接口。 MyBatis中关于Cache接口的实现类也使用了装饰者设计模式。 我们看下它的一些实现类： 简单说明： LRU – 最近最少使用的:移除最长时间不被使用的对象。 FIFO – 先进先出:按对象进入缓存的顺序来移除它们。 SOFT – 软引用:移除基于垃圾回收器状态和软引用规则的对象。 WEAK – 弱引用:更积极地移除基于垃圾收集器状态和弱引用规则的对象。12345&lt;cache eviction="FIFO" flushInterval="60000" size="512" readOnly="true"/&gt; 可以通过cache节点的eviction属性设置，也可以设置其他的属性。 cache-ref节点mapper配置文件中还可以加入cache-ref节点，它有个属性namespace。 如果每个mapper文件都是用cache-ref，且namespace都一样，那么就代表着真正意义上的全局缓存。 如果只用了cache节点，那仅代表这个这个mapper内部的查询被缓存了，其他mapper文件的不起作用，这并不是所谓的全局缓存。 总结总体来说，MyBatis的源码看起来还是比较轻松的，本文从实践和源码方面深入分析了MyBatis的缓存原理，希望对读者有帮助。]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>mybatis</tag>
        <tag>cache</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[架构设计之「服务限流」]]></title>
    <url>%2F2018%2F09%2F30%2F%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1%E4%B9%8B%E3%80%8C%E6%9C%8D%E5%8A%A1%E9%99%90%E6%B5%81%E3%80%8D%2F</url>
    <content type="text"><![CDATA[&emsp;&emsp;限流可以认为服务降级的一种，限流就是限制系统的输入和输出流量已达到保护系统的目的。一般来说系统的吞吐量是可以被测算的，为了保证系统的稳定运行，一旦达到的需要限制的阈值，就需要限制流量并采取一些措施以完成限制流量的目的。比如：延迟处理，拒绝处理，或者部分拒绝处理等等。 &emsp;&emsp;在介绍限流概念之前，我们先来聊聊身边有哪些限流，如果有在帝都的码农估计对限流是最深有感触的，帝都但凡开个XXX会议，各大地铁站都会限流。 &emsp;&emsp;每年的双11都是剁手族的天堂，11月11号0点0几秒的时候，下面这些场景或许你曾经遇到过。 当然，这几年双11各大电商对并发的支持做的越来越好，这里只是借鉴双11刚推出之际，常常需要应对的一些问题。 通过这两个场景，基本上服务限流的作用也就明白： 「服务限流」其实是指当系统资源不够，不足以应对大量请求，即系统资源与访问量出现矛盾的时候，我们为了保证有限的资源能够正常服务，因此对系统按照预设的规则进行流量限制或功能限制的一种方法。 一、为什么要做服务限流设计？再举一个我们生活中的例子：一些热门的旅游景点，往往会对每日的旅游参观人数有严格的限制，比如厦门的鼓浪屿、北京的故宫等，每天只会卖出固定数目的门票，如果你去的晚了，可能当天的票就已经卖完了，当天就无法进去游玩了。 为什么旅游景点要做这样的限制呢？多卖一些门票多赚一些钱岂不是更好？ 其实对于旅游景点而言，她们也很无奈，因为景点的服务资源有限嘛，每日能服务的人数是有限的，一旦放开限制了，景点的工作人员就会不够用，卫生情况也得不到保障，安全也有隐患，超密集的人群也会严重的影响游客的体验。但由于景区名气大，来游玩的旅客络绎不绝，远超出了景区的承载能力，因此景区只好做出限制每日人员流量的举措。 同理，在IT软件行业中，系统服务也是这样的。 如果你的系统理论是时间单位内可服务100W用户，但是今天却突然来了300W用户，由于用户流量的随机性，如果不加以限流，很有可能这300W用户一下子就压垮了系统，导致所有人都得不到服务。 因此为了保证系统至少还能为100W用户提供正常服务，我们需要对系统进行限流设计。 有的人可能会想，既然会有300W用户来访问，那为啥系统不干脆设计成能足以支撑这么大量用户的集群呢？ 这是个好问题。如果系统是长期有300W的用户来访问，肯定是要做上述升级的，但是常常面临的情况是，系统的日常访问量就是100W，只不过偶尔有一些不可预知的特定原因导致的短时间的流量激增，这个时候，公司往往出于节约成本的考虑，不会为了一个不常见的尖峰来把我们的系统扩容到最大的尺寸。 二、服务限流应该怎么做？对系统服务进行限流，一般有如下几个模式： 熔断：这个模式是需要系统在设计之初，就要把熔断措施考虑进去。当系统出现问题时，如果短时间内无法修复，系统要自动做出判断，开启熔断开关，拒绝流量访问，避免大流量对后端的过载请求。系统也应该能够动态监测后端程序的修复情况，当程序已恢复稳定时，可以关闭熔断开关，恢复正常服务。 服务降级：将系统的所有功能服务进行一个分级，当系统出现问题，需要紧急限流时，可将不是那么重要的功能进行降级处理，停止服务，这样可以释放出更多的资源供给核心功能的去用。例如在电商平台中，如果突发流量激增，可临时将商品评论、积分等非核心功能进行降级，停止这些服务，释放出机器和CPU等资源来保障用户正常下单，而这些降级的功能服务可以等整个系统恢复正常后，再来启动，进行补单/补偿处理。除了功能降级以外，还可以采用不直接操作数据库，而全部读缓存、写缓存的方式作为临时降级方案。 延迟处理：这个模式需要在系统的前端设置一个流量缓冲池，将所有的请求全部缓冲进这个池子，不立即处理。然后后端真正的业务处理程序从这个池子中取出请求依次处理，常见的可以用队列模式来实现。这就相当于用异步的方式去减少了后端的处理压力，但是当流量较大时，后端的处理能力有限，缓冲池里的请求可能处理不及时，会有一定程度延迟。 特权处理：这个模式需要将用户进行分类，通过预设的分类，让系统优先处理需要高保障的用户群体，其它用户群的请求就会延迟处理或者直接不处理。 那在实际项目中，对访问流量的限制，可采用如下几种技术方法： 熔断技术熔断的技术可以重点参考Netflix的开源组件hystrix的做法，主要有三个模块：熔断请求判断算法、熔断恢复机制、熔断报警。 计数器方法系统维护一个计数器，来一个请求就加1，请求处理完成就减1，当计数器大于指定的阈值，就拒绝新的请求。基于这个简单的方法，可以再延伸出一些高级功能，比如阈值可以不是固定值，是动态调整的。另外，还可以有多组计数器分别管理不同的服务，以保证互不影响等。 队列方法就是基于FIFO队列，所有请求都进入队列，后端程序从队列中取出待处理的请求依次处理。基于队列的方法，也可以延伸出更多的玩法来，比如可以设置多个队列以配置不同的优先级。 令牌桶方法首先还是要基于一个队列，请求放到队列里面。但除了队列以外，还要设置一个令牌桶，另外有一个脚本以持续恒定的速度往令牌桶里面放令牌，后端处理程序每处理一个请求就必须从桶里拿出一个令牌，如果令牌拿完了，那就不能处理请求了。我们可以控制脚本放令牌的速度来达到控制后端处理的速度，以实现动态流控。 三、服务限流的注意事项我们在做服务限流的时候，还是有一些原则和事项需要注意的： 实时监控：系统必须要做好全链路的实时监控，才能保证限流的及时检测和处理。 手动开关：除系统自动限流以外，还需要有能手动控制的开关，以保证随时都可以人工介入。 限流的性能：限流的功能理论上是会在一定程度影响到业务正常性能的，因此需要做到限流的性能优化和控制。 系统故障常常都是不可预测且难以避免的，因此作为系统设计师的我们，必须要提前预设各种措施，以应对随时可能的系统风险。 作者：奎哥 &emsp;&emsp;公众号：不止思考]]></content>
      <categories>
        <category>架构设计</category>
      </categories>
      <tags>
        <tag>高并发</tag>
        <tag>限流</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[单点登录原理与简单实现]]></title>
    <url>%2F2018%2F09%2F30%2F%E5%8D%95%E7%82%B9%E7%99%BB%E5%BD%95%E5%8E%9F%E7%90%86%E4%B8%8E%E7%AE%80%E5%8D%95%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[一、单系统登录机制 1、http无状态协议&emsp;&emsp;web应用采用browser/server架构，http作为通信协议。http是无状态协议，浏览器的每一次请求，服务器会独立处理，不与之前或之后的请求产生关联，这个过程用下图说明，三次请求/响应对之间没有任何联系 &emsp;&emsp;但这也同时意味着，任何用户都能通过浏览器访问服务器资源，如果想保护服务器的某些资源，必须限制浏览器请求；要限制浏览器请求，必须鉴别浏览器请求，响应合法请求，忽略非法请求；要鉴别浏览器请求，必须清楚浏览器请求状态。既然http协议无状态，那就让服务器和浏览器共同维护一个状态吧！这就是会话机制 2、会话机制&emsp;&emsp;浏览器第一次请求服务器，服务器创建一个会话，并将会话的id作为响应的一部分发送给浏览器，浏览器存储会话id，并在后续第二次和第三次请求中带上会话id，服务器取得请求中的会话id就知道是不是同一个用户了，这个过程用下图说明，后续请求与第一次请求产生了关联 &emsp;&emsp;服务器在内存中保存会话对象，浏览器怎么保存会话id呢？你可能会想到两种方式 请求参数 cookie &emsp;&emsp;将会话id作为每一个请求的参数，服务器接收请求自然能解析参数获得会话id，并借此判断是否来自同一会话，很明显，这种方式不靠谱。那就浏览器自己来维护这个会话id吧，每次发送http请求时浏览器自动发送会话id，cookie机制正好用来做这件事。cookie是浏览器用来存储少量数据的一种机制，数据以”key/value“形式存储，浏览器发送http请求时自动附带cookie信息 &emsp;&emsp;tomcat会话机制当然也实现了cookie，访问tomcat服务器时，浏览器中可以看到一个名为“JSESSIONID”的cookie，这就是tomcat会话机制维护的会话id，使用了cookie的请求响应过程如下图 3、登录状态&emsp;&emsp;有了会话机制，登录状态就好明白了，我们假设浏览器第一次请求服务器需要输入用户名与密码验证身份，服务器拿到用户名密码去数据库比对，正确的话说明当前持有这个会话的用户是合法用户，应该将这个会话标记为“已授权”或者“已登录”等等之类的状态，既然是会话的状态，自然要保存在会话对象中，tomcat在会话对象中设置登录状态如下 12HttpSession session = request.getSession();session.setAttribute("isLogin", true); &emsp;&emsp;用户再次访问时，tomcat在会话对象中查看登录状态 12HttpSession session = request.getSession();session.getAttribute("isLogin"); &emsp;&emsp;实现了登录状态的浏览器请求服务器模型如下图描述 &emsp;&emsp;每次请求受保护资源时都会检查会话对象中的登录状态，只有 isLogin=true 的会话才能访问，登录机制因此而实现。 二、多系统的复杂性 &emsp;&emsp;web系统早已从久远的单系统发展成为如今由多系统组成的应用群，面对如此众多的系统，用户难道要一个一个登录、然后一个一个注销吗？就像下图描述的这样 &emsp;&emsp;web系统由单系统发展成多系统组成的应用群，复杂性应该由系统内部承担，而不是用户。无论web系统内部多么复杂，对用户而言，都是一个统一的整体，也就是说，用户访问web系统的整个应用群与访问单个系统一样，登录/注销只要一次就够了 &emsp;&emsp;虽然单系统的登录解决方案很完美，但对于多系统应用群已经不再适用了，为什么呢？ &emsp;&emsp;单系统登录解决方案的核心是cookie，cookie携带会话id在浏览器与服务器之间维护会话状态。但cookie是有限制的，这个限制就是cookie的域（通常对应网站的域名），浏览器发送http请求时会自动携带与该域匹配的cookie，而不是所有cookie &emsp;&emsp;既然这样，为什么不将web应用群中所有子系统的域名统一在一个顶级域名下，例如“*.baidu.com”，然后将它们的cookie域设置为“baidu.com”，这种做法理论上是可以的，甚至早期很多多系统登录就采用这种同域名共享cookie的方式。 &emsp;&emsp;然而，可行并不代表好，共享cookie的方式存在众多局限。首先，应用群域名得统一；其次，应用群各系统使用的技术（至少是web服务器）要相同，不然cookie的key值（tomcat为JSESSIONID）不同，无法维持会话，共享cookie的方式是无法实现跨语言技术平台登录的，比如java、php、.net系统之间；第三，cookie本身不安全。 &emsp;&emsp;因此，我们需要一种全新的登录方式来实现多系统应用群的登录，这就是单点登录 三、单点登录 &emsp;&emsp;什么是单点登录？单点登录全称Single Sign On（以下简称SSO），是指在多系统应用群中登录一个系统，便可在其他所有系统中得到授权而无需再次登录，包括单点登录与单点注销两部分 1、登录&emsp;&emsp;相比于单系统登录，sso需要一个独立的认证中心，只有认证中心能接受用户的用户名密码等安全信息，其他系统不提供登录入口，只接受认证中心的间接授权。间接授权通过令牌实现，sso认证中心验证用户的用户名密码没问题，创建授权令牌，在接下来的跳转过程中，授权令牌作为参数发送给各个子系统，子系统拿到令牌，即得到了授权，可以借此创建局部会话，局部会话登录方式与单系统的登录方式相同。这个过程，也就是单点登录的原理，用下图说明 下面对上图简要描述 用户访问系统1的受保护资源，系统1发现用户未登录，跳转至sso认证中心，并将自己的地址作为参数 sso认证中心发现用户未登录，将用户引导至登录页面 用户输入用户名密码提交登录申请 sso认证中心校验用户信息，创建用户与sso认证中心之间的会话，称为全局会话，同时创建授权令牌 sso认证中心带着令牌跳转会最初的请求地址（系统1） 系统1拿到令牌，去sso认证中心校验令牌是否有效 sso认证中心校验令牌，返回有效，注册系统1 系统1使用该令牌创建与用户的会话，称为局部会话，返回受保护资源 用户访问系统2的受保护资源 系统2发现用户未登录，跳转至sso认证中心，并将自己的地址作为参数 sso认证中心发现用户已登录，跳转回系统2的地址，并附上令牌 系统2拿到令牌，去sso认证中心校验令牌是否有效 sso认证中心校验令牌，返回有效，注册系统2 系统2使用该令牌创建与用户的局部会话，返回受保护资源 &emsp;&emsp;用户登录成功之后，会与sso认证中心及各个子系统建立会话，用户与sso认证中心建立的会话称为全局会话，用户与各个子系统建立的会话称为局部会话，局部会话建立之后，用户访问子系统受保护资源将不再通过sso认证中心，全局会话与局部会话有如下约束关系 局部会话存在，全局会话一定存在 全局会话存在，局部会话不一定存在 全局会话销毁，局部会话必须销毁 &emsp;&emsp;你可以通过博客园、百度、csdn、淘宝等网站的登录过程加深对单点登录的理解，注意观察登录过程中的跳转url与参数 2、注销&emsp;&emsp;单点登录自然也要单点注销，在一个子系统中注销，所有子系统的会话都将被销毁，用下面的图来说明 &emsp;&emsp;sso认证中心一直监听全局会话的状态，一旦全局会话销毁，监听器将通知所有注册系统执行注销操作 下面对上图简要说明 用户向系统1发起注销请求 系统1根据用户与系统1建立的会话id拿到令牌，向sso认证中心发起注销请求 sso认证中心校验令牌有效，销毁全局会话，同时取出所有用此令牌注册的系统地址 sso认证中心向所有注册系统发起注销请求 各注册系统接收sso认证中心的注销请求，销毁局部会话 sso认证中心引导用户至登录页面 四、部署图&emsp;&emsp;单点登录涉及sso认证中心与众子系统，子系统与sso认证中心需要通信以交换令牌、校验令牌及发起注销请求，因而子系统必须集成sso的客户端，sso认证中心则是sso服务端，整个单点登录过程实质是sso客户端与服务端通信的过程，用下图描述 &emsp;&emsp;sso认证中心与sso客户端通信方式有多种，这里以简单好用的httpClient为例，web service、rpc、restful api都可以 五、实现&emsp;&emsp;只是简要介绍下基于java的实现过程，不提供完整源码，明白了原理，我相信你们可以自己实现。sso采用客户端/服务端架构，我们先看sso-client与sso-server要实现的功能（下面：sso认证中心=sso-server） sso-client 拦截子系统未登录用户请求，跳转至sso认证中心 接收并存储sso认证中心发送的令牌 与sso-server通信，校验令牌的有效性 建立局部会话 拦截用户注销请求，向sso认证中心发送注销请求 接收sso认证中心发出的注销请求，销毁局部会话 sso-server 验证用户的登录信息 创建全局会话 创建授权令牌 与sso-client通信发送令牌 校验sso-client令牌有效性 系统注册 接收sso-client注销请求，注销所有会话 &emsp;&emsp;接下来，我们按照原理来一步步实现sso吧！ 1、sso-client拦截未登录请求&emsp;&emsp;java拦截请求的方式有servlet、filter、listener三种方式，我们采用filter。在sso-client中新建LoginFilter.java类并实现Filter接口，在doFilter()方法中加入对未登录用户的拦截 123456789101112public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) throws IOException, ServletException &#123; HttpServletRequest req = (HttpServletRequest) request; HttpServletResponse res = (HttpServletResponse) response; HttpSession session = req.getSession(); if (session.getAttribute("isLogin")) &#123; chain.doFilter(request, response); return; &#125; //跳转至sso认证中心 res.sendRedirect("sso-server-url-with-system-url");&#125; 2、sso-server拦截未登录请求&emsp;&emsp;拦截从sso-client跳转至sso认证中心的未登录请求，跳转至登录页面，这个过程与sso-client完全一样 3、sso-server验证用户登录信息&emsp;&emsp;用户在登录页面输入用户名密码，请求登录，sso认证中心校验用户信息，校验成功，将会话状态标记为“已登录” 123456@RequestMapping("/login")public String login(String username, String password, HttpServletRequest req) &#123; this.checkLoginInfo(username, password); req.getSession().setAttribute("isLogin", true); return "success";&#125; 4、sso-server创建授权令牌&emsp;&emsp;授权令牌是一串随机字符，以什么样的方式生成都没有关系，只要不重复、不易伪造即可，下面是一个例子 1String token = UUID.randomUUID().toString(); 5、sso-client取得令牌并校验&emsp;&emsp;sso认证中心登录后，跳转回子系统并附上令牌，子系统（sso-client）取得令牌，然后去sso认证中心校验，在LoginFilter.java的doFilter()中添加几行 1234567891011// 请求附带token参数String token = req.getParameter("token");if (token != null) &#123; // 去sso认证中心校验token boolean verifyResult = this.verify("sso-server-verify-url", token); if (!verifyResult) &#123; res.sendRedirect("sso-server-url"); return; &#125; chain.doFilter(request, response);&#125; &emsp;&emsp;verify()方法使用httpClient实现，这里仅简略介绍，httpClient详细使用方法请参考官方文档 12HttpPost httpPost = new HttpPost("sso-server-verify-url-with-token");HttpResponse httpResponse = httpClient.execute(httpPost); 6、sso-server接收并处理校验令牌请求&emsp;&emsp;用户在sso认证中心登录成功后，sso-server创建授权令牌并存储该令牌，所以，sso-server对令牌的校验就是去查找这个令牌是否存在以及是否过期，令牌校验成功后sso-server将发送校验请求的系统注册到sso认证中心（就是存储起来的意思） &emsp;&emsp;令牌与注册系统地址通常存储在key-value数据库（如redis）中，redis可以为key设置有效时间也就是令牌的有效期。redis运行在内存中，速度非常快，正好sso-server不需要持久化任何数据。 &emsp;&emsp;令牌与注册系统地址可以用下图描述的结构存储在redis中，可能你会问，为什么要存储这些系统的地址？如果不存储，注销的时候就麻烦了，用户向sso认证中心提交注销请求，sso认证中心注销全局会话，但不知道哪些系统用此全局会话建立了自己的局部会话，也不知道要向哪些子系统发送注销请求注销局部会话 7、sso-client校验令牌成功创建局部会话&emsp;&emsp;令牌校验成功后，sso-client将当前局部会话标记为“已登录”，修改LoginFilter.java，添加几行 123if (verifyResult) &#123; session.setAttribute("isLogin", true);&#125; &emsp;&emsp;sso-client还需将当前会话id与令牌绑定，表示这个会话的登录状态与令牌相关，此关系可以用java的hashmap保存，保存的数据用来处理sso认证中心发来的注销请求 8、注销过程&emsp;&emsp;用户向子系统发送带有“logout”参数的请求（注销请求），sso-client拦截器拦截该请求，向sso认证中心发起注销请求 1234String logout = req.getParameter("logout");if (logout != null) &#123; this.ssoServer.logout(token);&#125; &emsp;&emsp;sso认证中心也用同样的方式识别出sso-client的请求是注销请求（带有“logout”参数），sso认证中心注销全局会话 12345678@RequestMapping("/logout")public String logout(HttpServletRequest req) &#123; HttpSession session = req.getSession(); if (session != null) &#123; session.invalidate();//触发LogoutListener &#125; return "redirect:/";&#125; &emsp;&emsp;sso认证中心有一个全局会话的监听器，一旦全局会话注销，将通知所有注册系统注销 12345678public class LogoutListener implements HttpSessionListener &#123; @Override public void sessionCreated(HttpSessionEvent event) &#123;&#125; @Override public void sessionDestroyed(HttpSessionEvent event) &#123; //通过httpClient向所有注册系统发送注销请求 &#125;&#125; 作者：凌承一 &emsp;&emsp;出处：http://www.cnblogs.com/ywlaker/]]></content>
      <categories>
        <category>架构设计</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>单点登录</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JDK1.8 HashMap源码分析]]></title>
    <url>%2F2018%2F09%2F29%2FJDK1.8-HashMap%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[一、HashMap概述在JDK1.8之前，HashMap采用数组+链表实现，即使用链表处理冲突，同一hash值的节点都存储在一个链表里。但是当位于一个桶中的元素较多，即hash值相等的元素较多时，通过key值依次查找的效率较低。而JDK1.8中，HashMap采用数组+链表+红黑树实现，当链表长度超过阈值(8)时，将链表转换为红黑树，这样大大减少了查找时间。 &emsp;&emsp;下图中代表jdk1.8之前的hashmap结构，左边部分即代表哈希表，也称为哈希数组，数组的每个元素都是一个单链表的头节点，链表是用来解决冲突的，如果不同的key映射到了数组的同一位置处，就将其放入单链表中。 jdk1.8之前hashmap结构图&emsp;&emsp;jdk1.8之前的hashmap都采用上图的结构，都是基于一个数组和多个单链表，hash值冲突的时候，就将对应节点以链表的形式存储。如果在一个链表中查找其中一个节点时，将会花费O（n）的查找时间，会有很大的性能损失。到了jdk1.8，当同一个hash值的节点数不小于8时，不再采用单链表形式存储，而是采用红黑树，如下图所示。jdk1.8 hashmap结构图 上图很形象的展示了HashMap的数据结构（数组+链表+红黑树），桶中的结构可能是链表，也可能是红黑树，红黑树的引入是为了提高效率。 二、涉及到的数据结构：处理hash冲突的链表和红黑树以及位桶1、链表的实现 &emsp;&emsp;Node是HashMap的一个内部类，实现了Map.Entry接口，本质是就是一个映射(键值对)。上图中的每个黑色圆点就是一个Node对象。来看具体代码： 12345678910111213141516171819202122232425262728293031323334353637383940//Node是单向链表，它实现了Map.Entry接口static class Node&lt;k,v&gt; implements Map.Entry&lt;k,v&gt; &#123; final int hash; final K key; V value; Node&lt;k,v&gt; next; //构造函数Hash值 键 值 下一个节点 Node(int hash, K key, V value, Node&lt;k,v&gt; next) &#123; this.hash = hash; this.key = key; this.value = value; this.next = next; &#125; public final K getKey() &#123; return key; &#125; public final V getValue() &#123; return value; &#125; public final String toString() &#123; return key + = + value; &#125; public final int hashCode() &#123; return Objects.hashCode(key) ^ Objects.hashCode(value); &#125; public final V setValue(V newValue) &#123; V oldValue = value; value = newValue; return oldValue; &#125; //判断两个node是否相等,若key和value都相等，返回true。可以与自身比较为true public final boolean equals(Object o) &#123; if (o == this) return true; if (o instanceof Map.Entry) &#123; Map.Entry&lt;!--?,?--&gt; e = (Map.Entry&lt;!--?,?--&gt;)o; if (Objects.equals(key, e.getKey()) &amp;&amp; Objects.equals(value, e.getValue())) return true; &#125; return false; &#125;&#125; &emsp;&emsp;可以看到，node中包含一个next变量，这个就是链表的关键点，hash结果相同的元素就是通过这个next进行关联的。 2、红黑树1234567891011121314151617181920//红黑树static final class TreeNode&lt;k,v&gt; extends LinkedHashMap.Entry&lt;k,v&gt; &#123; TreeNode&lt;k,v&gt; parent; // 父节点 TreeNode&lt;k,v&gt; left; //左子树 TreeNode&lt;k,v&gt; right;//右子树 TreeNode&lt;k,v&gt; prev; // needed to unlink next upon deletion boolean red; //颜色属性 TreeNode(int hash, K key, V val, Node&lt;k,v&gt; next) &#123; super(hash, key, val, next); &#125; //返回当前节点的根节点 final TreeNode&lt;k,v&gt; root() &#123; for (TreeNode&lt;k,v&gt; r = this, p;;) &#123; if ((p = r.parent) == null) return r; r = p; &#125; &#125;&#125; &emsp;&emsp;红黑树比链表多了四个变量，parent父节点、left左节点、right右节点、prev上一个同级节点，红黑树内容较多，不在赘述。 3、位桶1transient Node&lt;k,v&gt;[] table;//存储（位桶）的数组 HashMap类中有一个非常重要的字段，就是 Node[] table，即哈希桶数组，明显它是一个Node的数组。 &emsp;&emsp;有了以上3个数据结构，只要有一点数据结构基础的人，都可以大致联想到HashMap的实现了。首先有一个每个元素都是链表（可能表述不准确）的数组，当添加一个元素（key-value）时，就首先计算元素key的hash值，以此确定插入数组中的位置，但是可能存在同一hash值的元素已经被放在数组同一位置了，这时就添加到同一hash值的元素的后面，他们在数组的同一位置，但是形成了链表，所以说数组存放的是链表。而当链表长度太长时，链表就转换为红黑树，这样大大提高了查找的效率。 三、HashMap源码分析1、类的继承关系1public class HashMap&lt;K,V&gt; extends AbstractMap&lt;K,V&gt; implements Map&lt;K,V&gt;, Cloneable, Serializable &emsp;&emsp;可以看到HashMap继承自父类（AbstractMap），实现了Map、Cloneable、Serializable接口。其中，Map接口定义了一组通用的操作；Cloneable接口则表示可以进行拷贝，在HashMap中，实现的是浅层次拷贝，即对拷贝对象的改变会影响被拷贝的对象；Serializable接口表示HashMap实现了序列化，即可以将HashMap对象保存至本地，之后可以恢复状态。 2、类的属性12345678910111213141516171819202122232425262728public class HashMap&lt;K,V&gt; extends AbstractMap&lt;K,V&gt; implements Map&lt;K,V&gt;, Cloneable, Serializable &#123; // 序列号 private static final long serialVersionUID = 362498820763181265L; // 默认的初始容量是16 static final int DEFAULT_INITIAL_CAPACITY = 1 &lt;&lt; 4; // 最大容量 static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30; // 默认的填充因子 static final float DEFAULT_LOAD_FACTOR = 0.75f; // 当桶(bucket)上的结点数大于这个值时会转成红黑树 static final int TREEIFY_THRESHOLD = 8; // 当桶(bucket)上的结点数小于这个值时树转链表 static final int UNTREEIFY_THRESHOLD = 6; // 桶中结构转化为红黑树对应的table的最小大小 static final int MIN_TREEIFY_CAPACITY = 64; // 存储元素的数组，总是2的幂次倍 transient Node&lt;k,v&gt;[] table; // 存放具体元素的集 transient Set&lt;map.entry&lt;k,v&gt;&gt; entrySet; // 存放元素的个数，注意这个不等于数组的长度。 transient int size; // 每次扩容和更改map结构的计数器 transient int modCount; // 临界值 当实际大小(容量*填充因子)超过临界值时，会进行扩容 int threshold; // 填充因子 final float loadFactor;&#125; 类的数据成员很重要，以上也解释得很详细了。 3、类的构造函数（1）HashMap(int, float)型构造函数1234567891011121314151617public HashMap(int initialCapacity, float loadFactor) &#123; // 初始容量不能小于0，否则报错 if (initialCapacity &lt; 0) throw new IllegalArgumentException("Illegal initial capacity: " + initialCapacity); // 初始容量不能大于最大值，否则为最大值 if (initialCapacity &gt; MAXIMUM_CAPACITY) initialCapacity = MAXIMUM_CAPACITY; // 填充因子不能小于或等于0，不能为非数字 if (loadFactor &lt;= 0 || Float.isNaN(loadFactor)) throw new IllegalArgumentException("Illegal load factor: " + loadFactor); // 初始化填充因子 this.loadFactor = loadFactor; // 初始化threshold大小 this.threshold = tableSizeFor(initialCapacity); &#125; tableSizeFor(initialCapacity)返回大于initialCapacity的最小的二次幂数值。 123456789static final int tableSizeFor(int cap) &#123; int n = cap - 1; n |= n &gt;&gt;&gt; 1; n |= n &gt;&gt;&gt; 2; n |= n &gt;&gt;&gt; 4; n |= n &gt;&gt;&gt; 8; n |= n &gt;&gt;&gt; 16; return (n &lt; 0) ? 1 : (n &gt;= MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : n + 1;&#125; &gt;&gt;&gt; 操作符表示无符号右移，高位取0。 （2）HashMap(int)型构造函数。1234public HashMap(int initialCapacity) &#123; // 调用HashMap(int, float)型构造函数 this(initialCapacity, DEFAULT_LOAD_FACTOR);&#125; （3）HashMap()型构造函数。1234public HashMap() &#123; // 初始化填充因子 this.loadFactor = DEFAULT_LOAD_FACTOR; &#125; （4）HashMap(Map&lt;? extends K&gt;)型构造函数。123456public HashMap(Map&lt;? extends K, ? extends V&gt; m) &#123; // 初始化填充因子 this.loadFactor = DEFAULT_LOAD_FACTOR; // 将m中的所有元素添加至HashMap中 putMapEntries(m, false);&#125; putMapEntries(Map&lt;? extends K, ? extends V&gt; m, boolean evict)函数将m的所有元素存入本HashMap实例中。 123456789101112131415161718192021222324final void putMapEntries(Map&lt;? extends K, ? extends V&gt; m, boolean evict) &#123; int s = m.size(); if (s &gt; 0) &#123; // 判断table是否已经初始化 if (table == null) &#123; // pre-size // 未初始化，s为m的实际元素个数 float ft = ((float)s / loadFactor) + 1.0F; int t = ((ft &lt; (float)MAXIMUM_CAPACITY) ? (int)ft : MAXIMUM_CAPACITY); // 计算得到的t大于阈值，则初始化阈值 if (t &gt; threshold) threshold = tableSizeFor(t); &#125; // 已初始化，并且m元素个数大于阈值，进行扩容处理 else if (s &gt; threshold) resize(); // 将m中的所有元素添加至HashMap中 for (Map.Entry&lt;? extends K, ? extends V&gt; e : m.entrySet()) &#123; K key = e.getKey(); V value = e.getValue(); putVal(hash(key), key, value, false, evict); &#125; &#125;&#125; 4、hash算法在JDK 1.8中，hash方法如下： 1234static final int hash(Object key) &#123; int h; return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16);&#125; （1）首先获取对象的hashCode()值，然后将hashCode值右移16位，然后将右移后的值与原来的hashCode做异或运算，返回结果。（其中h&gt;&gt;&gt;16，在JDK1.8中，优化了高位运算的算法，使用了零扩展，无论正数还是负数，都在高位插入0）。 （2）在putVal源码中，我们通过(n-1)&amp;hash获取该对象的键在hashmap中的位置。（其中hash的值就是（1）中获得的值）其中n表示的是hash桶数组的长度，并且该长度为2的n次方，这样(n-1)&amp;hash就等价于hash%n。因为&amp;运算的效率高于%运算。 12345678final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) &#123; ... if ((p = tab[i = (n - 1) &amp; hash]) == null)//获取位置 tab[i] = newNode(hash, key, value, null); ...&#125; &emsp;&emsp;tab即是table，n是map集合的容量大小，hash是上面方法的返回值。因为通常声明map集合时不会指定大小，或者初始化的时候就创建一个容量很大的map对象，所以这个通过容量大小与key值进行hash的算法在开始的时候只会对低位进行计算，虽然容量的2进制高位一开始都是0，但是key的2进制高位通常是有值的，因此先在hash方法中将key的hashCode右移16位在与自身异或，使得高位也可以参与hash，更大程度上减少了碰撞率。 下面举例说明下，n为table的长度。 5、重要方法分析（1）putVal方法&emsp;&emsp;首先说明，HashMap并没有直接提供putVal接口给用户调用，而是提供的put方法，而put方法就是通过putVal来插入元素的。 1234public V put(K key, V value) &#123; // 对key的hashCode()做hash return putVal(hash(key), key, value, false, true); &#125; putVal方法执行过程可以通过下图来理解： ①.判断键值对数组table[i]是否为空或为null，否则执行resize()进行扩容； ②.根据键值key计算hash值得到插入的数组索引i，如果table[i]==null，直接新建节点添加，转向⑥，如果table[i]不为空，转向③； ③.判断table[i]的首个元素是否和key一样，如果相同直接覆盖value，否则转向④，这里的相同指的是hashCode以及equals； ④.判断table[i] 是否为treeNode，即table[i] 是否是红黑树，如果是红黑树，则直接在树中插入键值对，否则转向⑤； ⑤.遍历table[i]，判断链表长度是否大于8，大于8的话把链表转换为红黑树，在红黑树中执行插入操作，否则进行链表的插入操作；遍历过程中若发现key已经存在直接覆盖value即可； ⑥.插入成功后，判断实际存在的键值对数量size是否超多了最大容量threshold，如果超过，进行扩容。 具体源码如下：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; // 步骤①：tab为空则创建 // table未初始化或者长度为0，进行扩容 if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; // 步骤②：计算index，并对null做处理 // (n - 1) &amp; hash 确定元素存放在哪个桶中，桶为空，新生成结点放入桶中(此时，这个结点是放在数组中) if ((p = tab[i = (n - 1) &amp; hash]) == null) tab[i] = newNode(hash, key, value, null); // 桶中已经存在元素 else &#123; Node&lt;K,V&gt; e; K k; // 步骤③：节点key存在，直接覆盖value // 比较桶中第一个元素(数组中的结点)的hash值相等，key相等 if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) // 将第一个元素赋值给e，用e来记录 e = p; // 步骤④：判断该链为红黑树 // hash值不相等，即key不相等；为红黑树结点 else if (p instanceof TreeNode) // 放入树中 e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); // 步骤⑤：该链为链表 // 为链表结点 else &#123; // 在链表最末插入结点 for (int binCount = 0; ; ++binCount) &#123; // 到达链表的尾部 if ((e = p.next) == null) &#123; // 在尾部插入新结点 p.next = newNode(hash, key, value, null); // 结点数量达到阈值，转化为红黑树 if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st treeifyBin(tab, hash); // 跳出循环 break; &#125; // 判断链表中结点的key值与插入的元素的key值是否相等 if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) // 相等，跳出循环 break; // 用于遍历桶中的链表，与前面的e = p.next组合，可以遍历链表 p = e; &#125; &#125; // 表示在桶中找到key值、hash值与插入元素相等的结点 if (e != null) &#123; // 记录e的value V oldValue = e.value; // onlyIfAbsent为false或者旧值为null if (!onlyIfAbsent || oldValue == null) //用新值替换旧值 e.value = value; // 访问后回调 afterNodeAccess(e); // 返回旧值 return oldValue; &#125; &#125; // 结构性修改 ++modCount; // 步骤⑥：超过最大容量 就扩容 // 实际大小大于阈值则扩容 if (++size &gt; threshold) resize(); // 插入后回调 afterNodeInsertion(evict); return null;&#125; HashMap的数据存储实现原理（1）流程：1.根据key计算得到key.hash = (h = k.hashCode()) ^ (h &gt;&gt;&gt; 16)； 2.根据key.hash计算得到桶数组的索引index = key.hash &amp; (table.length - 1)，这样就找到该key的存放位置了： ① 如果该位置没有数据，用该数据新生成一个节点保存新数据，返回null； ② 如果该位置有数据是一个红黑树，那么执行相应的插入 / 更新操作； ③ 如果该位置有数据是一个链表，分两种情况一是该链表没有这个节点，另一个是该链表上有这个节点，注意这里判断的依据是key.hash是否一样： 如果该链表没有这个节点，那么采用尾插法新增节点保存新数据，返回null；如果该链表已经有这个节点了，那么找到该节点并更新新数据，返回老数据。 注意： HashMap的put会返回key的上一次保存的数据，比如：1234HashMap&lt;String, String&gt; map = new HashMap&lt;String, String&gt;(); System.out.println(map.put("a", "A")); // 打印null System.out.println(map.put("a", "AA")); // 打印A System.out.println(map.put("a", "AB")); // 打印AA （2）getNode方法说明：HashMap同样并没有直接提供getNode接口给用户调用，而是提供的get方法，而get方法就是通过getNode来取得元素的。1234public V get(Object key) &#123; Node&lt;k,v&gt; e; return (e = getNode(hash(key), key)) == null ? null : e.value;&#125; 12345678910111213141516171819202122232425final Node&lt;K,V&gt; getNode(int hash, Object key) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; first, e; int n; K k; // table已经初始化，长度大于0，根据hash寻找table中的项也不为空 if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (first = tab[(n - 1) &amp; hash]) != null) &#123; // 桶中第一项(数组元素)相等 if (first.hash == hash &amp;&amp; // always check first node ((k = first.key) == key || (key != null &amp;&amp; key.equals(k)))) return first; // 桶中不止一个结点 if ((e = first.next) != null) &#123; // 为红黑树结点 if (first instanceof TreeNode) // 在红黑树中查找 return ((TreeNode&lt;K,V&gt;)first).getTreeNode(hash, key); // 否则，在链表中查找 do &#123; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) return e; &#125; while ((e = e.next) != null); &#125; &#125; return null;&#125; （3）resize方法①.在jdk1.8中，resize方法是在hashmap中的键值对大于阀值时或者初始化时，就调用resize方法进行扩容； ②.每次扩展的时候，都是扩展2倍； ③.扩展后Node对象的位置要么在原位置，要么移动到原偏移量两倍的位置。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273final Node&lt;K,V&gt;[] resize() &#123; Node&lt;K,V&gt;[] oldTab = table;//oldTab指向hash桶数组 int oldCap = (oldTab == null) ? 0 : oldTab.length; int oldThr = threshold; int newCap, newThr = 0; if (oldCap &gt; 0) &#123;//如果oldCap不为空的话，就是hash桶数组不为空 if (oldCap &gt;= MAXIMUM_CAPACITY) &#123;//如果大于最大容量了，就赋值为整数最大的阀值 threshold = Integer.MAX_VALUE; return oldTab;//返回 &#125;//如果当前hash桶数组的长度在扩容后仍然小于最大容量 并且oldCap大于默认值16 else if ((newCap = oldCap &lt;&lt; 1) &lt; MAXIMUM_CAPACITY &amp;&amp; oldCap &gt;= DEFAULT_INITIAL_CAPACITY) newThr = oldThr &lt;&lt; 1; // double threshold 双倍扩容阀值threshold &#125; else if (oldThr &gt; 0) // initial capacity was placed in threshold newCap = oldThr; else &#123; // zero initial threshold signifies using defaults newCap = DEFAULT_INITIAL_CAPACITY; newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY); &#125; if (newThr == 0) &#123; float ft = (float)newCap * loadFactor; newThr = (newCap &lt; MAXIMUM_CAPACITY &amp;&amp; ft &lt; (float)MAXIMUM_CAPACITY ? (int)ft : Integer.MAX_VALUE); &#125; threshold = newThr; @SuppressWarnings(&#123;"rawtypes","unchecked"&#125;) Node&lt;K,V&gt;[] newTab = (Node&lt;K,V&gt;[])new Node[newCap];//新建hash桶数组 table = newTab;//将新数组的值复制给旧的hash桶数组 if (oldTab != null) &#123;//进行扩容操作，复制Node对象值到新的hash桶数组 for (int j = 0; j &lt; oldCap; ++j) &#123; Node&lt;K,V&gt; e; if ((e = oldTab[j]) != null) &#123;//如果旧的hash桶数组在j结点处不为空，复制给e oldTab[j] = null;//将旧的hash桶数组在j结点处设置为空，方便gc if (e.next == null)//如果e后面没有Node结点 newTab[e.hash &amp; (newCap - 1)] = e;//直接对e的hash值对新的数组长度求模获得存储位置 else if (e instanceof TreeNode)//如果e是红黑树的类型，那么添加到红黑树中 ((TreeNode&lt;K,V&gt;)e).split(this, newTab, j, oldCap); else &#123; // preserve order Node&lt;K,V&gt; loHead = null, loTail = null; Node&lt;K,V&gt; hiHead = null, hiTail = null; Node&lt;K,V&gt; next; do &#123; next = e.next;//将Node结点的next赋值给next if ((e.hash &amp; oldCap) == 0) &#123;//如果结点e的hash值与原hash桶数组的长度作与运算为0 if (loTail == null)//如果loTail为null loHead = e;//将e结点赋值给loHead else loTail.next = e;//否则将e赋值给loTail.next loTail = e;//然后将e复制给loTail &#125; else &#123;//如果结点e的hash值与原hash桶数组的长度作与运算不为0 if (hiTail == null)//如果hiTail为null hiHead = e;//将e赋值给hiHead else hiTail.next = e;//如果hiTail不为空，将e复制给hiTail.next hiTail = e;//将e复制个hiTail &#125; &#125; while ((e = next) != null);//直到e为空 if (loTail != null) &#123;//如果loTail不为空 loTail.next = null;//将loTail.next设置为空 newTab[j] = loHead;//将loHead赋值给新的hash桶数组[j]处 &#125; if (hiTail != null) &#123;//如果hiTail不为空 hiTail.next = null;//将hiTail.next赋值为空 newTab[j + oldCap] = hiHead;//将hiHead赋值给新的hash桶数组[j+旧hash桶数组长度] &#125; &#125; &#125; &#125; &#125; return newTab;&#125; 转自平凡希]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>jdk1.8</tag>
        <tag>HashMap</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[浅谈高性能数据库集群——读写分离]]></title>
    <url>%2F2018%2F09%2F20%2F%E6%B5%85%E8%B0%88%E9%AB%98%E6%80%A7%E8%83%BD%E6%95%B0%E6%8D%AE%E5%BA%93%E9%9B%86%E7%BE%A4%E2%80%94%E2%80%94%E8%AF%BB%E5%86%99%E5%88%86%E7%A6%BB%2F</url>
    <content type="text"><![CDATA[1 读写分离概述 读写分离概述 基本架构图： 基本架构图 2 适用场景 适用场景 读写分离不是银弹，并不是一有性能问题就上读写分离，而是应该先优化，例如优化慢查询，调整不合理的业务逻辑，引入缓存查询等只有确定系统没有优化空间后才考虑读写分离集群 3 引入的系统复杂度问题问题一 主从复制延迟 主从复制延迟 问题二 分配机制如何将读写操作区分开来，然后访问不同的数据库服务器？ 解决方案1 客户端程序代码封装实现基本架构图 程序代码封装实现分配基本架构图 程序代码封装 业界开源实现 Sharding-JDBC定位为轻量级Java框架，在Java的JDBC层提供的额外服务。 它使用客户端直连数据库，以jar包形式提供服务，无需额外部署和依赖，可理解为增强版的JDBC驱动，完全兼容JDBC和各种ORM框架。 Sharding-JDBC基本架构图 淘宝TDDL淘宝根据自身业务需求研发了 TDDL （ Taobao Distributed Data Layer ）框架，主要用于解决 分库分表场景下的访问路由（持久层与数据访问层的配合）以及异构数据库之间的数据同步 ，它是一个基于集中式配置的 JDBC DataSource 实现，具有分库分表、 Master/Salve 、动态数据源配置等功能。 淘宝TDDL基本架构图 解决方案2 服务端中间件封装基本架构图 服务端中间件封装实现分配基本架构图 服务端中间件封装 业界开源实现 MySQL官方推荐的MySQL Router MySQL Router架构图 MySQL Router是轻量级的中间件，可在应用程序和任何后端MySQL服务器之间提供透明路由。它可以用于各种各样的用例，例如通过有效地将数据库流量路由到适当的后端MySQL服务器来提供高可用性和可伸缩性。可插拔架构还使开发人员能够扩展MySQL Router以用于自定义用例。 基于MySQL Router可以实现读写分离，故障自动切换，负载均衡，连接池等功能。 MySQL官方提供的MySQL Proxy MySQL Proxy 360开源的Atlas Atlas架构图形象表示 Atlas总体架构Atlas是由平台部基础架构团队开发维护的一个基于MySQL协议的数据中间层项目。它是在mysql-proxy的基础上，对其进行了优化，增加了一些新的功能特性。 常见的开源数据库中间件对比 功能 Sharding-JDBC TDDL Amoeba Cobar MyCat 基于客户端还是服务端 客户端 客户端 服务端 服务端 服务端 分库分表 有 有 有 有 有 MySQL交互协议 JDBC Driver JDBC Driver 前端用NIO,后端用JDBC Driver 前端用NIO,后端用BIO 前后端均用NIO 支持的数据库 任意 任意 任意 MySQL 任意 参考 Mycat原理解析-Mycat架构分析 作者 陈彩华 文章转载交流请联系 caison@aliyun.com]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>分库分表</tag>
        <tag>高性能</tag>
        <tag>集群</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[浅谈高性能数据库集群——分库分表]]></title>
    <url>%2F2018%2F09%2F20%2F%E6%B5%85%E8%B0%88%E9%AB%98%E6%80%A7%E8%83%BD%E6%95%B0%E6%8D%AE%E5%BA%93%E9%9B%86%E7%BE%A4%E2%80%94%E2%80%94%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[分库分表概述 分库分表概述 读写分离分散数据库读写操作压力，分库分表分散存储压力 适用场景 适用场景 类似读写分离，分库分表也是确定没有其他优化空间之后才采取的优化方案。那如果业务真的发展很快岂不是很快要进行分库分表了？那为何不一开始就设计好呢？ 按照架构设计的“三原则”（简单原则，合适原则，演化原则），简单分析一下： 首先，这里的“如果”事实上发生的概率比较低，做10个业务有一个业务能活下去就很不错了，更何况快速发展，和中彩票的概率差不多。如果我们每个业务上来就按照淘宝、微信的规模去做架构设计，不但会累死自己，还会害死业务。 其次，如果业务真的发展很快，后面进行分库分表也不迟。因为业务发展好，相应的资源投入就会加大，可以投入更多的人和更多的钱，那业务分库带来的代码和业务复杂问题就可以通过加人来解决，成本问题也可以通过增加资金来解决。 业务分库 业务分库示例 业务分表业务分表概述 业务分表拆分方式 带来的问题垂直分表增加表操作的次数 水平分表 路由问题 路由问题 数据库操作问题 数据库操作问题 实现方法 实现方法 类似读写分离，具体实现也是“程序代码封装”和“中间件封装”，但具体实现复杂一些，因为还有要判断SQL中具体操作的表，具体操作(例如count、order by、group by等)，根据具体操作做不同的处理。 参考 从0开始学架构 —— 李运华 《浅谈高性能数据库集群——读写分离》—— 陈彩华 《架构设计方法初探》 —— 陈彩华 《分库分表、主从、读写分离》 作者 陈彩华 文章转载交流请联系 caison@aliyun.com]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>分库分表</tag>
        <tag>高性能</tag>
        <tag>集群</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL不为人知的主键与唯一索引约束]]></title>
    <url>%2F2018%2F09%2F19%2FMySQL%E4%B8%8D%E4%B8%BA%E4%BA%BA%E7%9F%A5%E7%9A%84%E4%B8%BB%E9%94%AE%E4%B8%8E%E5%94%AF%E4%B8%80%E7%B4%A2%E5%BC%95%E7%BA%A6%E6%9D%9F%2F</url>
    <content type="text"><![CDATA[此文摘自微信公众号【架构师之路】微信扫一扫关注该公众号 今天和大家简单聊聊MySQL的约束主键与唯一索引约束： PRIMARY KEY and UNIQUE Index Constraints 文章不长，保证有收获。 触发约束检测的时机： insert update 当检测到违反约束时，不同存储引擎的处理动作是不一样的。 如果存储引擎支持事务，SQL会自动回滚。 例子：create table t1 (id int(10) primary key)engine=innodb;insert into t1 values(1);insert into t1 values(1);其中第二条insert会因为违反约束，而导致回滚。 通常可以使用：show warnings; 来查看违反约束后的错误提示。 如果存储引擎不支持事务，SQL的执行会中断，此时可能会导致后续有符合条件的行不被操作，出现不符合预期的结果。 例子：create table t2 (id int(10) unique)engine=MyISAM;insert into t2 values(1);insert into t2 values(5);insert into t2 values(6);insert into t2 values(10);update t2 set id=id+1; update执行后，猜猜会得到什么结果集？猜想一：2, 6, 7, 11猜想二：1, 5, 6, 10...都不对，正确答案是：2, 5, 6, 10 第一行id=1，加1后，没有违反unique约束，执行成功；第二行id=5，加1后，由于id=6的记录存在，违反uinique约束，SQL终止，修改失败；第三行id=6，第四行id=10便不再执行；画外音：这太操蛋了，一个update语句，部分执行成功，部分执行失败。 为了避免这种情况出现，请使用InnoDB存储引擎，InnoDB在遇到违反约束时，会自动回滚update语句，一行都不会修改成功。 画外音：大家把存储引擎换成InnoDB，把上面的例子再跑一遍，印象更加深刻。 另外，对于insert的约束冲突，可以使用：insert … on duplicate key指出在违反主键或唯一索引约束时，需要进行的额外操作。 例子：create table t3 (id int(10) unique,flag char(10) default ‘true’)engine=MyISAM;insert into t3(id) values(1);insert into t3(id) values(5);insert into t3(id) values(6);insert into t3(id) values(10);insert into t3(id) values(10) on duplicate key update flag=’false’; insert执行后，猜猜会发生什么？ 插入id=10的记录，会违反unique约束，此时执行update flag=’false’，于是有一行记录被update了。 这相当于执行：update t3 set flag=’false’ where id=10; 仔细看，insert的结果返回，提示：Query OK, 2 rows affected有意思么？画外音：本文所有实验，基于MySQL5.6。 总结，对于主键与唯一索引约束： 执行insert和update时，会触发约束检查 InnoDB违反约束时，会回滚对应SQL MyISAM违反约束时，会中断对应的SQL，可能造成不符合预期的结果集 可以使用 insert … on duplicate key 来指定触发约束时的动作 通常使用 show warnings; 来查看与调试违反约束的ERROR 互联网大数据量高并发量业务，为了大家的身心健康，请使用InnoDB。 相关推荐：《业界难题 - 『跨库分页』的四种方案》《InnoDB，为什么并发如此之高？》《InnoDB，快照读，在RR和RC下有何差异？》 了解了几个坑，也是好的，求转。]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>SQL</tag>
        <tag>InnoDB</tag>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo-Next搭建个人博客（Hexo博客备份）]]></title>
    <url>%2F2018%2F09%2F17%2FHexo-Next%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%EF%BC%88Hexo%E5%8D%9A%E5%AE%A2%E5%A4%87%E4%BB%BD%EF%BC%89%2F</url>
    <content type="text"><![CDATA[一、需求：在Windows和Mac下需要对Hexo进行管理和更新，或者进行重新部署环境。 二、思路创建分支，一个分支用来存放Hexo生成的网站原始的文件，另一个分支用来存放生成的静态网页。 三、搭建的流程博客搭建请看我之前的文章 将themes/next/(我用的是NexT主题)中的.git/删除，否则无法将主题文件夹push； 在本地blog文件夹下创建文件.gitignore(一般都自带一个)，打开后写入 1234567.DS_StoreThumbs.dbdb.json*.lognode_modules/public/.deploy*/ 备注：刚开始好多备份的教程都没有说明.gitignore文件，在恢复之后，进行hexo d之后老是报错，发在邮箱里的错误内容 github给邮箱发送的内容，说：有模块没有初始化，在这里所指的就是.deploy_git这个模块 在本地blog文件夹下执行命令 123456789101112#git初始化git init#创建hexo分支，用来存放源码git checkout -b hexo#git 文件添加git add .#git 提交git commit -m "init"#添加远程仓库git remote add origin git@github.com:542869246/542869246.github.io.git#push到hexo分支git push origin hexo 执行hexo d -g生成网站并部署到GitHub上 这样一来，在GitHub上的git@github.com:542869246/542869246.github.io.git仓库就有两个分支，一个hexo分支用来存放网站的原始文件，一个master分支用来存放生成的静态网页。 四、恢复当重装电脑之后，或者想在其他电脑上修改博客，可以使用下列步骤： 1、先安装hexo$ npm install -g hexo-cli 2、存在github上的git clone下来git clone git@github.com:542869246/542869246.github.io.git 3、项目文件夹下npmcd项目名/ npm install –no-bin-links$ npm install hexo-deployer-git 4、重新配置github和coding的公钥 五、更新每次写作之后,可以使用下列步骤：1234hexo d#生成网站并部署到GitHub上git add .git commit -m 'update'git push origin hexo]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>Next</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[业界难题 - 『跨库分页』的四种方案]]></title>
    <url>%2F2018%2F09%2F10%2F%E4%B8%9A%E7%95%8C%E9%9A%BE%E9%A2%98%20-%20%E3%80%8E%E8%B7%A8%E5%BA%93%E5%88%86%E9%A1%B5%E3%80%8F%E7%9A%84%E5%9B%9B%E7%A7%8D%E6%96%B9%E6%A1%88%2F</url>
    <content type="text"><![CDATA[一、需求缘起分页需求 互联网很多业务都有分页拉取数据的需求，例如： （1）微信消息过多时，拉取第N页消息 （2）京东下单过多时，拉取第N页订单 （3）浏览58同城，查看第N页帖子 这些业务场景对应的消息表，订单表，帖子表分页拉取需求有这样一些特点： （1）有一个业务主键id, 例如msg_id, order_id, tiezi_id （2）分页排序是按照非业务主键id来排序的，业务中经常按照时间time来排序order by 在数据量不大时，可以通过在排序字段time上建立索引，利用SQL提供的offset/limit功能就能满足分页查询需求： select * from t_msg order by time offset 200 limit 100 select * from t_order order by time offset 200 limit 100 select * from t_tiezi order by time offset 200 limit 100 此处假设一页数据为100条，均拉取第3页数据。 分库需求 高并发大流量的互联网架构，一般通过服务层来访问数据库，随着数据量的增大，数据库需要进行水平切分，分库后将数据分布到不同的数据库实例（甚至物理机器）上，以达到降低数据量，增加实例数的扩容目的。 一旦涉及分库，逃不开“分库依据”patition key的概念，使用哪一个字段来水平切分数据库呢：大部分的业务场景，会使用业务主键id。 确定了分库依据patition key后，接下来要确定的是分库算法：大部分的业务场景，会使用业务主键id取模的算法来分库，这样即能够保证每个库的数据分布是均匀的，又能够保证每个库的请求分布是均匀的，实在是简单实现负载均衡的好方法，此法在互联网架构中应用颇多。 举一个更具体的例子： 用户库user，水平切分后变为两个库，分库依据patition key是uid，分库算法是uid取模：uid%2余0的数据会落到db0，uid%2余1的数据会落到db1。 问题的提出 仍然是上述用户库的例子，如果业务要查询“最近注册的第3页用户”，该如何实现呢？单库上，可以 select * from t_user order by time offset 200 limit 100 变成两个库后，分库依据是uid，排序依据是time，数据库层失去了time排序的全局视野，数据分布在两个库上，此时该怎么办呢？ 如何满足“跨越多个水平切分数据库，且分库依据与排序依据为不同属性，并需要进行分页”的查询需求，实现 select * from T order by time offset X limit Y的跨库分页SQL，是本文将要讨论的技术问题。 二、全局视野法 如上图所述，服务层通过uid取模将数据分布到两个库上去之后，每个数据库都失去了全局视野，数据按照time局部排序之后，不管哪个分库的第3页数据，都不一定是全局排序的第3页数据。 那到底哪些数据才是全局排序的第3页数据呢，暂且分三种情况讨论。 （1）极端情况，两个库的数据完全一样 如果两个库的数据完全相同，只需要每个库offset一半，再取半页，就是最终想要的数据（如上图中粉色部分数据）。 （2）极端情况，结果数据来自一个库 也可能两个库的数据分布及其不均衡，例如db0的所有数据的time都大于db1的所有数据的time，则可能出现：一个库的第3页数据，就是全局排序后的第3页数据（如上图中粉色部分数据）。 （3）一般情况，每个库数据各包含一部分 正常情况下，全局排序的第3页数据，每个库都会包含一部分（如上图中粉色部分数据）。 由于不清楚到底是哪种情况，所以必须每个库都返回3页数据，所得到的6页数据在服务层进行内存排序，得到数据全局视野，再取第3页数据，便能够得到想要的全局分页数据。 再总结一下这个方案的步骤： （1）将order by time offset X limit Y，改写成order by time offset 0 limit X+Y （2）服务层将改写后的SQL语句发往各个分库：即例子中的各取3页数据 （3）假设共分为N个库，服务层将得到N*(X+Y)条数据：即例子中的6页数据 （4）服务层对得到的N*(X+Y)条数据进行内存排序，内存排序后再取偏移量X后的Y条记录，就是全局视野所需的一页数据 方案优点：通过服务层修改SQL语句，扩大数据召回量，能够得到全局视野，业务无损，精准返回所需数据。 方案缺点（显而易见）： （1）每个分库需要返回更多的数据，增大了网络传输量（耗网络）； （2）除了数据库按照time进行排序，服务层还需要进行二次排序，增大了服务层的计算量（耗CPU）； （3）最致命的，这个算法随着页码的增大，性能会急剧下降，这是因为SQL改写后每个分库要返回X+Y行数据：返回第3页，offset中的X=200；假如要返回第100页，offset中的X=9900，即每个分库要返回100页数据，数据量和排序量都将大增，性能平方级下降。 三、业务折衷法“全局视野法”虽然性能较差，但其业务无损，数据精准，不失为一种方案，有没有性能更优的方案呢？ “任何脱离业务的架构设计都是耍流氓”，技术方案需要折衷，在技术难度较大的情况下，业务需求的折衷能够极大的简化技术方案。 业务折衷一：禁止跳页查询 在数据量很大，翻页数很多的时候，很多产品并不提供“直接跳到指定页面”的功能，而只提供“下一页”的功能，这一个小小的业务折衷，就能极大的降低技术方案的复杂度。 如上图，不够跳页，那么第一次只能够查第一页： （1）将查询order by time offset 0 limit 100，改写成order by time where time&gt;0 limit 100 （2）上述改写和offset 0 limit 100的效果相同，都是每个分库返回了一页数据（上图中粉色部分）； （3）服务层得到2页数据，内存排序，取出前100条数据，作为最终的第一页数据，这个全局的第一页数据，一般来说每个分库都包含一部分数据（如上图粉色部分）； 咦，这个方案也需要服务器内存排序，岂不是和“全局视野法”一样么？第一页数据的拉取确实一样，但每一次“下一页”拉取的方案就不一样了。 点击“下一页”时，需要拉取第二页数据，在第一页数据的基础之上，能够找到第一页数据time的最大值： 这个上一页记录的time_max，会作为第二页数据拉取的查询条件： （1）将查询order by time offset 100 limit 100，改写成order by time where time&gt;$time_max limit 100 （2）这下不是返回2页数据了（“全局视野法，会改写成offset 0 limit 200”），每个分库还是返回一页数据（如上图中粉色部分）； （3）服务层得到2页数据，内存排序，取出前100条数据，作为最终的第2页数据，这个全局的第2页数据，一般来说也是每个分库都包含一部分数据（如上图粉色部分）； 如此往复，查询全局视野第100页数据时，不是将查询条件改写为offset 0 limit 9900+100（返回100页数据），而是改写为time&gt;$time_max99 limit 100（仍返回一页数据），以保证数据的传输量和排序的数据量不会随着不断翻页而导致性能下降。 业务折衷二：允许数据精度损失“全局视野法”能够返回业务无损的精确数据，在查询页数较大，例如第100页时，会有性能问题，此时业务上是否能够接受，返回的100页不是精准的数据，而允许有一些数据偏差呢？ 数据库分库-**数据均衡原理** 使用patition key进行分库，在数据量较大，数据分布足够随机的情况下，各分库所有非patition key属性，在各个分库上的数据分布，统计概率情况是一致的。 例如，在uid随机的情况下，使用uid取模分两库，db0和db1： （1）性别属性，如果db0库上的男性用户占比70%，则db1上男性用户占比也应为70% （2）年龄属性，如果db0库上18-28岁少女用户比例占比15%，则db1上少女用户比例也应为15% （3）时间属性，如果db0库上每天10:00之前登录的用户占比为20%，则db1上应该是相同的统计规律 … 利用这一原理，要查询全局100页数据，offset 9900 limit 100改写为offset 4950 limit 50，每个分库偏移4950（一半），获取50条数据（半页），得到的数据集的并集，基本能够认为，是全局数据的offset 9900 limit 100的数据，当然，这一页数据的精度，并不是精准的。 根据实际业务经验，用户都要查询第100页网页、帖子、邮件的数据了，这一页数据的精准性损失，业务上往往是可以接受的，但此时技术方案的复杂度便大大降低了，既不需要返回更多的数据，也不需要进行服务内存排序了。 四、终极武器-二次查询法有没有一种技术方案，即能够满足业务的精确需要，无需业务折衷，又高性能的方法呢？这就是接下来要介绍的终极武器：“二次查询法”。 为了方便举例，假设一页只有5条数据，查询第200页的SQL语句为select * from T order by time offset 1000 limit 5; 步骤一：查询改写 将select * from T order by time offset 1000 limit 5 改写为select * from T order by time offset 500 limit 5 并投递给所有的分库，注意，这个offset的500，来自于全局offset的总偏移量1000，除以水平切分数据库个数2。 如果是3个分库，则可以改写为select * from T order by time offset 333 limit 5 假设这三个分库返回的数据(time, uid)如下： 可以看到，每个分库都是返回的按照time排序的一页数据。 步骤二：找到所返回3页全部数据的最小值 第一个库，5条数据的time最小值是1487501123 第二个库，5条数据的time最小值是1487501133 第三个库，5条数据的time最小值是1487501143 故，三页数据中，time最小值来自第一个库，time_min=1487501123，这个过程只需要比较各个分库第一条数据，时间复杂度很低 步骤三：查询二次改写 第一次改写的SQL语句是select * from T order by time offset 333 limit 5 第二次要改写成一个between语句，between的起点是time_min，between的终点是原来每个分库各自返回数据的最大值： 第一个分库，第一次返回数据的最大值是1487501523 所以查询改写为select * from T order by time where time between time_min and 1487501523 第二个分库，第一次返回数据的最大值是1487501323 所以查询改写为select * from T order by time where time between time_min and 1487501323 第三个分库，第一次返回数据的最大值是1487501553 所以查询改写为select * from T order by time where time between time_min and 1487501553 相对第一次查询，第二次查询条件放宽了，故第二次查询会返回比第一次查询结果集更多的数据，假设这三个分库返回的数据(time, uid)如下： 可以看到： 由于time_min来自原来的分库一，所以分库一的返回结果集和第一次查询相同（所以其实这次访问是可以省略的）； 分库二的结果集，比第一次多返回了1条数据，头部的1条记录（time最小的记录）是新的（上图中粉色记录）； 分库三的结果集，比第一次多返回了2条数据，头部的2条记录（time最小的2条记录）是新的（上图中粉色记录）； 步骤四：在每个结果集中虚拟一个time_min记录，找到time_min在全局的offset 在第一个库中，time_min在第一个库的offset是333 在第二个库中，(1487501133, uid_aa)的offset是333（根据第一次查询条件得出的），故虚拟time_min在第二个库的offset是331 在第三个库中，(1487501143, uid_aaa)的offset是333（根据第一次查询条件得出的），故虚拟time_min在第三个库的offset是330 综上，time_min在全局的offset是333+331+330=994 步骤五：既然得到了time_min在全局的offset，就相当于有了全局视野，根据第二次的结果集，就能够得到全局offset 1000 limit 5的记录 第二次查询在各个分库返回的结果集是有序的，又知道了time_min在全局的offset是994，一路排下来，容易知道全局offset 1000 limit 5的一页记录（上图中黄色记录）。 是不是非常巧妙？这种方法的优点是：可以精确的返回业务所需数据，每次返回的数据量都非常小，不会随着翻页增加数据的返回量。 不足是：需要进行两次数据库查询。 五、总结 今天介绍了解决“跨N库分页”这一难题的四种方法： 方法一：全局视野法 （1）将order by time offset X limit Y，改写成order by time offset 0 limit X+Y （2）服务层对得到的N*(X+Y)条数据进行内存排序，内存排序后再取偏移量X后的Y条记录 这种方法随着翻页的进行，性能越来越低。 方法二：业务折衷法-禁止跳页查询 （1）用正常的方法取得第一页数据，并得到第一页记录的time_max （2）每次翻页，将order by time offset X limit Y，改写成order by time where time&gt;$time_max limit Y 以保证每次只返回一页数据，性能为常量。 方法三：业务折衷法-允许模糊数据 （1）将order by time offset X limit Y，改写成order by time offset X/N limit Y/N 方法四：二次查询法 （1）将order by time offset X limit Y，改写成order by time offset X/N limit Y （2）找到最小值time_min （3）between二次查询，order by time between $time_min and $time_i_max （4）设置虚拟time_min，找到time_min在各个分库的offset，从而得到time_min在全局的offset （5）得到了time_min在全局的offset，自然得到了全局的offset X limit Y 原创： 58沈剑 转载来源：业界难题-“跨库分页”的四种方案]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python优雅写法，让你工作效率翻2倍]]></title>
    <url>%2F2018%2F09%2F06%2FPython%E4%BC%98%E9%9B%85%E5%86%99%E6%B3%95%EF%BC%8C%E8%AE%A9%E4%BD%A0%E5%B7%A5%E4%BD%9C%E6%95%88%E7%8E%87%E7%BF%BB2%E5%80%8D%2F</url>
    <content type="text"><![CDATA[为多个变量赋值有时，有多个变量需要赋值，这时你会怎么赋值呢？ 常规方法：常规方法是给变量逐个赋值。 123a = 0b = 1c = 2 优雅方法：直接按顺序对应一一赋值。1a, b, c = 0, 1, 2 序列解包需要取出列表中的元素。 常规方法：一般我们知道可以通过下标获取具体元素。12345678info = ['brucepk', 'man', 'python']name = info[0]sex = info[1]tech = info[2]print(name,sex,tech)# 结果brucepk man python 优雅方法：给出对应变量接收所有元素。 123456789101112131415161718192021222324info = ['brucepk', 'man', 'python']name,sex,tech = infoprint(name,sex,tech)# 结果brucepk man python``` #### 优雅你的判断语句我们用判断语句来定义一个绝对值函数。###### 常规方法：```pyx = -6if x &lt; 0: y = -xelse: y = xprint(y)# 结果6 优雅方法：123456x = -6y = -x if x&lt;0 else xprint(y)# 结果6 区间判断使用 and 连续两次判断的语句，条件都符合时才执行语句。 常规方法：1234567score = 82 if score &gt;=80 and score &lt; 90: level = 'B' print(level) # 结果 B 优雅方法：使用链式判断。1234567score = 82 if 80 &lt;= score &lt; 90: level = 'B' print(level) # 结果 B 多个值符合条件判断多个值任意一个值符合条件即为 True 的情况。 常规方法：1234567num = 1 if num == 1 or num == 3 or num == 5: type = '奇数' print(type) # 结果 奇数 优雅方法：使用关键字 in，让你的语句更优雅。1234567num = 1 if num in(1,3,5): type = '奇数' print(type) # 结果 奇数 判断是否为空判断元素是空还是非空。 常规方法：一般我们想到的是 len() 方法来判断元素长度，大于 0 则为非空。 12345678910A,B,C =[1,3,5],&#123;&#125;,''if len(A) &gt; 0: print('A 为非空')if len(B) &gt; 0: print('B 为非空')if len(C) &gt; 0: print('C 为非空')# 结果A 为非空 优雅方法：if 后面的执行条件是可以简写的，只要条件 是非零数值、非空字符串、非空 list 等，就判断为 True，否则为 False。 12345678910A,B,C =[1,3,5],&#123;&#125;,''if A: print('A 为非空')if B: print('B 为非空')if C: print('C 为非空')# 结果A 为非空 多条件内容判断至少一个成立常规方法：用 or 连接多个条件。123456math,English,computer =90,80,88 if math&lt;60 or English&lt;60 or computer&lt;60: print('not pass') # 结果 not pass 优雅方法：使用 any 语句。123456math,English,computer =90,59,88 if any([math&lt;60,English&lt;60,computer&lt;60]): print('not pass') # 结果 not pass 多条件内容判断全部成立常规方法：使用 and 连接条件做判断。123456math,English,computer =90,80,88 if math&gt;60 and English&gt;60 and computer&gt;60: print('pass') # 结果 pass 优雅方法：使用 all 方法。 123456math,English,computer =90,80,88if all([math&gt;60,English&gt;60,computer&gt;60]): print('pass')# 结果pass 遍历序列的元素和元素下标常规方法：使用 for 循环进行遍历元素和下标。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152L =['math', 'English', 'computer', 'Physics']for i in range(len(L)): print(i, ':', L[i])# 结果0 : math1 : English2 : computer3 : Physics``` ###### 优雅方法：使用 enumerate 函数。```pyL =['math', 'English', 'computer', 'Physics']for k,v in enumerate(L): print(k, ':', v)# 结果0 : math1 : English2 : computer3 : Physics``` #### 循环语句优化###### 常规方法：使用简单的 for 循环可以达到目的。```pyL = []for i in range(1, 6): L.append(i*i)print(L) #结果：[1, 4, 9, 16, 25]``` ###### 优雅方法：使用列表生成式，一行代码搞定。```pyprint([x*x for x in range(1, 6)]) #结果：[1, 4, 9, 16, 25] Python 这些优雅的写法学会了吗？自己赶紧动手试试吧。 推荐阅读Python骚操作 | 还原已撤回的微信消息 Python骚操作：微信远程控制电脑 微信最强花式操作，带你玩转-wxpy 手把手教你用 Python 来朗读网页]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python骚操作 | 还原已撤回的微信消息]]></title>
    <url>%2F2018%2F09%2F06%2FPython%E9%AA%9A%E6%93%8D%E4%BD%9C-%E8%BF%98%E5%8E%9F%E5%B7%B2%E6%92%A4%E5%9B%9E%E7%9A%84%E5%BE%AE%E4%BF%A1%E6%B6%88%E6%81%AF%2F</url>
    <content type="text"><![CDATA[一大早醒来，发现女神昨晚发来三条消息，但是显示都已撤回，OMG，我错过了什么？群里有一个漂亮妹纸的爆照照片撤回了，想看又看不到！群里大佬分享的经典语录被撤回了，感觉错过一个亿！怎么办？用无所不能的 Python 就可以将这些撤回的消息发给你的微信，让你从此走上人生巅峰! 项目环境语言：Python3编辑器：Pycharm 导包itchat：控制微信的第三方库 这个库相信大家不陌生了，之前写的 微信最强花式操作，带你玩转 wxpy 文章里用的 wxpy 库就是在 itchat 库的基础上封装的。 效果展示以下截图显示的撤回消息类型依次是文字消息、微信自带表情、图片、语音、定位地图、名片、公众号文章、音乐、视频。有群里撤回的，也有个人号撤回的。 程序思路主要由两部分组成：handler_receive_msg()：处理接收到的消息，将消息临时放在字典中。 send_msg_helper()：将撤回的消息自动发给文件传输助手。 程序分析首先，我们定义一个字典来储存消息，定义消息储存的临时路径。 123456# 说明：可以撤回的有文本文字、微信自带&amp;收藏的表情、图片、语音、位置、名片、分享、附件、视频msg_dict = &#123;&#125; # 定义字典储存消息rev_tmp_dir = "D:\PycharmProjects\pythonProcedure\com\zyf\weixin\wxpy.pkl" # 定义文件存储临时目录if not os.path.exists(rev_tmp_dir): os.mkdir(rev_tmp_dir)face_bug = None # 处理表情解决方法 接收信息处理先将我们需要处理的消息用 msg_register 装饰器进行注册，格式化本地时间，定义消息 ID 和消息时间。如果是群成员而且是自己微信好友撤回消息，则显示撤回消息的名称是你备注的名字，如果没有备注名字，则显示名称为微信昵称。 123456789101112131415161718@itchat.msg_register([TEXT, PICTURE, MAP, CARD, SHARING, RECORDING, ATTACHMENT, VIDEO, FRIENDS], isFriendChat=True, isGroupChat=True)def handler_receive_msg(msg): # 将接收到的消息存放在字典中，不接受不具有撤回功能的信息 global face_bug # 全局变量 msg_time_rec = time.strftime("%Y-%m-%d %H:%M:%S", time.localtime()) # 格式化本地时间戳 e: 2018-09-04 22:02:08 msg_id = msg['MsgId'] # 消息ID msg_time = msg['CreateTime'] # 消息时间 if 'ActualNickName' in msg: # 判断是否为群消息 from_user = msg['ActualUserName'] # 群消息的发送者,用户的唯一标识 msg_from = msg['ActualNickName'] friends = itchat.get_friends(update=True) # 获取所有好友 for friend in friends: if from_user == friend['UserName']: # 判断群里撤回消息的是否为自己好友 if friend['RemarkName']: # 优先使用好友的备注名称，没有则使用昵称 msg_from = friend['RemarkName'] else: msg_from = friend['NickName'] break 获取你的所有群的消息，判断出撤回的消息来自哪个群，显示出群名称。1234567groups = itchat.get_chatrooms(update=True) # 获取所有的群for group in groups: if msg['FromUserName'] == group['UserName']: # 根据群消息的FromUserName匹配是哪个群 group_name = group['NickName'] group_members = group['MemberCount'] breakgroup_name = group_name + '(' + str(group_members) + ')' 个人消息处理如果为个人聊天信息，也是优先显示备注名称，没有备注名就显示昵称。 123456else: # 否则输入个人消息 if itchat.search_friends(userName=msg['FromUserName'])['RemarkName']: # 优先使用备注名称 msg_from = itchat.search_friends(userName=msg['FromUserName'])['RemarkName'] else: msg_from = itchat.search_friends(userName=msg['FromUserName'])['NickName'] group_name = '' 各类型消息处理用判断语句对各种类型的消息进行处理，包括文字消息、微信自带的表情和收藏的表情、图片、语音、位置、名片、分享、附件、视频。多条件时这里用了 in 的用法，还记得之前讲 Python 优雅的写法 文章里把用 or 连接条件改成用 in，代码更简洁，这样我们通过项目不断的巩固之前学到的知识点，这样才能不断进步。123456789101112131415161718if msg['Type'] in ('Text', 'Friends'): msg_content = msg['Text'] # 如果发送的消息是文本或者好友推荐elif msg['Type'] in ('Recording', 'Attachment', 'Video', 'Picture'): msg_content = r"" + msg['FileName'] # 如果发送的消息是附件、视频、图片、语音 msg['Text'](rev_tmp_dir + msg['FileName']) # 保存文件elif msg['Type'] == 'Card': msg_content = msg['RecommendInfo']['NickName'] + r" 的名片"elif msg['Type'] == 'Map': x, y, location = re.search( "&lt;location x=\"(.*?)\" y=\"(.*?)\".*label=\"(.*?)\".*", msg['OriContent']).group(1, 2, 3) if location is None: msg_content = r"纬度-&gt;" + x.__str__() + " 经度-&gt;" + y.__str__() # 内容为详细的地址 else: msg_content = r"" + locationelif msg['Type'] == 'Sharing': # 如果消息为分享的音乐或者文章，详细的内容为文章的标题或者是分享的名字 msg_content = msg['Text'] msg_share_url = msg['Url'] # 分享链接face_bug = msg_content 更新信息字典 12345678# 更新字典msg_dict.update(&#123;msg_id: &#123;"msg_from": msg_from, "msg_time": msg_time, "msg_time_rec": msg_time_rec, "msg_type": msg["Type"], "msg_content": msg_content, "msg_share_url": msg_share_url, "group_name": group_name&#125;&#125;) 处理撤回消息先判断是否是撤回消息，将撤回消息发送到你的文件传输助手里，把上面函数储存的消息的发送人、发送类型、发送时间、撤回的内容发出来。以下是部分代码。 12345678910111213141516171819@itchat.msg_register(NOTE, isFriendChat=True, isGroupChat=True, isMpChat=True)# 收到note通知类消息，判断是不是撤回并进行相应操作def send_msg_helper(msg): global face_bug if re.search(r"\&lt;\!\[CDATA\[.*撤回了一条消息\]\]\&gt;", msg['Content']) is not None: # 获取消息的id old_msg_id = re.search( "\&lt;msgid\&gt;(.*?)\&lt;\/msgid\&gt;", msg['Content']).group(1) # 在返回的content查找撤回的消息的id old_msg = msg_dict.get(old_msg_id, &#123;&#125;) if len(old_msg_id) &lt; 11: itchat.send_file(rev_tmp_dir + face_bug, toUserName='filehelper') os.remove(rev_tmp_dir + face_bug) else: msg_body = "快来看啊，有人撤回消息啦！" + "\n" \ + old_msg.get('msg_from') + " 撤回了 " + old_msg.get("msg_type") + " 消息" + "\n" \ + old_msg.get('msg_time_rec') + "\n" \ + "撤回了什么 ⇣" + "\n" \ + r"" + old_msg.get('msg_content') 主函数最后用主函数执行微信的登录和运行。第一次需要扫码登录微信，登录时加上 hotReload 参数，为 True 时，短时间内再次运行会保存上次微信的登录态，不需要再次扫码登录。 123if __name__ == '__main__': itchat.auto_login(hotReload=True) itchat.run() 如果你电脑中有安装 Python 环境，在编辑器中直接运行源码或者在 cmd 中运行 py 文件即可。源码获取方式在文末给出。有需要的话以后可以做成界面化工具，挂在服务器上，支持做成界面化工具。 写在最后今天的分享就到这里了，需要优化的地方：长时间运行时会报 ConnectionError 提示的错误，但是不影响发送撤回消息的功能，后期加个异常捕捉机制优化下。 赶紧动手试试吧，把你朋友撤回的消息发给 TA 看看，看看他是什么反应，有趣的撤回消息发到朋友圈去提高逼格，奈斯！ 源码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122#!/usr/bin/env python# -*- coding: utf-8 -*-# @Time : 2018/9/6 16:09# @Author : yfzhou# @Site :# @File : withdraw.py# @Software: PyCharm# Life is short, I use python.import osimport reimport shutilimport timeimport itchatfrom itchat.content import *# 说明：可以撤回的有文本文字、微信自带&amp;收藏的表情、图片、语音、位置、名片、分享、附件、视频msg_dict = &#123;&#125; # 定义字典储存消息rev_tmp_dir = "D:\PycharmProjects\pythonProcedure\com\zyf\weixin\wxpy.pkl" # 定义文件存储临时目录if not os.path.exists(rev_tmp_dir): os.mkdir(rev_tmp_dir)face_bug = None # 处理表情解决方法@itchat.msg_register([TEXT, PICTURE, MAP, CARD, SHARING, RECORDING, ATTACHMENT, VIDEO, FRIENDS], isFriendChat=True, isGroupChat=True)def handler_receive_msg(msg): # 将接收到的消息存放在字典中，不接受不具有撤回功能的信息 global face_bug # 全局变量 msg_time_rec = time.strftime("%Y-%m-%d %H:%M:%S", time.localtime()) # 格式化本地时间戳 e: 2018-09-04 22:02:08 msg_id = msg['MsgId'] # 消息ID msg_time = msg['CreateTime'] # 消息时间 if 'ActualNickName' in msg: # 判断是否为群消息 from_user = msg['ActualUserName'] # 群消息的发送者,用户的唯一标识 msg_from = msg['ActualNickName'] friends = itchat.get_friends(update=True) # 获取所有好友 for friend in friends: if from_user == friend['UserName']: # 判断群里撤回消息的是否为自己好友 if friend['RemarkName']: # 优先使用好友的备注名称，没有则使用昵称 msg_from = friend['RemarkName'] else: msg_from = friend['NickName'] break groups = itchat.get_chatrooms(update=True) # 获取所有的群 for group in groups: if msg['FromUserName'] == group['UserName']: # 根据群消息的FromUserName匹配是哪个群 group_name = group['NickName'] group_members = group['MemberCount'] break group_name = group_name + '(' + str(group_members) + ')' else: # 否则输入个人消息 if itchat.search_friends(userName=msg['FromUserName'])['RemarkName']: # 优先使用备注名称 msg_from = itchat.search_friends(userName=msg['FromUserName'])['RemarkName'] else: msg_from = itchat.search_friends(userName=msg['FromUserName'])['NickName'] group_name = '' msg_content = None # 消息内容 msg_share_url = None # 分享的链接 if msg['Type'] in ('Text', 'Friends'): msg_content = msg['Text'] # 如果发送的消息是文本或者好友推荐 elif msg['Type'] in ('Recording', 'Attachment', 'Video', 'Picture'): msg_content = r"" + msg['FileName'] # 如果发送的消息是附件、视频、图片、语音 msg['Text'](rev_tmp_dir + msg['FileName']) # 保存文件 elif msg['Type'] == 'Card': msg_content = msg['RecommendInfo']['NickName'] + r" 的名片" elif msg['Type'] == 'Map': x, y, location = re.search( "&lt;location x=\"(.*?)\" y=\"(.*?)\".*label=\"(.*?)\".*", msg['OriContent']).group(1, 2, 3) if location is None: msg_content = r"纬度-&gt;" + x.__str__() + " 经度-&gt;" + y.__str__() # 内容为详细的地址 else: msg_content = r"" + location elif msg['Type'] == 'Sharing': # 如果消息为分享的音乐或者文章，详细的内容为文章的标题或者是分享的名字 msg_content = msg['Text'] msg_share_url = msg['Url'] # 分享链接 face_bug = msg_content # 更新字典 msg_dict.update(&#123;msg_id: &#123;"msg_from": msg_from, "msg_time": msg_time, "msg_time_rec": msg_time_rec, "msg_type": msg["Type"], "msg_content": msg_content, "msg_share_url": msg_share_url, "group_name": group_name&#125;&#125;)@itchat.msg_register(NOTE, isFriendChat=True, isGroupChat=True, isMpChat=True)# 收到note通知类消息，判断是不是撤回并进行相应操作def send_msg_helper(msg): global face_bug if re.search(r"\&lt;\!\[CDATA\[.*撤回了一条消息\]\]\&gt;", msg['Content']) is not None: # 获取消息的id old_msg_id = re.search( "\&lt;msgid\&gt;(.*?)\&lt;\/msgid\&gt;", msg['Content']).group(1) # 在返回的content查找撤回的消息的id old_msg = msg_dict.get(old_msg_id, &#123;&#125;) if len(old_msg_id) &lt; 11: itchat.send_file(rev_tmp_dir + face_bug, toUserName='filehelper') os.remove(rev_tmp_dir + face_bug) else: msg_body = "快来看啊，有人撤回消息啦！" + "\n" \ + old_msg.get('msg_from') + " 撤回了 " + old_msg.get("msg_type") + " 消息" + "\n" \ + old_msg.get('msg_time_rec') + "\n" \ + "撤回了什么 ⇣" + "\n" \ + r"" + old_msg.get('msg_content') # 如果是分享存在链接 if old_msg['msg_type'] == "Sharing": msg_body += "\n就是这个链接➣ " + old_msg.get('msg_share_url') itchat.send(msg_body, toUserName='filehelper') # 将撤回消息发送到文件助手 if old_msg["msg_type"] in ( "Picture", "Recording", "Video", "Attachment"): file = '@fil@%s' % (rev_tmp_dir + old_msg['msg_content']) itchat.send(msg=file, toUserName='filehelper') os.remove(rev_tmp_dir + old_msg['msg_content']) msg_dict.pop(old_msg_id) # 删除字典旧消息if __name__ == '__main__': itchat.auto_login(hotReload=True) itchat.run() 推荐阅读Python骚操作：微信远程控制电脑 微信最强花式操作，带你玩转-wxpy 手把手教你用 Python 来朗读网页]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[InnoDB，快照读，在RR和RC下有何差异？]]></title>
    <url>%2F2018%2F09%2F05%2FInnoDB%EF%BC%8C%E5%BF%AB%E7%85%A7%E8%AF%BB%EF%BC%8C%E5%9C%A8RR%E5%92%8CRC%E4%B8%8B%E6%9C%89%E4%BD%95%E5%B7%AE%E5%BC%82%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[为了保证文章知识体系的完整性，先简单解释下快照读，读提交，可重复读。 快照读(Snapshot Read) MySQL数据库，InnoDB存储引擎，为了提高并发，使用MVCC机制，在并发事务时，通过读取数据行的历史数据版本，不加锁，来提高并发的一种不加锁一致性读(Consistent Nonlocking Read)。 读提交(Read Committed) 数据库领域，事务隔离级别的一种，简称RC 它解决“读脏”问题，保证读取到的数据行都是已提交事务写入的 它可能存在“读幻影行”问题，同一个事务里，连续相同的read可能读到不同的结果集 可重复读(Repeated Read) 数据库领域，事务隔离级别的一种，简称RR 它不但解决“读脏”问题，还解决了“读幻影行”问题，同一个事务里，连续相同的read读到相同的结果集 在读提交(RC)，可重复读(RR)两个不同的事务的隔离级别下，快照读有什么不同呢？ 先说结论： 事务总能够读取到，自己写入(update /insert /delete)的行记录 RC下，快照读总是能读到最新的行数据快照，当然，必须是已提交事务写入的 RR下，某个事务首次read记录的时间为T，未来不会读取到T时间之后已提交事务写入的记录，以保证连续相同的read读到相同的结果集 画外音：可以看到 (1)和并发事务的开始时间没关系，和事务首次read的时间有关； (2)由于不加锁，和互斥关系也不大； InnoDB表： t(id PK, name); 表中有三条记录：1, shenjian2, zhangsan3, lisi case 1，两个并发事务A，B执行的时间序列如下（A先于B开始，B先于A结束）： A1: start transaction; B1: start transaction;A2: select from t; B2: insert into t values (4, wangwu);A3: select from t; B3: commit;A4: select * from t; 提问1：假设事务的隔离级别是可重复读RR，事务A中的三次查询，A2, A3, A4分别读到什么结果集？ 回答：RR下 (1)A2读到的结果集肯定是{1, 2, 3}，这是事务A的第一个read，假设为时间T； (2)A3读到的结果集也是{1, 2, 3}，因为B还没有提交； (3)A4读到的结果集还是{1, 2, 3}，因为事务B是在时间T之后提交的，A4得读到和A2一样的记录； 提问2：假设事务的隔离级别是读提交RC，A2, A3, A4又分别读到什么结果集呢？ 回答：RC下 (1)A2读到的结果集是{1, 2, 3}； (2)A3读到的结果集也是{1, 2, 3}，因为B还没有提交； (3)A4读到的结果集还是{1, 2, 3, 4}，因为事务B已经提交； case 2，仍然是上面的两个事务，只是A和B开始时间稍有不同（B先于A开始，B先于A结束）： &lt;span style=&quot;margin: 0px;padding: 0px;color: rgb(255, 76, 0);letter-spacing: 1px;font-size: 12px;&quot;&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp; B1: start transaction;&lt;/span&gt; A1: start transaction; A2: select from t; B2: insert into t values (4, wangwu);A3: select from t; B3: commit;A4: select * from t; 提问3：假设事务的隔离级别是可重复读RR，事务A中的三次查询，A2, A3, A4分别读到什么结果集？ 提问4：假设事务的隔离级别是读提交RC，A2, A3, A4的结果集又是什么呢？ 回答：事务的开始时间不一样，不会影响“快照读”的结果，所以结果集和case 1一样。 case 3，仍然是并发的事务A与B（A先于B开始，B先于A结束）： A1: start transaction; B1: start transaction; B2: insert into t values (4, wangwu); B3: commit;A2: select * from t; 提问5：假设事务的隔离级别是可重复读RR，事务A中的A2查询，结果集是什么？ 提问6：假设事务的隔离级别是读提交RC，A2的结果集又是什么呢？ 回答：在RR下， A2是事务A的第一个read，假设为时间T，它能读取到T之前提交事务写入的数据行，故结果集为{1, 2, 3, 4}。在RC下，没有疑问，一定是{1, 2, 3, 4}。 case 4，事务开始的时间再换一下（B先于A开始，B先于A结束）： &lt;span style=&quot;color: rgb(255, 76, 0);letter-spacing: 1px;font-size: 12px;&quot;&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp; B1: start transaction;&lt;/span&gt; A1: start transaction; B2: insert into t values (4, wangwu); B3: commit; A2: select * from t; 提问7：假设事务的隔离级别是可重复读RR，事务A中的A2查询，结果集是什么？ 提问8：假设事务的隔离级别是读提交RC，A2的结果集又是什么呢？ 回答：事务的开始时间不一样，不会影响“快照读”的结果，所以结果集和case 3一样。 啰嗦说了这么多，用昨天一位网友“山峰”同学的话总结： RR下，事务在第一个Read操作时，会建立Read View RC下，事务在每次Read操作时，都会建立Read View 相关推荐： 《InnoDB，并发如此之高的原因》]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>SQL</tag>
        <tag>InnoDB</tag>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[手把手教你用 Python 来朗读网页]]></title>
    <url>%2F2018%2F09%2F05%2F%E6%89%8B%E6%8A%8A%E6%89%8B%E6%95%99%E4%BD%A0%E7%94%A8-Python-%E6%9D%A5%E6%9C%97%E8%AF%BB%E7%BD%91%E9%A1%B5%2F</url>
    <content type="text"><![CDATA[是不是有的时候懒得自己看新闻？那么不妨试试用 Python 来朗读给你听吧。 网页转换成语音，步骤无外乎： 网页正文识别，获取到正文的文本内容； 文本转语音，通过接口将文本转换成语音文件； 语音文件的发声，即将语音文件读出； 网页正文识别我们之所以用 Python，就是因为 Python 有着丰富的库，网页正文识别也不在话下。这里我尝试了 readability、goose-extractor、cx-extractor-python， readabilityreadability 支持 Python3，使用 pip install readability-lxml 安装即可。 readability 使用起来也很方便： 123456import requestsfrom readability import Documentresponse = requests.get('https://hoxis.github.io/run-ansible-without-specifying-the-inventory-but-the-host-directly.html')doc = Document(response.text)print(doc.title()) 但是 readability 提取到的正文内容不是文本，里面仍包含 HTML 标签。 当然也可以结合其他组件再对 HTML 进行处理，如 html2text，我们这里就不再延伸，有兴趣的可以自行尝试。 goose3Goose 本来是一个用 Java 编写的文章提取器，后来就有了 Python 实现版： goose3 。 使用起来也很方便，同时对中文支持也不错。使用 pip install goose3 即可安装。 12345678910&gt;&gt;&gt; from goose3 import Goose&gt;&gt;&gt; from goose3.text import StopWordsChinese&gt;&gt;&gt; url = 'http://news.china.com/socialgd/10000169/20180616/32537640_all.html'&gt;&gt;&gt; g = Goose(&#123;'stopwords_class': StopWordsChinese&#125;)&gt;&gt;&gt; article = g.extract(url=url)&gt;&gt;&gt; print(article.cleaned_text[:150])北京时间6月15日23:00(圣彼得堡当地时间18:00)，2018年世界杯B组一场比赛在圣彼得堡球场展开角逐，伊朗1比0险胜摩洛哥，伊朗前锋阿兹蒙半场结束前错过单刀机会，鲍哈杜兹第95分钟自摆乌龙。这是伊朗20年来首度在世界杯决赛圈取胜。本届世界杯，既相继出现替补便进球，贴补梅开二度以及东道主 可以看出网页正文提取效果还不错，基本满足我们的要求，可以使用！ 注意：goose 还有另外一个 Python2 的版本：Python-Goose，使用方法和 goose3 基本一样。 文本转语音文本转语音，百度、阿里、腾讯、讯飞等都有提供 REST API 接口，阿里和腾讯的申请相对时间较长，阿里的貌似还要收费，百度和讯飞的在线申请后即可使用，没办法，好的东西得来总是要曲折一些。其中百度的没有调用量的限制（其实默认是 200000 次/天），讯飞有每天 500 次的限制。 这里我们使用百度的 REST API 接口中的语言合成接口，一方面原因是百度的调用次数没有限制，另一方面，我大致看了下讯飞的接口文档，接口限制还是比较多的。还有就是百度提供了 REST API 的 Python 封装，使用也更方便。 baidu-aip 的使用百度提供了 Python SDK，使用 pip install baidu-aip 可以直接安装。接口的使用可以参考接口文档：http://ai.baidu.com/docs#/TTS-Online-Python-SDK/top。 使用示例如下： 123456789101112131415161718192021from aip import AipSpeech"""你的 APPID AK SK 均可在服务控制台中的应用列表中查看。"""APP_ID = '你的 App ID'API_KEY = '你的 Api Key'SECRET_KEY = '你的 Secret Key'client = AipSpeech(APP_ID, API_KEY, SECRET_KEY)result = client.synthesis('你好，你在做什么', 'zh', 3, &#123; 'vol': 5,&#125;)# 识别正确返回语音二进制 错误则返回dict 参照下面错误码if not isinstance(result, dict): with open('auido.mp3', 'wb') as f: f.write(result) 接口参数： 参数类型描述是否必须texString合成的文本，使用UTF-8编码，请注意文本长度必须小于1024字节是langString语言选择,填写zh是ctpString客户端类型选择，web端填写1是cuidString用户唯一标识，用来区分用户，填写机器 MAC 地址或 IMEI 码，长度为60以内否spdString语速，取值0-9，默认为5中语速否pitString音调，取值0-9，默认为5中语调否volString音量，取值0-15，默认为5中音量否perString发音人选择,0为女声，1为男声，3为情感合成-度逍遥，4为情感合成-度丫丫，默认为普通女否 接口对单次传入的文本进行了限制，合成文本长度必须小于1024字节，如果文本长度过长，就需要进行切割处理，采用多次请求的方式，分别转换成语音文件，最后再将多个语音文件合并成一个。 文本切割可以使用如下代码将文本分割成多个长度为 500 的文本列表 12345678910111213141516171819202122232425262728293031# 将文本按 500 的长度分割成多个文本text_list = [text[i:i+500] for i in range(0, len(text), 500)]``` [](#语言文件合并 "语言文件合并")语言文件合并--------------------------我们使用 [pydub](https://github.com/jiaaro/pydub) 来处理生成的音频文件。使用 `pip install pydub` 即可安装。另外还 Ubuntu 环境需要安装依赖 `sudo apt-get install libav-tools`，Windows 环境需要到 [https://ffmpeg.zeranoe.com/builds/](https://ffmpeg.zeranoe.com/builds/) 下载 `FFmpeg`，并将其配置到环境变量中。若还有问题，可以参考官网配置：[https://github.com/jiaaro/pydub](https://github.com/jiaaro/pydub)。```py# 合并音频文件def merge_voice(file_list): voice_dict = &#123;&#125; song = None for i,f in enumerate(file_list): if i == 0: song = AudioSegment.from_file(f,"mp3") else: # 拼接音频文件 song += AudioSegment.from_file(f,"mp3") # 删除临时音频 os.unlink(f) # 导出合并后的音频文件，格式为MP3格式 file_name = str(uuid.uuid1()) + ".mp3" song.export(file_name, format="mp3") return file_name 通过百度的接口，我们可以将文字转化成音频文件，下面的问题就是如何播放音频文件。 音频文件播放网上获取到 Python 播放 wav 文件的方式由好几种，包括 pyaudio、pygame、winsound、playsound。不过测试下来，只有 playsound 成功。其他方式有兴趣的可以试下，有问题可以留言交流。 使用 pip install playsound 安装后即可使用。 使用也很简单： 12&gt; from playsound import playsound&gt;&gt;&gt; playsound('/path/to/a/sound/file/you/want/to/play.mp3') 说明：音频的播放需要在图形化页面下运行，因为命令行模式下，没有播放声音的出口。 实现代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576# encoding:utf-8import uuidimport reimport osimport argparsefrom pydub import AudioSegmentfrom aip import AipSpeechfrom playsound import playsoundfrom goose3 import Goosefrom goose3.text import StopWordsChinese""" 你的 百度 APPID AK SK """APP_ID = '11407664'API_KEY = 'GT69E8M6sgOcSnIGElrgXo1e'SECRET_KEY = 'fOCr1mwnyGOEjZg93GoonaGqzqp0paIB'# 命令行输入参数处理parser = argparse.ArgumentParser()parser.add_argument('-u', '--url', type=str, help="input the target url")# 获取参数args = parser.parse_args()URL = args.urlclient = AipSpeech(APP_ID, API_KEY, SECRET_KEY)def text_to_voice(text): file_name = str(uuid.uuid1()) + '.mp3' result = client.synthesis(text, 'zh', 3, &#123; 'vol': 5, &#125;) # 识别正确返回语音二进制 错误则返回 dict 参照下面错误码 if not isinstance(result, dict): with open(file_name, 'wb+') as f: f.write(result) return file_namedef get_text(url): g = Goose(&#123;'stopwords_class': StopWordsChinese&#125;) article = g.extract(url=url) return article.cleaned_text# 合并音频文件def merge_voice(file_list): voice_dict = &#123;&#125; song = None for i,f in enumerate(file_list): if i == 0: song = AudioSegment.from_file(f,"mp3") else: # 拼接音频文件 song += AudioSegment.from_file(f,"mp3") # 删除临时音频 os.unlink(f) # 导出合并后的音频文件，格式为MP3格式 file_name = str(uuid.uuid1()) + ".mp3" song.export(file_name, format="mp3") return file_nameif __name__ == "__main__": # url = "http://news.china.com/socialgd/10000169/20180616/32537640_all.html" text = get_text(URL) # 将文本按 500 的长度分割成多个文本 text_list = [text[i:i+500] for i in range(0, len(text), 500)] file_list = [] for t in text_list: file_list.append(text_to_voice(t)) # print(file_list) final_voice = merge_voice(file_list) print(final_voice) # 播放音频 playsound(final_voice) 运行1python page2voice.py -u "https://so.gushiwen.org/shiwenv_c244fc77f6fb.aspx" 运行后，代码就会自动解析网页并进行朗读啦。 总结至此，网页到音频的转换就结束了，当然程序没有这么完美，比如中英文混合的网页解析和转换的结果就不怎么理想，但是纯中文的新闻页面效果还是不错的。 源码已上传至 GitHub，欢迎取阅。]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python(wordcloud+jieba)生成中文词云图]]></title>
    <url>%2F2018%2F09%2F04%2FPython-wordcloud-jieba-%E7%94%9F%E6%88%90%E4%B8%AD%E6%96%87%E8%AF%8D%E4%BA%91%E5%9B%BE%2F</url>
    <content type="text"><![CDATA[基于Python的词云生成类库,很好用,而且功能强大.博主个人比较推荐 安装库12pip install wordcloudpip install jieba 话不多说，直接上代码 实例12345678910111213141516171819202122232425262728293031323334353637383940# -*- coding: utf-8 -*-# @Time : 2018/9/4 13:52# @Author : yfzhou# @Site : # @File : demo10.py# @Software: PyCharm# Life is short, I use python.# 词云生成工具from wordcloud import WordCloud, ImageColorGeneratorimport matplotlib.pyplot as pltfrom os import pathimport jieba# 获取当前的项目文件加的路径d = path.dirname(__file__)# 读取一个txt文件text = open(r'C:\Users\Administrator\Desktop\阿里传：这是阿里巴巴的世界美特里斯曼.txt', 'r', encoding='utf-8').read()# 读入背景图片bg_pic = plt.imread(r'C:\Users\Administrator\Pictures\Other\155061877268618276.jpg')wordlist_after_jieba = jieba.cut(text, cut_all=True)wl_space_split = " ".join(wordlist_after_jieba)# 生成词云font = d + r'static/simkai.ttf'wc = WordCloud( mask=bg_pic, background_color='white', font_path=font, scale=1.5, max_words=1500).generate(wl_space_split)image_colors = ImageColorGenerator(bg_pic)# 图片背景bg_color = ImageColorGenerator(bg_pic)# 开始画图plt.imshow(wc.recolor(color_func=bg_color))plt.axis('off')plt.show()# 保存图片wc.to_file(d + r"/image/render_09.png") text文本是《阿里传》font为字体路径 Wordcloud各参数含义1234567891011121314151617181920212223242526272829303132333435363738394041424344454647font_path : string #字体路径，需要展现什么字体就把该字体路径+后缀名写上，如：font_path = '黑体.ttf'width : int (default=400) #输出的画布宽度，默认为400像素height : int (default=200) #输出的画布高度，默认为200像素prefer_horizontal : float (default=0.90) #词语水平方向排版出现的频率，默认 0.9 （所以词语垂直方向排版出现频率为 0.1 ）mask : nd-array or None (default=None) #如果参数为空，则使用二维遮罩绘制词云。如果 mask 非空，设置的宽高值将被忽略，遮罩形状被 mask 取代。除全白（#FFFFFF）的部分将不会绘制，其余部分会用于绘制词云。如：bg_pic = imread('读取一张图片.png')，背景图片的画布一定要设置为白色（#FFFFFF），然后显示的形状为不是白色的其他颜色。可以用ps工具将自己要显示的形状复制到一个纯白色的画布上再保存，就ok了。scale : float (default=1) #按照比例进行放大画布，如设置为1.5，则长和宽都是原来画布的1.5倍min_font_size : int (default=4) #显示的最小的字体大小font_step : int (default=1) #字体步长，如果步长大于1，会加快运算但是可能导致结果出现较大的误差max_words : number (default=200) #要显示的词的最大个数stopwords : set of strings or None #设置需要屏蔽的词，如果为空，则使用内置的STOPWORDSbackground_color : color value (default=”black”) #背景颜色，如background_color='white',背景颜色为白色max_font_size : int or None (default=None) #显示的最大的字体大小mode : string (default=”RGB”) #当参数为“RGBA”并且background_color不为空时，背景为透明relative_scaling : float (default=.5) #词频和字体大小的关联性color_func : callable, default=None #生成新颜色的函数，如果为空，则使用 self.color_funcregexp : string or None (optional) #使用正则表达式分隔输入的文本collocations : bool, default=True #是否包括两个词的搭配colormap : string or matplotlib colormap, default=”viridis” #给每个单词随机分配颜色，若指定color_func，则忽略该方法random_state : int or None #为每个单词返回一个PIL颜色fit_words(frequencies) #根据词频生成词云generate(text) #根据文本生成词云generate_from_frequencies(frequencies[, ...]) #根据词频生成词云generate_from_text(text) #根据文本生成词云process_text(text) #将长文本分词并去除屏蔽词（此处指英语，中文分词还是需要自己用别的库先行实现，使用上面的 fit_words(frequencies) ）recolor([random_state, color_func, colormap]) #对现有输出重新着色。重新上色会比重新生成整个词云快很多to_array() #转化为 numpy arrayto_file(filename) #输出到文件 背景图 效果图]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[微信最强花式操作，带你玩转 wxpy]]></title>
    <url>%2F2018%2F09%2F04%2F%E5%BE%AE%E4%BF%A1%E6%9C%80%E5%BC%BA%E8%8A%B1%E5%BC%8F%E6%93%8D%E4%BD%9C%EF%BC%8C%E5%B8%A6%E4%BD%A0%E7%8E%A9%E8%BD%AC-wxpy%2F</url>
    <content type="text"><![CDATA[一、wxpy基本介绍与安装1.wxpy基本介绍wxpy基于itchat，使用了 Web 微信的通讯协议，通过大量接口优化提升了模块的易用性，并进行丰富的功能扩展。实现了微信登录、收发消息、搜索好友、数据统计、微信公众号、微信好友、微信群基本信息获取等功能。可用来实现各种微信个人号的自动化操作。 2.wxpy安装方法一：直接安装1pip install wxpy 方法二：豆瓣源安装（推荐）1pip install -i https://pypi.douban.com/simple/ wxpy 二、实践出真知1.给自己的文件传输助手发消息12345678910111213141516#!/usr/bin/env python# -*- coding: utf-8 -*-# @Time : 2018/9/4 10:46# @Author : yfzhou# @Site : # @File : demo1.py# @Software: PyCharm# Life is short, I use python.# 给自己的文件传输助手发消息from wxpy import *# 初始化一个机器人对象bot = Bot(cache_path=True)# 向文件传输助手发送消息bot.file_helper.send("hello,I'm Felix!") Bot类基本参数介绍：123456cache_path – 设置当前会话的缓存路径，并开启缓存功能；为 None (默认) 则不开启缓存功能。 开启缓存后可在短时间内避免重复扫码，缓存失效时会重新要求登陆。 设为 True 时，使用默认的缓存路径 ‘wxpy.pkl’。 qr_path – 保存二维码的路径 console_qr – 在终端中显示登陆二维码 运行后弹出一个二维码图片，用微信扫码登录即可，再回来看手机消息。 特别提醒：_使用的微信账号不能为新注册的账号，不然会报错KeyError: &#39;pass_ticket&#39;。 2.给指定朋友发送消息123456789101112131415161718192021222324252627#!/usr/bin/env python# -*- coding: utf-8 -*-# @Time : 2018/9/4 10:54# @Author : yfzhou# @Site : # @File : demo2.py# @Software: PyCharm# Life is short, I use python.from wxpy import *## 初始化一个机器人对象# cache_path缓存路径，给定值为第一次登录生成的缓存文件路径bot = Bot(cache_path="D:\PycharmProjects\pythonProcedure\com\zyf\weixin\wxpy.pkl")# 查找朋友"极简XksA"my_friend = bot.friends().search('张大饼')[0]# 发送消息my_friend.send('Felix TURING RoBot test') ''' 除此之外还有可以发送一下内容，自己动手尝试吧 发送图片 my\_friend.send\_image('hello.png') 发送视频 my\_friend.send\_video('hello.mp4') 发送文件 my\_friend.send\_file('hello.rar') ''' 运行结果： 3.群发消息12345678910111213import time# 初始化一个机器人对象# cache_path为登录状态缓存路径，给定值为第一次登录生成的缓存文件路径bot = Bot(cache_path="D:\PycharmProjects\pythonProcedure\com\zyf\weixin\wxpy.pkl")# 群发消息（谨慎使用，哈哈哈）my_friends = bot.friends(update=False)my_friends.pop(0) # 去除列表第一个元素（自己）for i in range(120): # 时间限制2分钟内最多发120次（具体看wxpy官方文档异常处理） friend = my_friends[i] friend.send('Good morning,the early bird catches the worm!(早上好，早起的鸟儿有虫吃！)') time.sleep(2) friend.send('不用回复，生活中一起加油！') 4.获取自己的微信好友数、活跃微信群数、关注微信公众号数1234567891011121314# 获取所有好友[返回列表包含Chats对象(你的所有好友，包括自己)]t0 = bot.friends(update=False)# 查看自己好友数(除开自己)print("我的好友数："+str(len(t0)-1))# 获取所有微信群[返回列表包含Groups对象]t1 = bot.groups(update=False)# 查看微信群数(活跃的)print("我的微信群聊数："+str(len(t1)))# 获取所有关注的微信公众号[返回列表包含Chats对象]t2 = bot.mps(update=False)# 查看关注的微信公众号数print("我关注的微信公众号数："+str(len(t2))) 运行结果： 1234# 注：如果直接把t0、t1、t2打印出就是对应得名称(不同类型，自己可以试一下)我的好友数：102我的微信群聊数：5我关注的微信公众号数：64 5.个人聊天机器人搭建（基于自己的）（1）自己的聊天机器人12345678910111213141516171819# 查找聊天对象my_friend = bot.friends().search('张大饼')[0]my_friend.send('Felix TURING RoBot test')# 自动回复# 如果想对所有好友实现机器人回复把参数 my_friend 改成 chats = [Friend]@bot.register(my_friend)def my_friednd_message(msg): print('[接收]' + str(msg)) if msg.type != 'Text': # 除文字外其他消息回复内容 ret = '你给我看了什么！[拜托]' elif "你来自哪里" in str(msg): # 特定问题回答 ret = "我来自China" else: # 文字消息自动回答 ret = '我爱你' print('[发送]' + str(ret)) return ret# 进入交互式的 Python 命令行界面，并堵塞当前线程embed() 6.个人聊天机器人搭建（基于图灵机器人的）（1）事前准备百度图灵机器人，注册图灵机器人账号,然后创建一个机器人，即可获得属于你的图灵机器人api。 （2） 创建属于自己的聊天机器人 方法一：使用Tuling类，简单实现 1234567891011121314151617181920212223242526#!/usr/bin/env python# -*- coding: utf-8 -*-# @Time : 2018/9/4 11:09# @Author : yfzhou# @Site : # @File : demo6.py# @Software: PyCharm# Life is short, I use python.from wxpy import *# 登录缓存路径,第一次设置为True# 生成缓存文件wxpy.pkl后，为该文件路径bot = Bot(cache_path="D:\PycharmProjects\pythonProcedure\com\zyf\weixin\wxpy.pkl")tuling = Tuling(api_key='b28b82730280474394c52d217d8de222')print('Felix机器人已经启动')# 我的小号，测试需谨慎my_friend = bot.friends().search('张大饼')[0]my_friend.send('Felix图灵机器人已启动，可以开始和劳资bb啦~~')# 如果想对所有好友实现机器人回复把参数my_friend改成chats = [Friend]# 使用图灵机器人自动与指定好友聊天@bot.register(my_friend)def reply_my_friend(msg): tuling.do_reply(msg)# 进入交互式的 Python 命令行界面，并堵塞当前线程embed() 方法二：自己手动发送post请求，有点麻烦哈哈哈~ 12345678910111213141516171819202122232425262728def auto_ai(text): url = "http://www.tuling123.com/openapi/api" api_key = "你的图灵接口api" payload = &#123; "key": api_key, "info": text, "userid": "userid" &#125; r = requests.post(url, data=json.dumps(payload)) result = json.loads(r.content) return "[Felix图灵机器人] " + result["text"]bot = Bot(cache_path="D:\PycharmProjects\pythonProcedure\com\zyf\weixin\wxpy.pkl") print('Felix图灵机器人已经启动')# 我的小号，测试需谨慎my_friednd = bot.friends().search('张大饼')[0]# 如果想对所有好友实现机器人回复把参数my_friend改成chats = [Friend]@bot.register(my_friednd)def my_friednd_message(msg): print('[接收]' + str(msg)) if msg.type != 'Text': ret = '你给我看了什么！[拜托]' else: ret = auto_ai(msg.text) print('[发送]' + str(ret)) return ret# 进入交互式的 Python 命令行界面，并堵塞当前线程embed() （3）聊天效果图基本测试，图灵机器人可以实现查询天气、车票、翻译、基本聊天等功能，比我们自己写的，，，哈哈哈。 7.来点有趣的获取微信好友性别、位置分布数据1234567891011121314151617from wxpy import *# 初始化一个机器人对象# cache_path缓存路径，给定值为第一次登录生成的缓存文件路径bot = Bot(cache_path="D:\PycharmProjects\pythonProcedure\com\zyf\weixin\wxpy.pkl")#获取好友列表(包括自己)my_friends = bot.friends(update=False)'''stats_text 函数：帮助我们简单统计微信好友基本信息简单的统计结果的文本 :param total: 总体数量 :param sex: 性别分布 :param top_provinces: 省份分布 :param top_cities: 城市分布 :return: 统计结果文本'''print(my_friends.stats_text()) 运行结果： 12345678910111213141516171819202122232425262728雨碎 共有 103 位微信好友男性: 68 (66.0%)女性: 27 (26.2%)TOP 10 省份江苏: 61 (59.22%)上海: 5 (4.85%)安徽: 2 (1.94%)广东: 2 (1.94%)北京: 2 (1.94%)Carinthia: 1 (0.97%)内蒙古: 1 (0.97%)Barcelona: 1 (0.97%)江西: 1 (0.97%)Auckland: 1 (0.97%)TOP 10 城市常州: 34 (33.01%)南京: 23 (22.33%)广州: 2 (1.94%)浦东新区: 2 (1.94%)苏州: 2 (1.94%)阜阳: 1 (0.97%)房山: 1 (0.97%)九江: 1 (0.97%)徐州: 1 (0.97%)无锡: 1 (0.97%) 获取好友微信昵称和个性签名，词云分析1234567891011121314151617bot = Bot(cache_path="D:\PycharmProjects\pythonProcedure\com\zyf\weixin\wxpy.pkl")#获取好友列表(包括自己)my_friends = bot.friends(update=False)# 微信昵称nick_name = ''# 微信个性签名wx_signature = ''for friend in my_friends: # 微信昵称：NickName nick_name = nick_name + friend.raw['NickName'] # 个性签名：Signature wx_signature = wx_signature + friend.raw['Signature']nick_name = jiebaclearText(nick_name)wx_signature = jiebaclearText(wx_signature)make_wordcloud(nick_name,1)make_wordcloud(wx_signature,2) 效果图： 5）获取关注微信公众号名称和基本简介，词云分析1234567891011121314# 获取微信公众号名称wx_public_name = ''# 公众号简介wx_pn_signature = ''# 获取微信公众号列表my_wx_pn = bot.mps(update=False)for wx_pn in my_wx_pn: wx_public_name = wx_public_name + wx_pn.raw['NickName'] wx_pn_signature = wx_pn_signature + wx_pn.raw['Signature']wx_public_name = jiebaclearText(wx_public_name)make_wordcloud(wx_public_name,3)wx_pn_signature = jiebaclearText(wx_pn_signature)make_wordcloud(wx_pn_signature,4) 效果图： 本文参考文档： 1.wxpy官方介绍文档: https://wxpy.readthedocs.io/zh/latest/messages.html 2.matplotlib官方介绍文档: https://matplotlib.org/ 源代码：https://gitee.com/ShaErHu/wxpy_matplotlib_learning]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用Python自动生成报表以邮件发送]]></title>
    <url>%2F2018%2F09%2F03%2F%E4%BD%BF%E7%94%A8Python%E8%87%AA%E5%8A%A8%E7%94%9F%E6%88%90%E6%8A%A5%E8%A1%A8%E4%BB%A5%E9%82%AE%E4%BB%B6%E5%8F%91%E9%80%81%2F</url>
    <content type="text"><![CDATA[数据分析师肯定每天都被各种各样的数据数据报表搞得焦头烂额，老板的，运营的、产品的等等。而且大部分报表都是重复性的工作，这篇文章就是帮助大家如何用Python来实现报表的自动发送，解放你的劳动力，可以让你有时间去做更有意思的事情。 首先来介绍下实现自动报表要使用到的Python库： pymysql 一个可以连接MySQL实例并且实现增删改查功能的库 datetime Python标准库中自带的关于时间的库 openpyxl 一个可以读写07版以后的Excel文档（.xlsx格式也支持）的库 smtplib SMTP即简单邮件传输协议，Python简单封装成了一个库 email 一个用来处理邮件消息的库 为什么使用openpyxl库来处理Excel呢？因为它支持每个sheet的行数为100W+，也是支持xlsx格式的文件。如果你接受xls文件，并且每个sheet的行数小于6W，也是可以使用xlwt库，它对大文件的读取速度要大于openpyxl。 接下来我们就进入实战部分，来正式实现这个过程。我把整个实现过程分成几个函数的方式来实现，这样看着会比较有结构感。 一、首先导入所有要用到的库12345678# encoding=utf-8import pymysql as pmsimport openpyxlimport datetimefrom email.mime.text import MIMETextfrom email.mime.multipart import MIMEMultipartfrom email.header import Headerimport smtplib 二、 编写一个传入sql就返回数据的函数get_datas(sql)123456789101112131415def get_datas(sql): # 一个传入sql导出数据的函数 # 跟数据库建立连接 conn = pms.connect(host='实例地址', user='用户', passwd='密码', database='库名', port=3306, charset="utf8") # 使用 cursor() 方法创建一个游标对象 cursor cur = conn.cursor() # 使用 execute() 方法执行 SQL cur.execute(sql) # 获取所需要的数据 datas = cur.fetchall() #关闭连接 cur.close() #返回所需的数据 return datas 三、 编写一个传入sql就返回数据的字段名称的函数get_datas(sql)，因为一个函数只能返回一个值，这边就用2个函数来分别返回数据和字段名称（也就是excel里的表头）12345678910def get_fields(sql): # 一个传入sql导出字段的函数 conn = pms.connect(host='rm-rj91p2yhl9dm2xmbixo.mysql.rds.aliyuncs.com', user='bi-analyzer', passwd='pcNzcKPnn', database='kikuu', port=3306, charset="utf8") cur = conn.cursor() cur.execute(sql) # 获取所需要的字段名称 fields = cur.description cur.close() return fields 四、 编写一个传入数据、字段名称、存储地址返回一个excel 的函数et_excel(data, field, file)123456789101112131415161718192021def get_excel(data, field, file): # 将数据和字段名写入excel的函数 #新建一个工作薄对象 new = openpyxl.Workbook() #激活一个新的sheet sheet = new.active #给sheet命名 sheet.title = '数据展示' #将字段名称循环写入excel第一行，因为字段格式列表里包含列表，每个列表的第一元素才是字段名称 for col in range(len(field)): #row代表行数，column代表列数，value代表单元格输入的值，行数和列数都是从1开始，这点于python不同要注意 _ = sheet.cell(row=1, column=col+1, value=u'%s' % field[col][0]) #将数据循环写入excel的每个单元格中 for row in range(len(data)): for col in range(len(field)): #因为第一行写了字段名称，所以要从第二行开始写入 _ = sheet.cell(row=row+2, column=col + 1, value=u'%s' % data[row][col]) #将生成的excel保存，这步是必不可少的 newworkbook = new.save(file) #返回生成的excel return newworkbook 五、 编写一个自动获取昨天日期字符串格式的函数getYesterday()123456789101112def getYesterday(): # 获取昨天日期的字符串格式的函数 #获取今天的日期 today = datetime.date.today() #获取一天的日期格式数据 oneday = datetime.timedelta(days=1) #昨天等于今天减去一天 yesterday = today - oneday #获取昨天日期的格式化字符串 yesterdaystr = yesterday.strftime('%Y-%m-%d') #返回昨天的字符串 return yesterdaystr 六、编写一个生成邮件的函数create_email(email_from, email_to, email_Subject, email_text, annex_path, annex_name)123456789101112131415161718192021def create_email(email_from, email_to, email_Subject, email_text, annex_path, annex_name): # 输入发件人昵称、收件人昵称、主题，正文，附件地址,附件名称生成一封邮件 #生成一个空的带附件的邮件实例 message = MIMEMultipart() #将正文以text的形式插入邮件中 message.attach(MIMEText(email_text, 'plain', 'utf-8')) #生成发件人名称（这个跟发送的邮件没有关系） message['From'] = Header(email_from, 'utf-8') #生成收件人名称（这个跟接收的邮件也没有关系） message['To'] = Header(email_to, 'utf-8') #生成邮件主题 message['Subject'] = Header(email_Subject, 'utf-8') #读取附件的内容 att1 = MIMEText(open(annex_path, 'rb').read(), 'base64', 'utf-8') att1["Content-Type"] = 'application/octet-stream' #生成附件的名称 att1["Content-Disposition"] = 'attachment; filename=' + annex_name #将附件内容插入邮件中 message.attach(att1) #返回邮件 return message 七、 生成一个发送邮件的函数send_email(sender, password, receiver, msg)123456789101112131415def send_email(sender, password, receiver, msg): # 一个输入邮箱、密码、收件人、邮件内容发送邮件的函数 try: #找到你的发送邮箱的服务器地址，已加密的形式发送 server = smtplib.SMTP_SSL("smtp.mxhichina.com", 465) # 发件人邮箱中的SMTP服务器 server.ehlo() #登录你的账号 server.login(sender, password) # 括号中对应的是发件人邮箱账号、邮箱密码 #发送邮件 server.sendmail(sender, receiver, msg.as_string()) # 括号中对应的是发件人邮箱账号、收件人邮箱账号（是一个列表）、邮件内容 print("邮件发送成功") server.quit() # 关闭连接 except Exception: print(traceback.print_exc()) print("邮件发送失败") 八、建立一个main函数，把所有的自定义内容输入进去，最后执行main函数123456789101112131415161718192021222324252627282930313233343536373839404142434445def main(): print(datetime.datetime.now()) my_sql = sql = "SELECT a.id '用户ID',\ a.gmtCreate '用户注册时间',\ af.lastLoginTime '最后登录时间',\ af.totalBuyCount '历史付款子单数',\ af.paidmountUSD '历史付款金额',\ af.lastPayTime '用户最后支付时间'\ FROM table a\ LEFT JOIN tableb af ON a.id= af.accountId ;" # 生成数据 my_data = get_datas(my_sql) # 生成字段名称 my_field = get_fields(my_sql) # 得到昨天的日期 yesterdaystr = getYesterday() # 文件名称 my_file_name = 'user attribute' + yesterdaystr + '.xlsx' # 文件路径 file_path = 'D:/work/report/' + my_file_name # 生成excel get_excel(my_data, my_field, file_path) my_email_from = 'BI部门自动报表机器人' my_email_to = '运营部' # 邮件标题 my_email_Subject = 'user' + yesterdaystr # 邮件正文 my_email_text = "Dear all,\n\t附件为每周数据，请查收！\n\nBI团队 " #附件地址 my_annex_path = file_path #附件名称 my_annex_name = my_file_name # 生成邮件 my_msg = create_email(my_email_from, my_email_to, my_email_Subject, my_email_text, my_annex_path, my_annex_name) my_sender = '阿里云邮箱' my_password = '我的密码' my_receiver = [10001@qq.com']#接收人邮箱列表 # 发送邮件 send_email(my_sender, my_password, my_receiver, my_msg) print(datetime.datetime.now())if __name__ == "__main__": main();]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python的22个编程技巧，Pick一下？]]></title>
    <url>%2F2018%2F09%2F03%2FPython%E7%9A%8422%E4%B8%AA%E7%BC%96%E7%A8%8B%E6%8A%80%E5%B7%A7%EF%BC%8CPick%E4%B8%80%E4%B8%8B%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[源 / 代码湾 编辑 / AI时间 1. 原地交换两个数字Python 提供了一个直观的在一行代码中赋值与交换（变量值）的方法，请参见下面的示例： 1x,y= 10,20print(x,y)x,y= y,xprint(x,y)#1 (10, 20)#2 (20, 10) 赋值的右侧形成了一个新的元组，左侧立即解析（unpack）那个（未被引用的）元组到变量 和 。 一旦赋值完成，新的元组变成了未被引用状态并且被标记为可被垃圾回收，最终也完成了变量的交换。 2. 链状比较操作符比较操作符的聚合是另一个有时很方便的技巧： 1n= 10result= 1&lt; n&lt; 20print(result)# Trueresult= 1&gt; n&lt;= 9print(result)# False 3. 使用三元操作符来进行条件赋值三元操作符是 if-else 语句也就是条件操作符的一个快捷方式： 12345678910111213141516171819[表达式为真的返回值] if [表达式] else [表达式为假的返回值]这里给出几个你可以用来使代码紧凑简洁的例子。下面的语句是说“如果 y 是 9，给 x 赋值 10，不然赋值为 20”。如果需要的话我们也可以延长这条操作链。x = 10 if (y == 9) else 20同样地，我们可以对类做这种操作：x = (classA if y == 1 else classB)(param1, param2)在上面的例子里 classA 与 classB 是两个类，其中一个类的构造函数会被调用。下面是另一个多个条件表达式链接起来用以计算最小值的例子：def small(a,b,c):returnaifa&lt;= banda&lt;= celse(bifb&lt;= aandb&lt;= celsec)print(small(1,0,1))print(small(1,2,2))print(small(2,2,3))print(small(5,4,3))#Output#0 #1 #2 #3我们甚至可以在列表推导中使用三元运算符：[m**2 if m &gt; 10 else m**4 for m in range(50)]#=&gt; [0, 1, 16, 81, 256, 625, 1296, 2401, 4096, 6561, 10000, 121, 144, 169, 196, 225, 256, 289, 324, 361, 400, 441, 484, 529, 576, 625, 676, 729, 784, 841, 900, 961, 1024, 1089, 1156, 1225, 1296, 1369, 1444, 1521, 1600, 1681, 1764, 1849, 1936, 2025, 2116, 2209, 2304, 2401] 4. 多行字符串基本的方式是使用源于 C 语言的反斜杠： 1multiStr= “select * from multi_rowwhere row_id &lt; 5”print(multiStr)# select * from multi_row where row_id &lt; 5 另一个技巧是使用三引号： 1multiStr= “””select * from multi_rowwhere row_id &lt; 5″””print(multiStr)#select * from multi_row#where row_id &lt; 5 上面方法共有的问题是缺少合适的缩进，如果我们尝试缩进会在字符串中插入空格。所以最后的解决方案是将字符串分为多行并且将整个字符串包含在括号中： 1multiStr= (“select * from multi_row ”“where row_id &lt; 5 ”“order by age”)print(multiStr)#select * from multi_row where row_id &lt; 5 order by age 5. 存储列表元素到新的变量中我们可以使用列表来初始化多个变量，在解析列表时，变量的数目不应该超过列表中的元素个数：【译者注：元素个数与列表长度应该严格相同，不然会报错】 1testList= [1,2,3]x,y,z= testListprint(x,y,z)#-&gt; 1 2 3 6. 打印引入模块的文件路径如果你想知道引用到代码中模块的绝对路径，可以使用下面的技巧：1import threadingimport socketprint(threading)print(socket)#1- #2- 7. 交互环境下的 “_” 操作符这是一个我们大多数人不知道的有用特性，在 Python 控制台，不论何时我们测试一个表达式或者调用一个方法，结果都会分配给一个临时变量： _（一个下划线）。 1&gt;&gt;&gt; 2+ 13&gt;&gt;&gt; _3&gt;&gt;&gt; print_3 “_” 是上一个执行的表达式的输出。 8. 字典/集合推导与我们使用的列表推导相似，我们也可以使用字典/集合推导，它们使用起来简单且有效，下面是一个例子： 1testDict= &#123;i: i *iforiinxrange(10)&#125;testSet= &#123;i *2foriinxrange(10)&#125;print(testSet)print(testDict)#set([0, 2, 4, 6, 8, 10, 12, 14, 16, 18])#&#123;0: 0, 1: 1, 2: 4, 3: 9, 4: 16, 5: 25, 6: 36, 7: 49, 8: 64, 9: 81&#125; 注：两个语句中只有一个 &lt;:> 的不同，另，在 Python3 中运行上述代码时，将 改为 。 9. 调试脚本我们可以在 模块的帮助下在 Python 脚本中设置断点，下面是一个例子：1import pdbpdb.set_trace() 我们可以在脚本中任何位置指定 并且在那里设置一个断点，相当简便。 10. 开启文件分享Python 允许运行一个 HTTP 服务器来从根路径共享文件，下面是开启服务器的命令：12345# Python 2python -m SimpleHTTPServer# Python 3python3 -m http.server 上面的命令会在默认端口也就是 8000 开启一个服务器，你可以将一个自定义的端口号以最后一个参数的方式传递到上面的命令中。 11. 检查 Python 中的对象我们可以通过调用 dir() 方法来检查 Python 中的对象，下面是一个简单的例子：12test= [1,3,5,7]print(dir(test))[‘__add__’, ‘__class__’, ‘__contains__’, ‘__delattr__’, ‘__delitem__’, ‘__delslice__’, ‘__doc__’, ‘__eq__’, ‘__format__’, ‘__ge__’, ‘__getattribute__’, ‘__getitem__’, ‘__getslice__’, ‘__gt__’, ‘__hash__’, ‘__iadd__’, ‘__imul__’, ‘__init__’, ‘__iter__’, ‘__le__’, ‘__len__’, ‘__lt__’, ‘__mul__’, ‘__ne__’, ‘__new__’, ‘__reduce__’, ‘__reduce_ex__’, ‘__repr__’, ‘__reversed__’, ‘__rmul__’, ‘__setattr__’, ‘__setitem__’, ‘__setslice__’, ‘__sizeof__’, ‘__str__’, ‘__subclasshook__’, ‘append’, ‘count’, ‘extend’, ‘index’, ‘insert’, ‘pop’, ‘remove’, ‘reverse’, ‘sort’] 12. 简化 if 语句我们可以使用下面的方式来验证多个值：1if m in [1,3,5,7]: 而不是：1if m==1 or m==3 or m==5 or m==7: 或者，对于 in 操作符我们也可以使用 ‘{1,3,5,7}’ 而不是 ‘[1,3,5,7]’，因为 set 中取元素是 O(1) 操作。 13. 一行代码计算任何数的阶乘Python 2.x.1result= (lambdak: reduce(int.__mul__,range(1,k+1),1))(3)print(result)#-&gt; 6 Python 3.x.123import functoolsresult= (lambdak: functools.reduce(int.__mul__,range(1,k+1),1))(3)print(result)#-&gt; 6 14. 找到列表中出现最频繁的数12test= [1,2,3,4,2,2,3,1,4,4,4]print(max(set(test),key=test.count))#-&gt; 4 15. 重置递归限制Python 限制递归次数到 1000，我们可以重置这个值： 12345import sysx=1001print(sys.getrecursionlimit())#1-&gt; 1000sys.setrecursionlimit(x)print(sys.getrecursionlimit())#2-&gt; 1001 请只在必要的时候采用上面的技巧。 16. 检查一个对象的内存使用在 Python 2.7 中，一个 32 比特的整数占用 24 字节，在 Python 3.5 中利用 28 字节。为确定内存使用，我们可以调用 getsizeof 方法： 在 Python 2.7 中123import sysx=1print(sys.getsizeof(x))#-&gt; 24 在 Python 3.5 中 123import sysx=1print(sys.getsizeof(x))#-&gt; 28 17. 使用 slots 来减少内存开支你是否注意到你的 Python 应用占用许多资源特别是内存？有一个技巧是使用 slots 类变量来在一定程度上减少内存开支。 12345678910111213141516#In Python 3.5import sysclass FileSystem(object): def __init__(self,files,folders,devices): self.files= files self.folders= folders self.devices= devices print(sys.getsizeof(FileSystem))#1-&gt; 1016class FileSystem1(object): __slots__= [‘files’,’folders’,’devices’] def __init__(self,files,folders,devices): self.files= files self.folders= folders self.devices= devices print(sys.getsizeof(FileSystem1))#2-&gt; 888 很明显，你可以从结果中看到确实有内存使用上的节省，但是你只应该在一个类的内存开销不必要得大时才使用 slots。只在对应用进行性能分析后才使用它，不然地话，你只是使得代码难以改变而没有真正的益处。 【译者注：在我的 win10 python2.7 中上面的结果是： 1#In Python 2.7 win10#1-&gt; 896#2-&gt; 1016 所以，这种比较方式是不那么让人信服的，使用 slots 主要是用以限定对象的属性信息，另外，当生成对象很多时花销可能会小一些，具体可以参见 python 官方文档: The slots declaration takes a sequence of instance variables and reserves just enough space in each instance to hold a value for each variable. Space is saved because dict is not created for each instance. 18. 使用 lambda 来模仿输出方法1import syslprint=lambda *args:sys.stdout.write(” “.join(map(str,args)))lprint(“python”,”tips”,1000,1001)#-&gt; python tips 1000 1001 19.从两个相关的序列构建一个字典123t1= (1,2,3)t2= (10,20,30)print(dict(zip(t1,t2)))#-&gt; &#123;1: 10, 2: 20, 3: 30&#125; 20. 一行代码搜索字符串的多个前后缀1print(“http://www.google.com”.startswith((“http://”,”https://”)))print(“http://www.google.co.uk”.endswith((“.com”,”.co.uk”)))#1-&gt; True#2-&gt; True 21. 不使用循环构造一个列表1import itertoolstest= [[-1,-2],[30,40],[25,35]]print(list(itertools.chain.from_iterable(test)))#-&gt; [-1, -2, 30, 40, 25, 35] 22. 在 Python 中实现一个真正的 switch-case 语句下面的代码使用一个字典来模拟构造一个 switch-case。 1def xswitch(x):returnxswitch._system_dict.get(x,None)xswitch._system_dict= &#123;‘files’: 10,’folders’: 5,’devices’: 2&#125;print(xswitch(‘default’))print(xswitch(‘devices’))#1-&gt; None#2-&gt; 2]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo-Next搭建个人博客（SEO优化）]]></title>
    <url>%2F2018%2F08%2F29%2FHexo-Next%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%EF%BC%88SEO%E4%BC%98%E5%8C%96%EF%BC%89%2F</url>
    <content type="text"><![CDATA[&nbsp;&nbsp;&nbsp;&nbsp;推广是一个烦人的事情啊喂，特别是对于我们搞技术的来说，可能就不擅长推广，那么怎么才能让别人知道我们呢，我们就要想办法让别人通过搜索就可以搜索到你博客的内容，给我们带来自然流量，这就需要seo优化,让我们的站点变得对搜索引擎友好。 SEO是由英文Search Engine Optimization缩写而来， 中文意译为“搜索引擎优化”。SEO是指通过站内优化比如网站结构调整、网站内容建设、网站代码优化等以及站外优化。 让百度收录你的站点我们首先要做的就是让各大搜索引擎收录你的站点，我们在刚建站的时候各个搜索引擎是没有收录我们网站的，在搜索引擎中输入site:&lt;域名&gt;,如果如下图所示就是说明我们的网站并没有被百度收录。我们可以直接点击下面的“网址提交”来提交我们的网站 验证网站所有权登录百度站长平台：http://zhanzhang.baidu.com,只要有百度旗下的账号就可以登录，登录成功之后在站点管理中点击添加网站然后输入你的站点地址，建议输入的网站为www开头的，不要输入github.io的，因为github是不允许百度的spider爬取github上的内容的，所以如果想让你的站点被百度收录，只能使用自己购买的域名在选择完网站的类型之后需要验证网站的所有权，验证网站所有权的方式有三种：文件验证。html标签验证和cname解析验证，使用哪一种方式都可以，都是比较简单的，但是一定要注意，使用文件验证文件存放的位置需要放在source文件夹下，如果是html文件那么hexo就会将其编译，所以必须要加上的layout:false，这样就不会被hexo编译。（如果验证文件是txt格式的就不需要），其他两种方式也是很简单的，我个人推荐文件验证和cname验证，cname验证最为简单，只需加一条解析就好~ 生成网站地图我们需要使用npm自动生成网站的sitemap，然后将生成的sitemap提交到百度和其他搜索引擎 安装sitemap插件12npm install hexo-generator-sitemap --save npm install hexo-generator-baidu-sitemap --save 修改博客配置文件在根目录配置文件中修改url为你的站点地址123456# URL## If your site is put in a subdirectory, set url as 'http://yoursite.com/child' and root as '/child/'url: https://yfzhou.coding.meroot: /permalink: :title.htmlpermalink_defaults: 执行完之后就会在网站根目录生成sitemap.xml文件和baidusitemap.xml文件，可以通过https://yfzhou.coding.me/baidusitemap.xml,查看该文件是否生成，其中sitemap.xml文件是搜索引擎通用的文件，baidusitemap.xml是百度专用的sitemap文件。 向百度提交链接然后我们就可以将我们生成的sitemap文件提交给百度，还是在百度站长平台，找到链接提交，这里我们可以看到有两种提交方式，自动提交和手动提交，自动提交又分为主动推送、自动推送和sitemap 如何选择链接提交方式1、主动推送：最为快速的提交方式，推荐您将站点当天新产出链接立即通过此方式推送给百度，以保证新链接可以及时被百度收录。2、自动推送：最为便捷的提交方式，请将自动推送的JS代码部署在站点的每一个页面源代码中，部署代码的页面在每次被浏览时，链接会被自动推送给百度。可以与主动推送配合使用。3、sitemap：您可以定期将网站链接放到sitemap中，然后将sitemap提交给百度。百度会周期性的抓取检查您提交的sitemap，对其中的链接进行处理，但收录速度慢于主动推送。4、手动提交：一次性提交链接给百度，可以使用此种方式。 一般主动提交比手动提交效果好，这里介绍主动提交的三种方法从效率上来说： 主动推送&gt;自动推送&gt;sitemap 主动推送安装插件npm install hexo-baidu-url-submit --save然后再根目录的配置文件中新增字段12345baidu_url_submit: count: 100 # 提交最新的一个链接 host: https://yfzhou.coding.me # 在百度站长平台中注册的域名 token: 8OGYpxowYnhgVsUM # 请注意这是您的秘钥， 所以请不要把博客源代码发布在公众仓库里! path: baidu_urls.txt # 文本文档的地址， 新链接会保存在此文本文档里 在加入新的deploye12deploy:- type:baidu_url_submitter 这样执行hexo deploy的时候，新的链接就会被推送了 设置自动推送在主题配置文件下设置,将baidu_push设置为true：12# Enable baidu push so that the blog will push the url to baidu automatically which is very helpful for SEObaidu_push: true 然后就会将一下代码自动推送到百度，位置是themes\next\layout_scripts\baidu_push.swig,这样每次访问博客中的页面就会自动向百度提交sitemap12345678910111213141516&#123;% if theme.baidu_push %&#125;&lt;script&gt;(function()&#123; var bp = document.createElement('script'); var curProtocol = window.location.protocol.split(':')[0]; if (curProtocol === 'https') &#123; bp.src = 'https://zz.bdstatic.com/linksubmit/push.js'; &#125; else &#123; bp.src = 'http://push.zhanzhang.baidu.com/push.js'; &#125; var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(bp, s);&#125;)();&lt;/script&gt;&#123;% endif %&#125; sitemap将我们上一步生成的sitemap文件提交到百度就可以了~我记得被百度收录过程还是蛮久的，一度让我以为我的方法有问题，提交链接在站长工具中有显示大概是有两天的时候，站点被百度收录大概花了半个月= =，让大家看一下现在的成果在百度搜索site:cherryblog.site已经可以搜索到结果在搜索框输入域名也可以找到站点输入关键字第一条就是 其他seo优化 seo优化应该说是一个收益延迟的行为，可能你做的优化短期内看不到什么效果，但是一定要坚持，seo优化也是有很深的可以研究的东西，从我们最初的网站设计，和最基础的标签的选择都有很大的关系，网站设计就如我们刚刚说的，要让用户点击三次可以到达网站的任何一个页面，要增加高质量的外链，增加相关推荐（比如说我们经常见到右侧本站的最高阅读的排名列表），然后就是给每一个页面加上keyword和描述在代码中，我们应该写出能让浏览器识别的语义化HTML，这样有助于爬虫抓取更多的有效信息：爬虫依赖于标签来确定上下文和各个关键字的权重；并且对外链设置nofollow标签，避免spider爬着爬着就爬出去了（减少网站的跳出率），并且我们要尽量在一些比较大的网站增加我们站点的曝光率，因为spider会经常访问大站，比如我们在掘金等技术社区发表文章中带有我们的站点，这样spider是很有可能爬到我们中的站点的，so…. 网站外链的推广度、数量和质量网站的内链足够强大网站的原创质量网站的年龄时间网站的更新频率（更新次数越多越好）网站的服务器网站的流量：流量越高网站的权重越高网站的关键词排名：关键词排名越靠前，网站的权重越高网站的收录数量：网站百度收录数量越多，网站百度权重越高网站的浏览量及深度：用户体验越好，网站的百度权重越高]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>Next</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo-Next搭建个人博客（代码块复制功能）]]></title>
    <url>%2F2018%2F08%2F27%2FHexo-Next%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%EF%BC%88%E4%BB%A3%E7%A0%81%E5%9D%97%E5%A4%8D%E5%88%B6%E5%8A%9F%E8%83%BD%EF%BC%89%2F</url>
    <content type="text"><![CDATA[为了提高博客代码块的用户体验，仅仅代码高亮还不行，最好还能一键复制代码。故此文将讲述Hexo NexT主题博客的代码块复制功能配置。 下载 clipboard.js三方插件 clipboardjs ，相关介绍和兼容性我就不赘述了，去它主页或github上看。 下载地址： clipboard.js clipboard.min.js 推荐 保存文件clipboard.js / clipboard.min.js ，目录如下：.\themes\next\source\js\src clipboardjs 使用也是在.\themes\next\source\js\src目录下，创建clipboard-use.js，文件内容如下： 123456789101112131415161718 /*页面载入完成后，创建复制按钮*/ !function (e, t, a) &#123; /* code */ var initCopyCode = function()&#123; var copyHtml = ''; copyHtml += '&lt;button class="btn-copy" data-clipboard-snippet=""&gt;'; //fa fa-globe可以去字体库替换自己想要的图标copyHtml += ' &lt;i class="fa fa-clipboard"&gt;&lt;/i&gt;&lt;span&gt;copy&lt;/span&gt;'; copyHtml += '&lt;/button&gt;'; $(".highlight .code pre").before(copyHtml); new ClipboardJS('.btn-copy', &#123; target: function(trigger) &#123; return trigger.nextElementSibling; &#125; &#125;); &#125; initCopyCode(); &#125;(window, document); 在.\themes\next\source\css\_custom\custom.styl样式文件中添加下面代码：123456789101112131415161718192021222324252627282930313233343536//代码块复制按钮.highlight&#123; //方便copy代码按钮（btn-copy）的定位 position: relative;&#125;.btn-copy &#123; display: inline-block; cursor: pointer; background-color: #eee; background-image: linear-gradient(#fcfcfc,#eee); border: 1px solid #d5d5d5; border-radius: 3px; -webkit-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; -webkit-appearance: none; font-size: 13px; font-weight: 700; line-height: 20px; color: #333; -webkit-transition: opacity .3s ease-in-out; -o-transition: opacity .3s ease-in-out; transition: opacity .3s ease-in-out; padding: 2px 6px; position: absolute; right: 5px; top: 5px; opacity: 0;&#125;.btn-copy span &#123; margin-left: 5px;&#125;.highlight:hover .btn-copy&#123; opacity: 1;&#125; 引用在.\themes\next\layout\_layout.swig文件中，添加引用（注：在 swig 末尾或 body 结束标签（&lt;/body&gt;）之前添加）：123&lt;!-- 代码块复制功能 --&gt;&lt;script type="text/javascript" src="/js/src/clipboard.min.js"&gt;&lt;/script&gt; &lt;script type="text/javascript" src="/js/src/clipboard-use.js"&gt;&lt;/script&gt; 效果的可以去我博客看 补充懂代码的也可以将clipboard.min.js和clipboard-use.js合并为一个文件，再在.\themes\next\layout\_layout.swig文件中使用。当然clipboard.min.js也可以直接用三方cdn的方式引入也行。]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>Next</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo-Next搭建个人博客（主题优化）]]></title>
    <url>%2F2018%2F08%2F27%2FHexo-Next%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%EF%BC%88%E4%B8%BB%E9%A2%98%E4%BC%98%E5%8C%96%EF%BC%89%2F</url>
    <content type="text"><![CDATA[Hexo版本是3.7.1 NexT.Pisces v5.1.4 1. 常用命令 Hexo的命令极简单，安装后只需要记住四个常用的即可。执行命令需要Git当前处于blog文件夹根目录下。 generate 生成静态文件。hexo g server 启动服务器。hexo s deploy 部署网站。部署网站前，需要预先生成静态文件。hexo d clean 清除缓存文件 (db.json) 和已生成的静态文件 (public)。hexo clean 卸载Hexonpm uninstall hexo-cli -g 2.更换主题,以Next主题为例12$ cd your-hexo-site$ git clone https://github.com/iissnan/hexo-theme-next themes/next 修改Hexo 站点目录下的_config.yml的主题 1234# Extensions## Plugins: https://hexo.io/plugins/## Themes: https://hexo.io/themes/theme: next 3.站点初始设置1234567# Sitetitle: Hexo #网站标题subtitle: #网站副标题description: #网站描述author: author #您的名字language: zh-Hans #网站使用的语言timezone: Asia/Shanghai #网站时区。Hexo 默认使用您电脑的时区。 打开Hexo 站点目录下的_config.yml修改内容如下 4.设置主题风格 打开themes/next下的_config.yml文件，搜索 scheme关键字，将你需用启用的scheme 前面注释 # 去除即可。 123456789# ---------------------------------------------------------------# Scheme Settings# ---------------------------------------------------------------# Schemes#scheme: Muse # 默认 Scheme，这是 NexT 最初的版本，黑白主调，大量留白#scheme: Mist # Muse 的紧凑版本，整洁有序的单栏外观scheme: Pisces # 双栏 Scheme，小家碧玉似的清新#scheme: Gemini # 类似 Pisces 5.设置菜单项的显示文本和图标 更新说明：NexT.Pisces v5.1.3, 版本更换了修改菜单图标方式。参看详细信息 NexT 使用的是 Font Awesome 提供的图标， Font Awesome 提供了 600+ 的图标，可以满足绝大的多数的场景，同时无须担心在 Retina 屏幕下 图标模糊的问题。 5.1设置菜单项的显示文本： 打开themes/next/languages下的zh-Hans.yml文件,搜索 menu关键字，修改对应中文或者新增 123456789101112menu: home: 首页 archives: 归档 categories: 分类 tags: 标签 about: 关于 search: 搜索 schedule: 日程表 sitemap: 站点地图 commonweal: 公益404 # 新增menu catalogue: 目录 5.2设定菜单项的图标 打开themes/next下的_config.yml文件，搜索 menu_icons关键字，修改对应图标名称或者新增对应menu的图标 123456789101112131415161718# Enable/Disable menu icons.# Icon Mapping:# Map a menu item to a specific FontAwesome icon name.# Key is the name of menu item and value is the name of FontAwesome icon. Key is case-senstive.# When an question mask icon presenting up means that the item has no mapping icon.menu_icons: enable: true #KeyMapsToMenuItemKey: NameOfTheIconFromFontAwesome home: home about: user categories: th schedule: calendar tags: tags archives: archive sitemap: sitemap commonweal: heartbeat #新增menu_icon catalogue: th-list 5.3设置菜单项对应的文件目录 打开themes/next下的_config.yml文件，搜索 menu关键字，以#注释原有的菜单项，或者新增新的菜单项 123456789101112131415# ---------------------------------------------------------------# Menu Settings# ---------------------------------------------------------------# When running the site in a subdirectory (e.g. domain.tld/blog), remove the leading slash (/archives -&gt; archives)menu: home: / categories: /categories/ #about: /about/ archives: /archives/ #tags: /tags/ #sitemap: /sitemap.xml #commonweal: /404/ #新增menu catalogue: /catalogues/ 除了home，archives,/后面都需要手动创建这个页面 5.4创建菜单项对应文件目录,以分类为例 在终端窗口下，定位到 Hexo 站点目录下。使用 hexo new page 新建一个页面，命名为 categories ： 12$ cd your-hexo-site$ hexo new page categories 5.5编辑刚新建的页面,设置分类12345title: 分类date: 2014-12-22 12:39:04categories: Testing #分类名type: "categories"--- 6.头像设置 6.1添加头像 打开themes/next下的_config.yml文件，搜索 Sidebar Avatar关键字，去掉avatar前面的# 1234# Sidebar Avatar# in theme directory(source/images): /images/avatar.jpg# in site directory(source/uploads): /uploads/avatar.jpgavatar: http://example.com/avatar.png 或者使用本地图片,把图片放入themes/next/source/images下,修改avatar 1avatar: /images/blogLogo.png 6.2设置头像边框为圆形框 打开位于themes/next/source/css/_common/components/sidebar/下的sidebar-author.syl文件,修改如下 123456789101112.site-author-image &#123; display: block; margin: 0 auto; padding: $site-author-image-padding; max-width: $site-author-image-width; height: $site-author-image-height; border: $site-author-image-border-width solid $site-author-image-border-color; // 修改头像边框 border-radius: 50%; -webkit-border-radius: 50%; -moz-border-radius: 50%;&#125; 6.3特效：鼠标放置头像上旋转 123456789101112131415161718192021.site-author-image &#123; display: block; margin: 0 auto; padding: $site-author-image-padding; max-width: $site-author-image-width; height: $site-author-image-height; border: $site-author-image-border-width solid $site-author-image-border-color; // 修改头像边框 border-radius: 50%; -webkit-border-radius: 50%; -moz-border-radius: 50%; // 设置旋转 transition: 1.4s all;&#125;// 可旋转的圆形头像,`hover`动作.site-author-image:hover &#123; -webkit-transform: rotate(360deg); -moz-transform: rotate(360deg); -ms-transform: rotate(360deg); -transform: rotate(360deg);&#125; 7.浏览页面的时候显示当前浏览进度 打开themes/next下的_config.yml,搜索关键字scrollpercent,把false改为true 12# Scroll percent label in b2t button scrollpercent: true 如果想把top按钮放在侧边栏,打开themes/next下的_config.yml,搜索关键字b2t,把false改为true 12345# Back to top in sidebar b2t: true # Scroll percent label in b2t button scrollpercent: true 效果如下图： 8.文章创建和删除 创建文章 123 $ cd you-site $ hexo new post "you title"# 可以使用n代替new 文章目录you-site/source/_posts 删除文章 123$ hexo clean在/source/_posts/中直接删除了相应的.md文件$ hexo g 9.标签设置 9.1创建标签目录 hexo初始是没有标签目录的需要自己创建 12$ cd you-site$ hexo new page tags 创建完成后,打开you-site/source/tags的index.md,修改如下 12345---title: #页面主题date: 2017-08-18 15:00:55 #当前创建文件时间type: "tags" # 设置页面类型--- 得到如下界面 9.2设置标签云 标签云的生成:是根据你创建的文章，设定标签类型，自定生成的。某个标签下的 文章越多则，标签越高大 设置文章标签:打开you-site/source/_posts的you title.md,默认tags:为空,后面加上标签名即可 1234567---layout: layouttitle: 标签1date: 2017-08-18 15:41:18tags: 标签1 #此文章在`标签1 `标签下#tags: [标签1,标签2] #此文章在`标签1,标签2`下--- 10侧边栏设置10.1 设置侧边栏社交链接 打开themes/next下的_config.yml文件,搜索关键字social,然后添加社交站点名称与地址即可。 12345678# Social linkssocial: GitHub: https://github.com/your-user-name Twitter: https://twitter.com/your-user-name Weibo: https://weibo.com/your-user-name douban: https://douban.com/people/your-user-name zhihu: https://www.zhihu.com/people/your-user-name # 等等 10.2 设置侧边栏社交图标 打开themes/next下的_config.yml文件,搜索关键字social_icons，添加社交站点名称（注意大小写）图标，Font Awesome图标地址 12345678social_icons: enable: true # Icon Mappings. # KeyMapsToSocalItemKey: NameOfTheIconFromFontAwesome GitHub: github Twitter: twitter Weibo: weibo Linkedin: linkedin 10.3RSS 在你Hexo 站点目录下 1$ npm install hexo-generator-feed --save 打开Hexo 站点下的_config.yml,添加如下配置 12345678# feed# Dependencies: https://github.com/hexojs/hexo-generator-feedfeed: type: atom path: atom.xml limit: 20 hub: content: 10.4友情链接 打开themes/next下的_config.yml文件,搜索关键字Blog rolls 1234567# Blog rollslinks_title: 友情链接 #标题links_layout: block #布局，一行一个连接#links_layout: inlinelinks: #连接 baidu: http://example.com/ google: http://example.com/ 11好玩的写作样式用一些特殊的样式，可以增加文章的可读性。不过也不是越多越好，没必要写一篇文章就把下面的样式全部用一遍，这样只会适得其反，从下面的样式中选几个自己觉得比较好的、经常会用的就行。而且写博客重点是文章的文字内容，而不是这些样式，样式只是为了让文章更美观，更适合阅读。这和我们用 Markdown 写文章是一样的道理，用 Markdown 而不是直接写 HTML 代码，就是为了将更多时间花在文字上。 11.01主题自带样式 文本居中引用效果：人生乃是一面镜子，从镜子里认识自己，我要称之为头等大事，也只是我们追求的目的！ 源码：123456&#123;% cq %&#125;人生乃是一面镜子，从镜子里认识自己，我要称之为头等大事，也只是我们追求的目的！&#123;% endcq %&#125; 更多 NexT 主题自带的标签样式，请点击：http://theme-next.iissnan.com/tag-plugins.html 11.02主题自带样式 note 标签在主题配置文件_config.yml里有一个关于这个的配置，但官方文档没有提供 HTML 的使用方式，个人认为这种方式更简单，也不会产生一些奇怪的显示 bug…… 1&lt;div class="note default"&gt;&lt;p&gt;default&lt;/p&gt;&lt;/div&gt; default 1&lt;div class="note primary"&gt;&lt;p&gt;primary&lt;/p&gt;&lt;/div&gt; primary 1&lt;div class="note success"&gt;&lt;p&gt;success&lt;/p&gt;&lt;/div&gt; success 1&lt;div class="note info"&gt;&lt;p&gt;info&lt;/p&gt;&lt;/div&gt; info 1&lt;div class="note warning"&gt;&lt;p&gt;warning&lt;/p&gt;&lt;/div&gt; warning 1&lt;div class="note danger"&gt;&lt;p&gt;danger&lt;/p&gt;&lt;/div&gt; danger 1&lt;div class="note danger no-icon"&gt;&lt;p&gt;danger no-icon&lt;/p&gt;&lt;/div&gt; danger no-icon 首先可以在主题配置文件中需要配置下，贴上我的：123456789# Note tag (bs-callout).note: # 风格 style: flat # 要不要图标 icons: true # 圆角矩形 border_radius: 3 light_bg_offset: 里面的三种风格长啥样？开启图标长啥样？可以查看这个页面，更多的介绍也在这个页面，请自行查看。 11.03主题自带样式 label 标签首先可以在主题配置文件中有配置，需要配置下，贴上我的:12# Label tag.label: true 然后效果如下（@ 前面的是label的名字，后面的是要显示的文字）： default 1&#123;% label default@default %&#125; primary 1&#123;% label primary@primary %&#125; success 1&#123;% label success@success %&#125; info 1&#123;% label info@info %&#125; warning 1&#123;% label warning@warning %&#125; danger 1&#123;% label danger@danger %&#125; 11.03主题自带样式 tabs 标签效果：选项卡 1选项卡 2选项卡 3这是选项卡 1 呵呵哈哈哈哈哈哈哈哈呵呵哈哈哈哈哈哈哈哈呵呵哈哈哈哈哈哈哈哈呵呵哈哈哈哈哈哈哈哈呵呵哈哈哈哈哈哈哈哈呵呵哈哈哈哈哈哈哈哈……这是选项卡 2这是选项卡 3 哇，你找到我了！φ(≧ω≦*)♪～ 源码：1234567891011&#123;% tabs 选项卡, 2 %&#125;&lt;!-- tab --&gt;**这是选项卡 1** 呵呵哈哈哈哈哈哈哈哈呵呵哈哈哈哈哈哈哈哈呵呵哈哈哈哈哈哈哈哈呵呵哈哈哈哈哈哈哈哈呵呵哈哈哈哈哈哈哈哈呵呵哈哈哈哈哈哈哈哈……&lt;!-- endtab --&gt;&lt;!-- tab --&gt;**这是选项卡 2**&lt;!-- endtab --&gt;&lt;!-- tab --&gt;**这是选项卡 3** 哇，你找到我了！φ(≧ω≦*)♪～&lt;!-- endtab --&gt;&#123;% endtabs %&#125; 首先可以在主题配置文件中有配置，需要配置下，贴上我的：1234567# Tabs tag.tabs: enable: true transition: tabs: true labels: true border_radius: 0 然后上面源码中, 2表示一开始在第二个选项卡，非必须，若数值为-1则隐藏选项卡内容。更多用法请查看这个页面。 11.04主题自带样式 tabs 标签源码：1&#123;% btn https://www.baidu.com, 点击下载百度, download fa-lg fa-fw %&#125; 效果：点击下载百度关于按钮的更多使用可以前往这个页面查看。]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>Next</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用 Python 生成二维码]]></title>
    <url>%2F2018%2F08%2F27%2F%E4%BD%BF%E7%94%A8-Python-%E7%94%9F%E6%88%90%E4%BA%8C%E7%BB%B4%E7%A0%81%2F</url>
    <content type="text"><![CDATA[&nbsp;&nbsp;&nbsp;&nbsp;新时代，人们有人信新的追求，自然而然会有新发明的诞生。去年，在“一带一路”国际合作高峰论坛举行期间，20国青年投票选出中国的“新四大发明”：高铁、扫码支付、共享单车和网购。其中扫码支付指手机通过扫描二维码跳转到支付页面，再进行付款。这种新的支付方式，造就二维码满天飞的现象。那么让我们来扒一扒如何使用 Python 来生成二维码图片。 1 二维码二维码（2-dimensional bar code），是用某种特定的几何图形按一定规律在平面（二维方向上）分布的黑白相间的图形记录数据符号信息的。它能将数字、英文字母、汉字、日文字母、特殊符号(如空格，%，/ 等)、二进制等信息记录到一个正方形的图片中。 因此，在转换的过程中，离不开编码压缩方式。在许多种类的二维条码中，常用的码制有：Data Matrix, Maxi Code, Aztec, QR Code, Vericode, PDF417, Ultracode, Code 49, Code 16K等。 二维码在现实生活中的应用越来与普遍，归于功于 QR code 码制的流行。我们常说的二维码就是它。所以，二维码又被称为 QR code。 QR code 是一种矩阵式二维条码（又称棋盘式二维条码）。它是在一个矩形空间通过黑、白像素在矩阵中的不同分布进行编码。在矩阵相应元素位置上，用点（方点、圆点或其他形状）的出现表示二进制“1”，点的不出现表示二进制的“0”，点的排列组合确定了矩阵式二维条码所代表的意义。 2 二维码结构我们的目的是要使用 Python 生成 QR 码，那我们需要先了解二维码(QR 码)的结构。根据标准（ISO/IEC 18004），我们可以了解到 QR 码结构如下： 图片来源网络 1) 功能图形功能图形是不参与编码数据的区域。它包含空白区、位置探测图形、位置探测图形分隔符、定位图形、校正图形五大模块。 空白区 空白区顾名思义就是要留空白。因此，这里不能有任何图样或标记。这样才能保证 QR 能被识别。 位置探测图形 这个有点类似中文的“回”字。在 QR 码中有个这样的标识，它分别的左上、右上和左下角。作用是协助扫描软件定位 QR 码并转换坐标系。我们在扫描二维码的时候，不管是竖着扫、横着扫、斜着扫都能识别出内容，主要是它的功劳。 位置探测图形分隔符 主要作用是区分功能图形和编码区域。 定位图形 它由黑白间隔的各自各自组成的线条。主要用于指示标识密度和确定坐标系。原因是 QR 码一种有 40 个版本，也就是说有 40 种尺寸。每种二维码的尺寸越大，扫描的距离就越远。 校正图形 只有 Version 2 及以上的QR码有校正标识。校正标识用于进一步校正坐标系。 2) 编码区域编码区域是数据进行编码存储的区域。它由格式信息、版本信息、数据和纠错码字三部分构成。 格式信息 所有尺寸的二维码都有该信息。它存放一些格式化数据的信息，例如容错级别、数据掩码，和额外的自身 BCH 容错码。 版本信息 版本信息是规定二维码的规格。前面讲到 QR 码一共有 40 种规格的矩阵（一般为黑白色），从21x21（版本1），到177x177（版本40），每一版本符号比前一版本 每边增加4个模块。 数据和纠错码 主要是存储实际数据以及用于纠错码字。 3 二维码的绘制过程二维码已经是有一套国际标准，绘制二维码过程的严格按照标准来执行。这个过程是比较复杂，我自己也是看了大概，然后总结出大致绘制过程。如果你想深入了解绘制细节，可以阅读标准。 二维码的绘制大概过程如下：1）在二维码的左上角、左下角、右上角绘制位置探测图形。位置探测图形一定是一个 7x7 的矩阵。2）绘制校正图形。校正图形一定是一个 5x5 的矩阵。3）绘制两条连接三个位置探测图形的定位图形。4）在上述图片的基础上，继续绘制格式信息。5）接着绘制版本信息。6）填充数据码和纠错码到二维码图中。7）最后是绘制蒙版图案。因为按照上述方式填充内容，可能会出现大面积的空白或黑块的情况，导致扫描识别会十分困难。所以需要对整个图像与蒙版进行蒙版操作(Masking)，蒙版操作即为异或 XOR 操作。在这一步，我们可以将数据排列成各种图片。 4 二维码的生成我们既然已经了解二维码原理，那么可以利用 Python 生成二维码。然而网络上高人比比皆是。已经有大神编写了 Python 生成二维码的第三方库，所以我们不需要重复造轮子, 使用现成的库即可。 我就推荐两个库：qrcode&nbsp;和&nbsp;python-qrcode。 qrcode qrcode 运行在 Python 3 版本上，它可以玩出很多花样。例如能生成以下三种二维码图片：普通二维码、带图片的艺术二维码（黑白与彩色）、动态二维码（黑白与彩色）。它比较适合直接用于生成二维码图片的场景。 安装 qrcode 库可以使用 pip 方式。但是该库依赖&nbsp;pillow、numpy&nbsp;和&nbsp;imageio。因此，我们需要先安装依赖库，再安装 qrcode。最后的安装命令如下： 12345# 逐一安装pip install pillowpip install numpypip install imageiopip install myqr 该库生成带图片的艺术二维码算是一大亮点，具体用法如下: 1myqr https://github.com -p github.jpg -c 上述命令作用是将 github 主页写到彩色二维码中。 该库还支持生成 gif 的彩色二维码图片，具体用法如下：1myqr https://github.com -p github.gif -c -con 1.5 -bri 1.6 效果图如下： 最后补上该库的 Github 地址：https://github.com/sylnsfar/qrcode python-qrcode python-qrcode&nbsp;相比&nbsp;qrcode&nbsp;要稍微逊色一点。不过它也有自己的特色。它支持生成矢量图，而且比较适合在代码中生成二维码的场景。 安装 python-qrcode 同样建议使用 pip 方式，安装命令如下：1pip install qrcode 在 Python 代码中，最简单的用法是这样。1import qrcodeimg = qrcode.make('https://github.com') 它也支持自定义二维码的信息，具体用法如下：1234567891011import qrcodeqr = qrcode.QRCode( version=1, error_correction=qrcode.constants.ERROR_CORRECT_L, box_size=10, border=4,)qr.add_data('https://github.com')qr.make(fit=True)img = qr.make_image(fill_color="black", back_color="white") 如果你想深入了解该库，可以到 Github 仓库阅读相关的文档。Github 地址是：https://github.com/lincolnloop/python-qrcode 此文摘自微信公众号【Python中文社区】微信扫一扫关注该公众号]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[都是套路：高并发系统的降级特技]]></title>
    <url>%2F2018%2F08%2F26%2F%E9%83%BD%E6%98%AF%E5%A5%97%E8%B7%AF%EF%BC%9A%E9%AB%98%E5%B9%B6%E5%8F%91%E7%B3%BB%E7%BB%9F%E7%9A%84%E9%99%8D%E7%BA%A7%E7%89%B9%E6%8A%80%2F</url>
    <content type="text"><![CDATA[&nbsp;&nbsp;&nbsp;&nbsp;在开发高并发系统时有三把利器用来保护系统：缓存、降级和限流。之前已经有一些文章介绍过缓存和限流了。本文将详细聊聊降级。 当访问量剧增、服务出现问题（如响应时间慢或不响应）或非核心服务影响到核心流程的性能时，仍然需要保证服务还是可用的，即使是有损服务。 系统可以根据一些关键数据进行自动降级，也可以配置开关实现人工降级。本文将介绍一些笔者在实际工作中遇到的或见到过的一些降级方案供大家参考。 降级的最终目的是保证核心服务可用，即使是有损的。而且有些服务是无法降级的（如加入购物车、结算）。 降级预案在进行降级之前要对系统进行梳理，看看系统是不是可以丢卒保帅；从而梳理出哪些必须誓死保护，哪些可降级；比如可以参考日志级别设置预案： 一般：比如有些服务偶尔因为网络抖动或者服务正在上线而超时，可以自动降级； 警告：有些服务在一段时间内成功率有波动（如在95~100%之间），可以自动降级或人工降级，并发送告警； 错误：比如可用率低于90%，或者数据库连接池被打爆了，或者访问量突然猛增到系统能承受的最大阀值，此时可以根据情况自动降级或者人工降级； 严重错误：比如因为特殊原因数据错误了，此时需要紧急人工降级。 降级的类别 降级按照是否自动化可分为：自动开关降级和人工开关降级。 降级按照功能可分为：读服务降级、写服务降级。 降级按照处于的系统层次可分为：多级降级。 降级的功能点降级的功能点主要从服务端链路考虑，即根据用户访问的服务调用链路来梳理哪里需要降级： 页面降级：在大促或者某些特殊情况下，某些页面占用了一些稀缺服务资源，在紧急情况下可以对其整个降级，以达到丢卒保帅； 页面片段降级：比如商品详情页中的商家部分因为数据错误了，此时需要对其进行降级； 页面异步请求降级：比如商品详情页上有推荐信息/配送至等异步加载的请求，如果这些信息响应慢或者后端服务有问题，可以进行降级； 服务功能降级：比如渲染商品详情页时需要调用一些不太重要的服务：相关分类、热销榜等，而这些服务在异常情况下直接不获取，即降级即可； 读降级：比如多级缓存模式，如果后端服务有问题，可以降级为只读缓存，这种方式适用于对读一致性要求不高的场景； 写降级：比如秒杀抢购，我们可以只进行Cache的更新，然后异步同步扣减库存到DB，保证最终一致性即可，此时可以将DB降级为Cache。 爬虫降级：在大促活动时，可以将爬虫流量导向静态页或者返回空数据从而降级保护后端稀缺资源。 降级策略1、自动开关降级自动降级是根据系统负载、资源使用情况、SLA等指标进行降级。 超时降级当访问的数据库/http服务/远程调用响应慢或者长时间响应慢，且该服务不是核心服务的话可以在超时后自动降级； 比如商品详情页上有推荐内容/评价，但是推荐内容/评价暂时不展示对用户购物流程不会产生很大的影响 对于这种服务是可以超时降级的。如果是调用别人的远程服务，和对方定义一个服务响应最大时间，如果超时了则自动降级。 之前总结过一些的文章《使用httpclient必须知道的参数设置及代码写法、存在的风险》和《dbcp配置及jdbc超时设置总结》。在实际场景用一定主要配置好超时时间和超时重试次数和机制。 统计失败次数降级有时候依赖一些不稳定的API，比如调用外部机票服务，当失败调用次数达到一定阀值自动降级；然后通过异步线程去探测服务是否恢复了，则取消降级。 故障降级比如要调用的远程服务挂掉了（网络故障、DNS故障、http服务返回错误的状态码、rpc服务抛出异常），则可以直接降级。 降级后的处理方案有： 默认值（比如库存服务挂了，返回默认现货）兜底数据（比如广告挂了，返回提前准备好的一些静态页面）缓存（之前暂存的一些缓存数据） 限流降级当我们去秒杀或者抢购一些限购商品时，此时可能会因为访问量太大而导致系统崩溃，此时开发者会使用限流来进行限制访问量，当达到限流阀值，后续请求会被降级； 降级后的处理方案可以是： 排队页面（将用户导流到排队页面等一会重试）无货（直接告知用户没货了）错误页（如活动太火爆了，稍后重试） 2、人工开关降级1. 在大促期间通过监控发现线上的一些服务存在问题，这个时候需要暂时将这些服务摘掉；2. 还有有时候通过任务系统调用一些服务，但是服务依赖的数据库可能存在：网卡被打满了、挂掉了或者很多慢查询，此时需要暂停下任务系统让服务方进行处理；3. 还有发现突然调用量太大，可能需要改变处理方式（比如同步转换为异步）； 此时就可以使用开关来完成降级。 开关可以存放到配置文件、存放到数据库、存放到Redis/ZooKeeper；如果不是存放在本地，可以定期同步开关数据（比如1秒同步一次）。然后通过判断某个KEY的值来决定是否降级。 另外对于新开发的服务想上线进行灰度测试；但是不太确定该服务的逻辑是否正确，此时就需要设置开关，当新服务有问题可以通过开关切换回老服务。 还有多机房服务，如果某个机房挂掉了，此时需要将一个机房的服务切到另一个机房，此时也可以通过开关完成切换。 还有一些是因为功能问题需要暂时屏蔽掉某些功能，比如商品规格参数数据有问题，数据问题不能用回滚解决，此时需要开关控制降级。 3、读服务降级对于读服务降级一般采用的策略有： 暂时切换读（降级到读缓存、降级到走静态化）暂时屏蔽读（屏蔽读入口、屏蔽某个读服务） 在《应用多级缓存模式支撑海量读服务》中曾经介绍过读服务，即： 接入层缓存→应用层本地缓存→分布式缓存→RPC服务/DB 我们会在接入层、应用层设置开关，当分布式缓存、RPC服务/DB有问题自动降级为不调用。当然这种情况适用于对读一致性要求不高的场景。 页面降级、页面片段降级、页面异步请求降级都是读服务降级，目的是丢卒保帅（比如因为这些服务也要使用核心资源、或者占了带宽影响到核心服务）或者因数据问题暂时屏蔽。 还有一种是页面静态化场景： 动态化降级为静态化：比如平时网站可以走动态化渲染商品详情页，但是到了大促来临之际可以将其切换为静态化来减少对核心资源的占用，而且可以提升性能；其他还有如列表页、首页、频道页都可以这么玩；可以通过一个程序定期的推送静态页到缓存或者生成到磁盘，出问题时直接切过去； 静态化降级为动态化：比如当使用静态化来实现商品详情页架构时，平时使用静态化来提供服务，但是因为特殊原因静态化页面有问题了，需要暂时切换回动态化来保证服务正确性。 以上都保证出问题了有预案，用户还是可以使用网站，不影响用户购物。 4、写服务降级写服务在大多数场景下是不可降级的，不过可以通过一些迂回战术来解决问题。比如将同步操作转换为异步操作，或者限制写的量/比例。 比如扣减库存一般这样操作： 方案1：a、扣减DB库存； b、扣减成功后更新Redis中的库存； 方案2：a、扣减Redis库存； b、同步扣减DB库存，如果扣减失败则回滚Redis库存； 前两种方案非常依赖DB，假设此时DB性能跟不上则扣减库存就会遇到问题；因此我们可以想到方案3： a、扣减Redis库存： b、正常同步扣减DB库存，性能扛不住时降级为发送一条扣减DB库存的消息，然后异步进行DB库存扣减实现最终一致即可； 这种方式发送扣减DB库存消息也可能成为瓶颈；这种情况我们可以考虑方案4： a、扣减Redis库存； b、正常同步扣减DB库存，性能扛不住时降级为写扣减DB库存消息到本机，然后本机通过异步进行DB库存扣减来实现最终一致性。 也就是说正常情况可以同步扣减库存，在性能扛不住时降级为异步；另外如果是秒杀场景可以直接降级为异步，从而保护系统。 还有如下单操作可以在大促时暂时降级将下单数据写入Redis，然后等峰值过去了再同步回DB，当然也有更好的解决方案，但是更复杂，不是本文的重点。 还有如用户评价，如果评价量太大，也可以把评价从同步写降级为异步写。当然也可以对评价按钮进行按比例开放（比如一些人的看不到评价操作按钮）。比如评价成功后会发一些奖励，在必要的时候降级同步到异步。 5、多级降级缓存是离用户最近越高效；而降级是离用户越近越能对系统保护的好。因为业务的复杂性导致越到后端QPS/TPS越低。 页面JS降级开关：主要控制页面功能的降级，在页面中通过JS脚本部署功能降级开关，在适当时机开启/关闭开关； 接入层降级开关：主要控制请求入口的降级，请求进入后会首先进入接入层，在接入层可以配置功能降级开关，可以根据实际情况进行自动/人工降级； 这个可以参考《京东商品详情页服务闭环实践》，尤其在后端应用服务出问题时，通过接入层降级从而给应用服务有足够的时间恢复服务； 应用层降级开关：主要控制业务的降级，在应用中配置相应的功能开关，根据实际业务情况进行自动/人工降级。 总结：降级能保障系统在大促中活下来，而不是死去，达到丢卒保帅的作用。对用户提供有损服务，总比不服务要好。根据自己的场景设计相应的降级策略，保障系统在危机时刻能通过降级手段平稳度过。 EnweiTech原创作品。转载请注明出处https://blog.csdn.net/enweitech]]></content>
      <categories>
        <category>架构设计</category>
      </categories>
      <tags>
        <tag>高并发</tag>
        <tag>降级</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[缓存雪崩、缓存穿透、缓存预热、缓存更新、缓存降级等问题]]></title>
    <url>%2F2018%2F08%2F26%2F%E7%BC%93%E5%AD%98%E9%9B%AA%E5%B4%A9%E3%80%81%E7%BC%93%E5%AD%98%E7%A9%BF%E9%80%8F%E3%80%81%E7%BC%93%E5%AD%98%E9%A2%84%E7%83%AD%E3%80%81%E7%BC%93%E5%AD%98%E6%9B%B4%E6%96%B0%E3%80%81%E7%BC%93%E5%AD%98%E9%99%8D%E7%BA%A7%E7%AD%89%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[&nbsp;&nbsp;&nbsp;&nbsp;前面一节说到了《为什么说Redis是单线程的以及Redis为什么这么快！》，今天给大家整理一篇关于Redis经常被问到的问题：缓存雪崩、缓存穿透、缓存预热、缓存更新、缓存降级等概念的入门及简单解决方案。 一、缓存雪崩&nbsp;&nbsp;&nbsp;&nbsp;缓存雪崩我们可以简单的理解为：由于原有缓存失效，新缓存未到期间(例如：我们设置缓存时采用了相同的过期时间，在同一时刻出现大面积的缓存过期)，所有原本应该访问缓存的请求都去查询数据库了，而对数据库CPU和内存造成巨大压力，严重的会造成数据库宕机。从而形成一系列连锁反应，造成整个系统崩溃。 缓存正常从Redis中获取，示意图如下： 缓存失效瞬间示意图如下： &nbsp;&nbsp;&nbsp;&nbsp;缓存失效时的雪崩效应对底层系统的冲击非常可怕！大多数系统设计者考虑用加锁或者队列的方式保证来保证不会有大量的线程对数据库一次性进行读写，从而避免失效时大量的并发请求落到底层存储系统上。还有一个简单方案就时讲缓存失效时间分散开，比如我们可以在原有的失效时间基础上增加一个随机值，比如1-5分钟随机，这样每一个缓存的过期时间的重复率就会降低，就很难引发集体失效的事件。 以下简单介绍两种实现方式的伪代码： （1）碰到这种情况，一般并发量不是特别多的时候，使用最多的解决方案是加锁排队，伪代码如下： 1234567891011121314151617181920212223//伪代码public object GetProductListNew() &#123; int cacheTime = 30; String cacheKey = "product_list"; String lockKey = cacheKey; String cacheValue = CacheHelper.get(cacheKey); if (cacheValue != null) &#123; return cacheValue; &#125; else &#123; synchronized(lockKey) &#123; cacheValue = CacheHelper.get(cacheKey); if (cacheValue != null) &#123; return cacheValue; &#125; else &#123; //这里一般是sql查询数据 cacheValue = GetProductListFromDB(); CacheHelper.Add(cacheKey, cacheValue, cacheTime); &#125; &#125; return cacheValue; &#125;&#125; &nbsp;&nbsp;&nbsp;&nbsp;加锁排队只是为了减轻数据库的压力，并没有提高系统吞吐量。假设在高并发下，缓存重建期间key是锁着的，这是过来1000个请求999个都在阻塞的。同样会导致用户等待超时，这是个治标不治本的方法！ &nbsp;&nbsp;&nbsp;&nbsp;注意：加锁排队的解决方式分布式环境的并发问题，有可能还要解决分布式锁的问题；线程还会被阻塞，用户体验很差！因此，在真正的高并发场景下很少使用！ （2）还有一个解决办法解决方案是：给每一个缓存数据增加相应的缓存标记，记录缓存的是否失效，如果缓存标记失效，则更新数据缓存，实例伪代码如下： 1234567891011121314151617181920212223//伪代码public object GetProductListNew() &#123; int cacheTime = 30; String cacheKey = "product_list"; //缓存标记 String cacheSign = cacheKey + "_sign"; String sign = CacheHelper.Get(cacheSign); //获取缓存值 String cacheValue = CacheHelper.Get(cacheKey); if (sign != null) &#123; return cacheValue; //未过期，直接返回 &#125; else &#123; CacheHelper.Add(cacheSign, "1", cacheTime); ThreadPool.QueueUserWorkItem((arg) -&gt; &#123; //这里一般是 sql查询数据 cacheValue = GetProductListFromDB(); //日期设缓存时间的2倍，用于脏读 CacheHelper.Add(cacheKey, cacheValue, cacheTime * 2); &#125;); return cacheValue; &#125;&#125; 解释说明： 1、缓存标记：记录缓存数据是否过期，如果过期会触发通知另外的线程在后台去更新实际key的缓存； 2、缓存数据：它的过期时间比缓存标记的时间延长1倍，例：标记缓存时间30分钟，数据缓存设置为60分钟。 这样，当缓存标记key过期后，实际缓存还能把旧数据返回给调用端，直到另外的线程在后台更新完成后，才会返回新缓存。 &nbsp;&nbsp;&nbsp;&nbsp;关于缓存崩溃的解决方法，这里提出了三种方案：使用锁或队列、设置过期标志更新缓存、为key设置不同的缓存失效时间，还有一各被称为“二级缓存”的解决方法，有兴趣的读者可以自行研究。 二、缓存穿透&nbsp;&nbsp;&nbsp;&nbsp;缓存穿透是指用户查询数据，在数据库没有，自然在缓存中也不会有。这样就导致用户查询的时候，在缓存中找不到，每次都要去数据库再查询一遍，然后返回空（相当于进行了两次无用的查询）。这样请求就绕过缓存直接查数据库，这也是经常提的缓存命中率问题。 &nbsp;&nbsp;&nbsp;&nbsp;有很多种方法可以有效地解决缓存穿透问题，最常见的则是采用布隆过滤器，将所有可能存在的数据哈希到一个足够大的bitmap中，一个一定不存在的数据会被这个bitmap拦截掉，从而避免了对底层存储系统的查询压力。 &nbsp;&nbsp;&nbsp;&nbsp;另外也有一个更为简单粗暴的方法，如果一个查询返回的数据为空（不管是数据不存在，还是系统故障），我们仍然把这个空结果进行缓存，但它的过期时间会很短，最长不超过五分钟。通过这个直接设置的默认值存放到缓存，这样第二次到缓冲中获取就有值了，而不会继续访问数据库，这种办法最简单粗暴！123456789101112131415161718192021222324//伪代码public object GetProductListNew() &#123; int cacheTime = 30; String cacheKey = "product_list"; String cacheValue = CacheHelper.Get(cacheKey); if (cacheValue != null) &#123; return cacheValue; &#125; cacheValue = CacheHelper.Get(cacheKey); if (cacheValue != null) &#123; return cacheValue; &#125; else &#123; //数据库查询不到，为空 cacheValue = GetProductListFromDB(); if (cacheValue == null) &#123; //如果发现为空，设置个默认值，也缓存起来 cacheValue = string.Empty; &#125; CacheHelper.Add(cacheKey, cacheValue, cacheTime); return cacheValue; &#125;&#125; 把空结果，也给缓存起来，这样下次同样的请求就可以直接返回空了，即可以避免当查询的值为空时引起的缓存穿透。同时也可以单独设置个缓存区域存储空值，对要查询的key进行预先校验，然后再放行给后面的正常缓存处理逻辑。 三、缓存预热&nbsp;&nbsp;&nbsp;&nbsp;缓存预热这个应该是一个比较常见的概念，相信很多小伙伴都应该可以很容易的理解，缓存预热就是系统上线后，将相关的缓存数据直接加载到缓存系统。这样就可以避免在用户请求的时候，先查询数据库，然后再将数据缓存的问题！用户直接查询事先被预热的缓存数据！ 解决思路： 1、直接写个缓存刷新页面，上线时手工操作下； 2、数据量不大，可以在项目启动的时候自动进行加载； 3、定时刷新缓存； 四、缓存更新&nbsp;&nbsp;&nbsp;&nbsp;除了缓存服务器自带的缓存失效策略之外（Redis默认的有6中策略可供选择），我们还可以根据具体的业务需求进行自定义的缓存淘汰，常见的策略有两种： （1）定时去清理过期的缓存； （2）当有用户请求过来时，再判断这个请求所用到的缓存是否过期，过期的话就去底层系统得到新数据并更新缓存。 &nbsp;&nbsp;&nbsp;&nbsp;两者各有优劣，第一种的缺点是维护大量缓存的key是比较麻烦的，第二种的缺点就是每次用户请求过来都要判断缓存失效，逻辑相对比较复杂！具体用哪种方案，大家可以根据自己的应用场景来权衡。 五、缓存降级&nbsp;&nbsp;&nbsp;&nbsp;当访问量剧增、服务出现问题（如响应时间慢或不响应）或非核心服务影响到核心流程的性能时，仍然需要保证服务还是可用的，即使是有损服务。系统可以根据一些关键数据进行自动降级，也可以配置开关实现人工降级。 &nbsp;&nbsp;&nbsp;&nbsp;降级的最终目的是保证核心服务可用，即使是有损的。而且有些服务是无法降级的（如加入购物车、结算）。 &nbsp;&nbsp;&nbsp;&nbsp;在进行降级之前要对系统进行梳理，看看系统是不是可以丢卒保帅；从而梳理出哪些必须誓死保护，哪些可降级；比如可以参考日志级别设置预案： （1）一般：比如有些服务偶尔因为网络抖动或者服务正在上线而超时，可以自动降级； （2）警告：有些服务在一段时间内成功率有波动（如在95~100%之间），可以自动降级或人工降级，并发送告警； （3）错误：比如可用率低于90%，或者数据库连接池被打爆了，或者访问量突然猛增到系统能承受的最大阀值，此时可以根据情况自动降级或者人工降级； （4）严重错误：比如因为特殊原因数据错误了，此时需要紧急人工降级。 六、总结&nbsp;&nbsp;&nbsp;&nbsp;这些都是实际项目中，可能碰到的一些问题，也是面试的时候经常会被问到的知识点，实际上还有很多很多各种各样的问题，文中的解决方案，也不可能满足所有的场景，相对来说只是对该问题的入门解决方法。一般正式的业务场景往往要复杂的多，应用场景不同，方法和解决方案也不同，由于上述方案，考虑的问题并不是很全面，因此并不适用于正式的项目开发，但是可以作为概念理解入门，具体解决方案要根据实际情况来确定！ 参考文章： 1、http://www.cnblogs.com/zhangweizhong/p/6258797.html2、http://www.cnblogs.com/zhangweizhong/p/5884761.html3、http://blog.csdn.net/zeb_perfect/article/details/54135506]]></content>
      <categories>
        <category>redis</category>
      </categories>
      <tags>
        <tag>redis</tag>
        <tag>缓存</tag>
        <tag>集群分布式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[为什么说Redis是单线程的以及Redis为什么这么快！]]></title>
    <url>%2F2018%2F08%2F26%2F%E4%B8%BA%E4%BB%80%E4%B9%88%E8%AF%B4Redis%E6%98%AF%E5%8D%95%E7%BA%BF%E7%A8%8B%E7%9A%84%E4%BB%A5%E5%8F%8ARedis%E4%B8%BA%E4%BB%80%E4%B9%88%E8%BF%99%E4%B9%88%E5%BF%AB%EF%BC%81%2F</url>
    <content type="text"><![CDATA[一、前言&nbsp;&nbsp;&nbsp;&nbsp;近乎所有与Java相关的面试都会问到缓存的问题，基础一点的会问到什么是“二八定律”、什么是“热数据和冷数据”，复杂一点的会问到缓存雪崩、缓存穿透、缓存预热、缓存更新、缓存降级等问题，这些看似不常见的概念，都与我们的缓存服务器相关，一般常用的缓存服务器有Redis、Memcached等，而笔者目前最常用的也只有Redis这一种。 &nbsp;&nbsp;&nbsp;&nbsp;如果你在以前面试的时候还没有遇到过面试官问你《为什么说Redis是单线程的以及Redis为什么这么快！》，那么你看到这篇文章的时候，你应该觉得是一件很幸运的事情！如果你刚好是一位高逼格的面试官，你也可以拿这道题去面试对面“望穿秋水”般的小伙伴，测试一下他的掌握程度。 &nbsp;&nbsp;&nbsp;&nbsp;好啦！步入正题！我们先探讨一下Redis是什么，Redis为什么这么快、然后在探讨一下为什么Redis是单线程的？ 二、Redis简介&nbsp;&nbsp;&nbsp;&nbsp;Redis是一个开源的内存中的数据结构存储系统，它可以用作：数据库、缓存和消息中间件。 &nbsp;&nbsp;&nbsp;&nbsp;它支持多种类型的数据结构，如字符串（String），散列（Hash），列表（List），集合（Set），有序集合（Sorted Set或者是ZSet）与范围查询，Bitmaps，Hyperloglogs 和地理空间（Geospatial）索引半径查询。其中常见的数据结构类型有：String、List、Set、Hash、ZSet这5种。 &nbsp;&nbsp;&nbsp;&nbsp;Redis 内置了复制（Replication），LUA脚本（Lua scripting）， LRU驱动事件（LRU eviction），事务（Transactions） 和不同级别的磁盘持久化（Persistence），并通过 Redis哨兵（Sentinel）和自动分区（Cluster）提供高可用性（High Availability）。 &nbsp;&nbsp;&nbsp;&nbsp;Redis也提供了持久化的选项，这些选项可以让用户将自己的数据保存到磁盘上面进行存储。根据实际情况，可以每隔一定时间将数据集导出到磁盘（快照），或者追加到命令日志中（AOF只追加文件），他会在执行写命令时，将被执行的写命令复制到硬盘里面。您也可以关闭持久化功能，将Redis作为一个高效的网络的缓存数据功能使用。 &nbsp;&nbsp;&nbsp;&nbsp;Redis不使用表，他的数据库不会预定义或者强制去要求用户对Redis存储的不同数据进行关联。 &nbsp;&nbsp;&nbsp;&nbsp;数据库的工作模式按存储方式可分为：硬盘数据库和内存数据库。Redis 将数据储存在内存里面，读写数据的时候都不会受到硬盘 I/O 速度的限制，所以速度极快。 （1）硬盘数据库的工作模式：（2）内存数据库的工作模式： &nbsp;&nbsp;&nbsp;&nbsp;看完上述的描述，对于一些常见的Redis相关的面试题，是否有所认识了，例如：什么是Redis、Redis常见的数据结构类型有哪些、Redis是如何进行持久化的等。 三、Redis到底有多快&nbsp;&nbsp;&nbsp;&nbsp;Redis采用的是基于内存的采用的是单进程单线程模型的 KV 数据库，由C语言编写，官方提供的数据是可以达到100000+的QPS（每秒内查询次数）。这个数据不比采用单进程多线程的同样基于内存的 KV 数据库 Memcached 差！有兴趣的可以参考官方的基准程序测试《How fast is Redis？》（https://redis.io/topics/benchmarks） &nbsp;&nbsp;&nbsp;&nbsp;横轴是连接数，纵轴是QPS。此时，这张图反映了一个数量级，希望大家在面试的时候可以正确的描述出来，不要问你的时候，你回答的数量级相差甚远！ 四、Redis为什么这么快1、完全基于内存，绝大部分请求是纯粹的内存操作，非常快速。数据存在内存中，类似于HashMap，HashMap的优势就是查找和操作的时间复杂度都是O(1)； 2、数据结构简单，对数据操作也简单，Redis中的数据结构是专门进行设计的； 3、采用单线程，避免了不必要的上下文切换和竞争条件，也不存在多进程或者多线程导致的切换而消耗 CPU，不用去考虑各种锁的问题，不存在加锁释放锁操作，没有因为可能出现死锁而导致的性能消耗； 4、使用多路I/O复用模型，非阻塞IO； 5、使用底层模型不同，它们之间底层实现方式以及与客户端之间通信的应用协议不一样，Redis直接自己构建了VM 机制 ，因为一般的系统调用系统函数的话，会浪费一定的时间去移动和请求； 以上几点都比较好理解，下边我们针对多路 I/O 复用模型进行简单的探讨： （1）多路 I/O 复用模型 &nbsp;&nbsp;&nbsp;&nbsp;多路I/O复用模型是利用 select、poll、epoll 可以同时监察多个流的 I/O 事件的能力，在空闲的时候，会把当前线程阻塞掉，当有一个或多个流有 I/O 事件时，就从阻塞态中唤醒，于是程序就会轮询一遍所有的流（epoll 是只轮询那些真正发出了事件的流），并且只依次顺序的处理就绪的流，这种做法就避免了大量的无用操作。 &nbsp;&nbsp;&nbsp;&nbsp;这里“多路”指的是多个网络连接，“复用”指的是复用同一个线程。采用多路 I/O 复用技术可以让单个线程高效的处理多个连接请求（尽量减少网络 IO 的时间消耗），且 Redis 在内存中操作数据的速度非常快，也就是说内存内的操作不会成为影响Redis性能的瓶颈，主要由以上几点造就了 Redis 具有很高的吞吐量。 五、那么为什么Redis是单线程的&nbsp;&nbsp;&nbsp;&nbsp;我们首先要明白，上边的种种分析，都是为了营造一个Redis很快的氛围！官方FAQ表示，因为Redis是基于内存的操作，CPU不是Redis的瓶颈，Redis的瓶颈最有可能是机器内存的大小或者网络带宽。既然单线程容易实现，而且CPU不会成为瓶颈，那就顺理成章地采用单线程的方案了（毕竟采用多线程会有很多麻烦！）。 可以参考：https://redis.io/topics/faq &nbsp;&nbsp;&nbsp;&nbsp;看到这里，你可能会气哭！本以为会有什么重大的技术要点才使得Redis使用单线程就可以这么快，没想到就是一句官方看似糊弄我们的回答！但是，我们已经可以很清楚的解释了为什么Redis这么快，并且正是由于在单线程模式的情况下已经很快了，就没有必要在使用多线程了！ &nbsp;&nbsp;&nbsp;&nbsp;但是，我们使用单线程的方式是无法发挥多核CPU 性能，不过我们可以通过在单机开多个Redis 实例来完善！ 警告1：这里我们一直在强调的单线程，只是在处理我们的网络请求的时候只有一个线程来处理，一个正式的Redis Server运行的时候肯定是不止一个线程的，这里需要大家明确的注意一下！例如Redis进行持久化的时候会以子进程或者子线程的方式执行（具体是子线程还是子进程待读者深入研究）；例如我在测试服务器上查看Redis进程，然后找到该进程下的线程： ps命令的“-T”参数表示显示线程（Show threads, possibly with SPID column.）“SID”栏表示线程ID，而“CMD”栏则显示了线程名称。 警告2：在上图中FAQ中的最后一段，表述了从Redis 4.0版本开始会支持多线程的方式，但是，只是在某一些操作上进行多线程的操作！所以该篇文章在以后的版本中是否还是单线程的方式需要读者考证！ 六、注意点1、我们知道Redis是用”单线程-多路复用IO模型”来实现高性能的内存数据服务的，这种机制避免了使用锁，但是同时这种机制在进行sunion之类的比较耗时的命令时会使redis的并发下降。因为是单一线程，所以同一时刻只有一个操作在进行，所以，耗时的命令会导致并发的下降，不只是读并发，写并发也会下降。而单一线程也只能用到一个CPU核心，所以可以在同一个多核的服务器中，可以启动多个实例，组成master-master或者master-slave的形式，耗时的读命令可以完全在slave进行。 需要改的redis.conf项：1234pidfile /var/run/redis/redis_6377.pid #pidfile要加上端口号port 6377 #这个是必须改的logfile /var/log/redis/redis_6377.log #logfile的名称也加上端口号dbfilename dump_6377.rdb #rdbfile也加上端口号 2、“我们不能任由操作系统负载均衡，因为我们自己更了解自己的程序，所以，我们可以手动地为其分配CPU核，而不会过多地占用CPU，或是让我们关键进程和一堆别的进程挤在一起。”。CPU 是一个重要的影响因素，由于是单线程模型，Redis 更喜欢大缓存快速 CPU， 而不是多核 在多核 CPU 服务器上面，Redis 的性能还依赖NUMA 配置和处理器绑定位置。最明显的影响是 redis-benchmark 会随机使用CPU内核。为了获得精准的结果，需要使用固定处理器工具（在 Linux 上可以使用 taskset）。最有效的办法是将客户端和服务端分离到两个不同的 CPU 来高校使用三级缓存。 七、扩展&nbsp;&nbsp;&nbsp;&nbsp;以下也是你应该知道的几种模型，祝你的面试一臂之力！ 1、单进程多线程模型：MySQL、Memcached、Oracle（Windows版本）； 2、多进程模型：Oracle（Linux版本）； 3、Nginx有两类进程，一类称为Master进程(相当于管理进程)，另一类称为Worker进程（实际工作进程）。启动方式有两种： （1）单进程启动：此时系统中仅有一个进程，该进程既充当Master进程的角色，也充当Worker进程的角色。 （2）多进程启动：此时系统有且仅有一个Master进程，至少有一个Worker进程工作。 （3）Master进程主要进行一些全局性的初始化工作和管理Worker的工作；事件处理是在Worker中进行的。 参考文章： 1、http://www.syyong.com/db/Redis-why-the-use-of-single-process-and-single-threaded-way-so-fast.html2、http://blog.csdn.net/xxb2008/article/details/422385573、http://blog.csdn.net/hobbs136/article/details/76197194、http://blog.csdn.net/yushitao/article/details/43565851]]></content>
      <categories>
        <category>redis</category>
      </categories>
      <tags>
        <tag>redis</tag>
        <tag>线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java类加载器深入探索]]></title>
    <url>%2F2018%2F08%2F24%2FJava%E7%B1%BB%E5%8A%A0%E8%BD%BD%E5%99%A8%E6%B7%B1%E5%85%A5%E6%8E%A2%E7%B4%A2%2F</url>
    <content type="text"><![CDATA[什么是.class文件？&nbsp;&nbsp;&nbsp;&nbsp;class文件全名称为Java class文件，主要在平台无关性和网络移动性方面使Java更适合网络。它在平台无关性方面的任务是：为Java程序提供独立于底层主机平台的二进制形式的服务。class文件径打破了C或者C++等语言所遵循的传统，使用这些传统语言写的程序通常首先被编译，然后被连接成单独的、专门支持特定硬件平台和操作系统的二进制文件。通常情况下，一个平台上的二进制可执行文件不能在其他平台上工作。而Java class文件是可以运行在任何支持Java虚拟机的硬件平台和操作系统上的二进制文件。而这也是Java宣称的“一次编译，到处运行”的真正原因，因为各个系统上的Java文件都是被编译成.class文件，然后通过虚拟机来加载运行的。 什么是类加载器？&nbsp;&nbsp;&nbsp;&nbsp;类加载器是一个用来加载类文件的类。Java源代码通过javac编译器编译成类文件。然后JVM来执行类文件中的字节码来执行程序。类加载器负责加载文件系统、网络或其他来源的类文件。有三种默认使用的类加载器：Bootstrap类加载器、Extension类加载器和System类加载器（或者叫作Application类加载器）。每种类加载器都有设定好从哪里加载类。 生成一个对象实例发生了什么事？&nbsp;&nbsp;&nbsp;&nbsp;生成一个实例，程序主要会把对应的类的java文件使用编译器生成字节码文件，然后等此类被调用静态变量或方法或生成实例时，虚拟机自动去相应目录查找字节码文件，并加载到虚拟机当中，然后生成对应的实例对象。每一个字节码文件只会被加载一次。其过程如下： 类加载的方式&nbsp;&nbsp;&nbsp;&nbsp;Java提供两种方法来达成动态行，一种是隐式的，另一种是显式的。这两种方式底层用到的机制完全相同，差异只有程序代码不同。隐式的就是当用到new这个Java关键字时，会让类加载器依需求载入所需的类。显式的又分为两种方法：一种是借用java.lang.Class里的forName()方法，另一种则是借用java.lang.ClassLoader里的loadClass()方法。 类加载器的树状组织结构及加载文件目录&nbsp;&nbsp;&nbsp;&nbsp;Java 中的类加载器大致可以分成两类，一类是系统提供的，另外一类则是由 Java 应用开发人员编写的。系统提供的类加载器主要有下面三个： (1) Bootstrap ClassLoader（引导类加载器） : 它用来加载 Java 的核心库，是用原生代码来实现的，并不继承自 java.lang.ClassLoader。将存放于&lt;JAVA_HOME&gt;\lib目录中的，或者被-Xbootclasspath参数所指定的路径中的，并且是虚拟机识别的（仅按照文件名识别，如 rt.jar 名字不符合的类库即使放在lib目录中也不会被加载）类库加载到虚拟机内存中。启动类加载器无法被Java程序直接引用 (2) Extension ClassLoader（扩展类加载器） : 它用来加载 Java 的扩展库。Java 虚拟机的实现会提供一个扩展库目录。该类加载器在此目录里面查找并加载 Java 类。将&lt;JAVA_HOME&gt;\lib\ext目录下的，或者被java.ext.dirs系统变量所指定的路径中的所有类库加载。开发者可以直接使用扩展类加载器。 (3) Application ClassLoader或叫System Classloader （系统类加载器）: 负责加载用户类路径(ClassPath)上所指定的类库,开发者可直接使用。它根据 Java 应用的类路径（CLASSPATH）来加载 Java 类。一般来说，Java 应用的类都是由它来完成加载的。可以通过 ClassLoader.getSystemClassLoader()来获取它。 以下有两种方式来取得类加载器的组织结构： 1234567891011121314151617package com.lin;public class ClassLoadTest1 &#123; public static void main(String[] args) &#123; ClassLoader loader = ClassLoadTest1.class.getClassLoader(); ClassLoader loader1 = ClassLoader.getSystemClassLoader(); //从子到父取得加载器 while (loader != null) &#123; System.out.println(loader.toString()); loader = loader.getParent(); &#125; while (loader1 != null) &#123; System.out.println(loader1.toString()); loader1 = loader1.getParent(); &#125; &#125; &#125; 输出结果： 可以看到，两种方法都是先取得 Application ClassLoader，然后再取得Extension ClassLoader。 表 1. ClassLoader 中与加载类相关的方法 方法说明getParent()返回该类加载器的父类加载器。loadClass(String name)加载名称为 name的类，返回的结果是 java.lang.Class类的实例。findClass(String name)查找名称为 name的类，返回的结果是 java.lang.Class类的实例。findLoadedClass(String name)查找名称为 name的已经被加载过的类，返回的结果是 java.lang.Class类的实例defineClass(String name, byte[] b, int off, int len)把字节数组 b中的内容转换成 Java 类，返回的结果是 java.lang.Class类的实例。这个方法被声明为 final的。resolveClass(Class&lt;?&gt; c)链接指定的 Java 类。 &nbsp;&nbsp;&nbsp;&nbsp;除了系统提供的类加载器以外，开发人员可以通过继承 java.lang.ClassLoader类的方式实现自己的类加载器，以满足一些特殊的需求。除了引导类加载器之外，所有的类加载器都有一个父类加载器。通过 表 1中给出的 getParent()方法可以得到。对于系统提供的类加载器来说，系统类加载器的父类加载器是扩展类加载器，而扩展类加载器的父类加载器是引导类加载器；对于开发人员编写的类加载器来说，其父类加载器是加载此类加载器 Java 类的类加载器。因为类加载器 Java 类如同其它的 Java 类一样，也是要由类加载器来加载的。一般来说，开发人员编写的类加载器的父类加载器是系统类加载器。类加载器通过这种方式组织起来，形成树状结构。树的根节点就是引导类加载器。下图 中给出了一个典型的类加载器树状组织结构示意图，其中的箭头指向的是父类加载器。 每次加载的具体的过程： 类加载器工作过程类装载器就是寻找类的字节码文件，并构造出类在JVM内部表示的对象组件。在Java中，类装载器把一个类装入JVM中，要经过以下步骤：(1) 装载：查找和导入Class文件；(2) 链接：把类的二进制数据合并到JRE中；&nbsp;&nbsp;&nbsp;&nbsp;(a)校验：检查载入Class文件数据的正确性；&nbsp;&nbsp;&nbsp;&nbsp;(b)准备：给类的静态变量分配存储空间；&nbsp;&nbsp;&nbsp;&nbsp;(c)解析：将符号引用转成直接引用； (3) 初始化：对类的静态变量，静态代码块执行初始化操作 类加载器的工作原理(1)委托机制&nbsp;&nbsp;&nbsp;&nbsp;当一个类加载和初始化的时候，类仅在有需要加载的时候被加载。假设你有一个应用需要的类叫作Abc.class，首先加载这个类的请求由Application类加载器委托给它的父类加载器Extension类加载器，然后再委托给Bootstrap类加载器。Bootstrap类加载器会先看看rt.jar中有没有这个类，因为并没有这个类，所以这个请求由回到Extension类加载器，它会查看jre/lib/ext目录下有没有这个类，如果这个类被Extension类加载器找到了，那么它将被加载，而Application类加载器不会加载这个类；而如果这个类没有被Extension类加载器找到，那么再由Application类加载器从classpath中寻找。记住classpath定义的是类文件的加载目录，而PATH是定义的是可执行程序如javac，java等的执行路径。 工作过程：如果一个类加载器接收到了类加载的请求，它首先把这个请求委托给他的父类加载器去完成，每个层次的类加载器都是如此，因此所有的加载请求都应该传送到顶层的启动类加载器中，只有当父加载器反馈自己无法完成这个加载请求（它在搜索范围中没有找到所需的类）时，子加载器才会尝试自己去加载。 &nbsp;&nbsp;&nbsp;&nbsp;好处：java类随着它的类加载器一起具备了一种带有优先级的层次关系。例如类java.lang.Object，它存放在rt.jar中，无论哪个类加载器要加载这个类，最终都会委派给启动类加载器进行加载，因此Object类在程序的各种类加载器环境中都是同一个类。相反，如果用户自己写了一个名为java.lang.Object的类，并放在程序的Classpath中，那系统中将会出现多个不同的Object类，java类型体系中最基础的行为也无法保证，应用程序也会变得一片混乱。 &nbsp;&nbsp;&nbsp;&nbsp;首先需要说明一下 Java 虚拟机是如何判定两个 Java 类是相同的。Java 虚拟机不仅要看类的全名是否相同，还要看加载此类的类加载器是否一样。只有两者都相同的情况，才认为两个类是相同的。即便是同样的字节代码，被不同的类加载器加载之后所得到的类，也是不同的。比如一个 Java 类 com.example.Sample，编译之后生成了字节代码文件 Sample.class。两个不同的类加载器 ClassLoaderA和 ClassLoaderB分别读取了这个 Sample.class文件，并定义出两个 java.lang.Class类的实例来表示这个类。这两个实例是不相同的。对于 Java 虚拟机来说，它们是不同的类。试图对这两个类的对象进行相互赋值，会抛出运行时异常 ClassCastException。下面通过示例来具体说明。 1234567891011package com.lin; public class Sample &#123; private Sample instance; public void setSample(Object instance) &#123; this.instance = (Sample) instance; &#125; public void say()&#123; System.out.println("Hello LinBingwen"); &#125;&#125; 然后是使用： 12345678910111213141516171819202122232425262728293031323334353637383940package com.lin; import java.net.*;import java.lang.reflect.*; public class ClassLoadTest4&#123; public static void main(String[] args) throws ClassNotFoundException, MalformedURLException, IllegalAccessException, NoSuchMethodException, InstantiationException, InvocationTargetException&#123; ClassLoader pClassLoader = ClassLoader.getSystemClassLoader(); // 以System ClassLoader作为父类加载器 URL[] baseUrls = &#123;new URL("file:/E:/workspace/Eclipse/ClassLoadTest")&#125;; // 搜索类库的目录 final String binaryName = "com.lin.Sample"; // 需要加载的类的二进制名称 ClassLoader userClassLoader1 = new URLClassLoader(baseUrls, pClassLoader); ClassLoader userClassLoader2 = new URLClassLoader(baseUrls, pClassLoader); Class clazz1 = userClassLoader1.loadClass(binaryName); Class clazz2 = userClassLoader2.loadClass(binaryName); Object instance1 = clazz1.newInstance(); Object instance2 = clazz2.newInstance(); // 调用say方法 clazz1.getMethod("say").invoke(instance1); clazz2.getMethod("say").invoke(instance2); // 输出类的二进制名称 System.out.println(clazz1.toString()); System.out.println(clazz2.toString()); // 比较两个类的地址是否相同 System.out.println(clazz1 == clazz2); // 比较两个类是否相同或是否为继承关系 System.out.println(clazz1.isAssignableFrom(clazz2)); // 查看类型转换是否成功 boolean ret = true; try&#123; Method setSampleMethod = clazz1.getMethod("setSample", java.lang.Object.class); setSampleMethod.invoke(instance1, instance2); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; System.out.println(ret); &#125; &#125; 输出结果： 因为都是从 ClassLoader.getSystemClassLoader(); // 以System ClassLoader作为父类加载器，所以两个加载器其实是一样的。 (2)可见性机制根据可见性机制，子类加载器可以看到父类加载器加载的类，而反之则不行。所以下面的例子中，当Abc.class已经被Application类加载器加载过了，然后如果想要使用Extension类加载器加载这个类，将会抛出java.lang.ClassNotFoundException异常。 1234567891011121314151617181920212223package com.lin; import java.util.logging.Level;import java.util.logging.Logger; public class ClassLoadTest2 &#123; public static void main(String[] args) &#123; try &#123; //打印当前的类加载器 System.out.println("ClassLoadTest2.getClass().getClassLoader() : " + ClassLoadTest2.class.getClassLoader()); //使用扩展类加载器再次加载子类加载器加载过的 Class.forName(" com.lin.ClassLoadTest1", true , ClassLoadTest2.class.getClassLoader().getParent()); &#125; catch (ClassNotFoundException ex) &#123; Logger.getLogger(ClassLoadTest2.class.getName()).log(Level.SEVERE, null, ex); &#125; &#125; &#125; (3)单一性机制根据这个机制，父加载器加载过的类不能被子加载器加载第二次。虽然重写违反委托和单一性机制的类加载器是可能的，但这样做并不可取。你写自己的类加载器的时候应该严格遵守这三条机制。 参考文章： 1、https://www.ibm.com/developerworks/cn/java/j-lo-classloader/ 2、http://www.cnblogs.com/ITtangtang/p/3978102.html 3、http://www.cnblogs.com/rason2008/archive/2012/01/01/2309718.html 4、http://www.importnew.com/6581.html 林炳文Evankaka原创作品。转载请注明出处http://blog.csdn.net/evankaka]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>反射</tag>
        <tag>类加载器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java反射探索研究]]></title>
    <url>%2F2018%2F08%2F24%2FJava%E5%8F%8D%E5%B0%84%E6%8E%A2%E7%B4%A2%E7%A0%94%E7%A9%B6%2F</url>
    <content type="text"><![CDATA[&nbsp;&nbsp;&nbsp;&nbsp;摘要：本文详细深入讲解是Java中反射的机制，并介绍了如何通过反射来生成对象、调用函数、取得字段、设置字段的方法。最后，给出了一些反射常用到的实例。 一、反射（1）概念&nbsp;&nbsp;&nbsp;&nbsp;反射含义：可以获取正在运行的Java对象。（2）功能&nbsp;&nbsp;&nbsp;&nbsp;1)在运行时判断任意一个对象所属的类&nbsp;&nbsp;&nbsp;&nbsp;2)在运行时构造任意一个类的对象&nbsp;&nbsp;&nbsp;&nbsp;3) 在运行时判断任意一个类所具有的成员变量和方法&nbsp;&nbsp;&nbsp;&nbsp;4)在运行时调用任意一个对象的方法（3）实现Java反射的类&nbsp;&nbsp;&nbsp;&nbsp;1)Class：它表示正在运行的Java应用程序中的类和接口&nbsp;&nbsp;&nbsp;&nbsp;2)Field：提供有关类或接口的属性信息，以及对它的动态访问权限&nbsp;&nbsp;&nbsp;&nbsp;3)Constructor：提供关于类的单个构造方法的信息以及对它的访问权限&nbsp;&nbsp;&nbsp;&nbsp;4)Method：提供关于类或接口中某个方法信息&nbsp;&nbsp;&nbsp;&nbsp;注意：Class类是Java反射中最重要的一个功能类，所有获取对象的信息(包括：方法/属性/构造方法/访问权限)都需要它来实现 （4）取得class的三种方法 1234Dog dog = new Dog(); Class&lt;?&gt; dogClass = dog.getClass();Class&lt;?&gt; dogClass1 = Dog.class;Class&lt;?&gt; dogClass2 = Class.forName("com.lin.Dog");//注意要添加异常抛出 （5）关键方法 方法关键字含义getDeclaredMethods()获取所有的方法getReturnType()获得方法的放回类型getParameterTypes()获得方法的传入参数类型getDeclaredMethod(“方法名”,参数类型.class,……)获得特定的方法构造方法关键字含义getDeclaredConstructors()获取所有的构造方法getDeclaredConstructor(参数类型.class,……)获取特定的构造方法父类和父接口含义getSuperclass()获取某类的父类getInterfaces()获取某类实现的接口 （6）一些区别函数 public Method[] getMethods()返回某个类的所有公用（public）方法包括其继承类的公用方法，当然也包括它所实现接口的方法。 public Method[] getDeclaredMethods()对象表示的类或接口声明的所有方法，包括公共、保护、默认（包）访问和私有方法，但不包括继承的方法。当然也包括它所实现接口的方法。 getFields()获得某个类的所有的公共（public）的字段，包括父类。 getDeclaredFields()获得某个类的所有申明的字段，即包括public、private和proteced，但是不包括父类的申明字段。 下面来看一个例子说明： 动物接口 123456789package com.lin; public interface Aminal &#123; public String eat(String obj); public int run(int obj); &#125; 实现类： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465package com.lin; import java.util.jar.Attributes.Name; public class Dog implements Aminal &#123; private String name; private int age; public Dog() &#123; // TODO 自动生成的构造函数存根 &#125; public Dog(String name,int age) &#123; this.name = name; this.age = age; &#125; public Dog(String name) &#123; this.name = name; this.age = 10; &#125; private void sleep(int x) &#123; System.out.println(name + "睡觉" + x + "分钟"); &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; this.age = age; &#125; @Override public String eat(String obj) &#123; System.out.println(name + "吃"+ obj); return ; &#125; @Override public int run(int obj) &#123; System.out.println("跑，速度："+ obj); return 0; &#125; @Override public String toString() &#123; return "狗名：" + name + " 狗的年纪：" + age; &#125; private static void play() &#123; System.out.println("狗狗自己玩啊玩"); &#125; &#125; 来看看各自的调用： 123456789101112131415161718192021222324252627282930313233343536package com.lin; import java.lang.reflect.Method; public class ReflectLearning &#123; public static void main(String[] args) throws ClassNotFoundException &#123; Dog dog = new Dog(); System.out.println(dog.getClass()); System.out.println(dog.getClass().getName()); Class&lt;?&gt; dogClass = dog.getClass(); Class&lt;?&gt; dogClass1 = Dog.class; Class&lt;?&gt; dogClass2 = Class.forName("com.lin.Dog"); Method[] methods1 = dogClass.getMethods(); System.out.println("====================通过getMethods取得方法开始===================="); for (Method method : methods1) &#123; System.out.println(method); &#125; System.out.println("====================通过getMethods取得方法结束===================="); Method[] methods2 = dogClass.getDeclaredMethods(); System.out.println("====================通过getDeclaredMethods取得方法开始===================="); for (Method method : methods2) &#123; System.out.println(method); &#125; System.out.println("====================通过getDeclaredMethods取得方法结束===================="); &#125; &#125; 来看下结果： getMethods方法 getDeclareMethos方法： 从上面可以看出getMethods()返回某个类的所有公用（public）方法包括其继承类的公用方法，当然也包括它所实现接口的方法。getDeclaredMethods()对象表示的类或接口声明的所有方法，包括公共、保护、默认（包）访问和私有方法，但不包括继承的方法。当然也包括它所实现接口的方法。 二、通过反射调用构造函数（1）、列出所有的构造函数： 1234567Constructor&lt;?&gt;[] constructors = dogClass.getConstructors(); System.out.println("====================列出所有的构造函数结束====================");for (Constructor&lt;?&gt; constructor : constructors) &#123; System.out.println(constructor);&#125;System.out.println("====================列出所有的构造函数结束===================="); 输出结果： （2）、通过反射生成对象 1234567891011121314System.out.println("====================通过newInstance()来生成对象，一定在有默认构造函数====================");Dog dog1 = (Dog) dogClass.newInstance();dog1.setName("狗狗1号");dog1.setAge(7);System.out.println(dog1); System.out.println("====================通过newInstance(参数)方法一来生成对象====================");Dog dog2 = (Dog)constructors[0].newInstance("狗狗2号");System.out.println(dog2); System.out.println("====================通过newInstance(参数)方法二来生成对象====================");Constructor con1 = dogClass.getConstructor(new Class[]&#123;String.class,int.class&#125;); //主要就是这句了Dog dog3 = (Dog) con1.newInstance(new Object[]&#123;"狗狗3号",14&#125;);System.out.println(dog3); 输出结果： 从上面可以看出，先通过getConstructor(new Class[]{xxxx.class,yyy.class}),再通过con1.newInstance(new Object[]{“xxxxx”,…});的方式是最灵活的，可以自动根据输入的参数类型和个数，找到对应的构造函数来调用。第二种方法需要得到构造函数的数组，并且需要知道对应哪一个构造函数。第一种就只能调用无参构造函数。 三、通过反射调用普通函数、静态函数（1）取得函数的一些基本信息 123456Class&lt;?&gt; dogClass = Dog.class;Method[] methods = dogClass.getDeclaredMethods();for (Method method : methods) &#123; System.out.println("函数名："+method.getName() +" 函数类型："+ method.getModifiers() + " 函数返回： "+ method.getReturnType() + " 函数参数个数：" + method.getParameterCount()); &#125; 输出结果： 其中函数类型对应表如下：PUBLIC: 1PRIVATE: 2PROTECTED: 4STATIC: 8FINAL: 16SYNCHRONIZED: 32VOLATILE: 64TRANSIENT: 128NATIVE: 256INTERFACE: 512ABSTRACT: 1024STRICT: 2048 （2）方法调用 这是当前狗类的方法： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768package com.lin; import java.util.jar.Attributes.Name; public class Dog implements Aminal &#123; private String name; private int age; public Dog() &#123; // TODO 自动生成的构造函数存根 &#125; public Dog(String name,int age) &#123; this.name = name; this.age = age; &#125; public Dog(String name) &#123; this.name = name; this.age = 10; &#125; private void sleep(int x) &#123; System.out.println(name + "睡觉" + x + "分钟"); &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; this.age = age; &#125; @Override public String eat(String obj) &#123; System.out.println(name + "吃"+ obj); return null; &#125; @Override public int run(int obj) &#123; System.out.println("跑，速度："+ obj); return 0; &#125; @Override public String toString() &#123; return "狗名：" + name + " 狗的年纪：" + age; &#125; private static void play() &#123; System.out.println("狗狗自己玩啊玩"); &#125; &#125; 不同方法的调用过程： 123456789101112131415//调用私有方法Method method1 = dogClass.getDeclaredMethod("sleep", int.class);//不要用getMethod，它只能取到public方法Dog dog1 = (Dog) dogClass.getConstructor(new Class[] &#123;String.class&#125;).newInstance(new Object[]&#123;"狗狗1号"&#125;);method1.setAccessible(true);//私有方法一定要加这句method1.invoke(dog1, 12);//调用私有静态方法 Method method2 = dogClass.getDeclaredMethod("play");//不要用getMethod，它只能取到public方法 method2.setAccessible(true);//私有方法一定要加这句 method2.invoke(dogClass.newInstance()); //调用公共方法Method method3 = dogClass.getMethod("eat", String.class);//这里也可以用getDeclaredMethodDog dog3 = new Dog("狗狗3号", 45);method3.invoke(dog3, "苹果～"); 输出结果： 方法调用这里一定要记住getMethod和getDeclaredMethod的区别，并且在调用私有的方法之前一定要加setAccessible（true）这一句，要不会报错！ 四、通过反射取得字段、设置字段值(1)怎么通过反射获取类的属性&nbsp;&nbsp;&nbsp;&nbsp;a)Class.getDeclaredField(String name);返回一个 Field 对象，该对象反映此 Class 对象所表示的类或接口的指定已声明字段。&nbsp;&nbsp;&nbsp;&nbsp;b)Class.getDeclaredFields();返回 Field 对象的一个数组，这些对象反映此 Class 对象所表示的类或接口所声明的所有字段。&nbsp;&nbsp;&nbsp;&nbsp;c)Class.getField(String name);返回一个 Field 对象，它反映此 Class 对象所表示的类或接口的指定公共成员字段。&nbsp;&nbsp;&nbsp;&nbsp;d)Class.getField();返回一个包含某些 Field 对象的数组，这些对象反映此 Class 对象所表示的类或接口的所有可访问公共字段。 (2)进行属性获取更改 1234567891011Dog dog1 = new Dog("狗狗1号", 12);System.out.println(dog1);Class&lt;?&gt; dogClass = dog1.getClass();Field field1 = dogClass.getDeclaredField("name");//注意，getField只能取得public的字段field1.setAccessible(true);//私有变量必须先设置Accessible为true System.out.println("原本狗名：" + field1.get(dog1));field1.set(dog1,"狗狗2号");System.out.println(dog1); 输出结果： 值得注意的是获取私有属性的时候必须先设置Accessible为true，然后才能获取。 五、反射常用工具类（1）bean复制工具&nbsp;&nbsp;&nbsp;&nbsp;这里可以使用commons-beanutils中的copyProperties()方法，自己写是为了加深对反射的理解。 1、toString的基类 12345678910111213141516171819202122232425262728293031323334package com.lin; import java.lang.reflect.Field;import java.text.SimpleDateFormat;import java.util.Date; /** * bean基類 * @author lin * */public class BaseBean &#123; public String toString() &#123; StringBuffer sb = new StringBuffer(); SimpleDateFormat sdf = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss"); Class&lt;?&gt; cls = this.getClass(); Field[] fields = cls.getDeclaredFields(); sb.append(cls.getName() + "&#123;"); for (Field field : fields) &#123; try &#123; field.setAccessible(true); sb.append(field.getName()); sb.append("="); sb.append(field.get(this)); sb.append(" "); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; sb.append("&#125;"); return sb.toString(); &#125;&#125; 2、bean复制工具 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101package com.lin; import java.lang.reflect.Field;import java.lang.reflect.InvocationTargetException;import java.lang.reflect.Method; /** * 将一个JavaBean风格对象的属性值拷贝到另一个对象的同名属性中 (如果不存在同名属性的就不拷贝） **/ public class BeanCopy &#123; private static String GET = "get"; private static String SET = "set"; /** * * @param source * @param target * @throws Exception */ public static void copy(Object source,Object target)&#123; Class&lt;?&gt; sourceClz = source.getClass(); Class&lt;?&gt; targetClz = target.getClass(); // 得到Class对象所表征的类的所有属性(包括私有属性) Field[] sourceFields = sourceClz.getDeclaredFields(); if (sourceFields.length == 0) &#123; sourceFields = sourceClz.getSuperclass().getDeclaredFields(); &#125; int len = sourceFields.length; for (int i = 0; i &lt; len; i++) &#123; String fieldName = sourceFields[i].getName(); Field targetField = null; // 得到targetClz对象所表征的类的名为fieldName的属性，不存在就进入下次循环 try &#123; targetField = targetClz.getDeclaredField(fieldName); &#125; catch (NoSuchFieldException e) &#123; try &#123; targetField = targetClz.getSuperclass().getDeclaredField(fieldName); &#125; catch (NoSuchFieldException e1) &#123; e1.printStackTrace(); &#125; catch (SecurityException e1) &#123; e1.printStackTrace(); &#125; &#125; if (targetField == null) &#123; continue; &#125; // 判断sourceClz字段类型和targetClz同名字段类型是否相同 if (sourceFields[i].getType() == targetField.getType()) &#123; // 由属性名字得到对应get和set方法的名字 String getMethodName = GET + fieldName.substring(0, 1).toUpperCase() + fieldName.substring(1); String setMethodName = SET + fieldName.substring(0, 1).toUpperCase() + fieldName.substring(1); // 由方法的名字得到get和set方法的Method对象 Method getMethod; Method setMethod; try &#123; try &#123; getMethod = sourceClz.getDeclaredMethod(getMethodName,new Class[] &#123;&#125;);//get方法入參為空 &#125; catch (NoSuchMethodException e) &#123; getMethod = sourceClz.getSuperclass().getDeclaredMethod(getMethodName,new Class[] &#123;&#125;); &#125; try &#123; setMethod = targetClz.getDeclaredMethod(setMethodName,sourceFields[i].getType());//set方法入參不為空 &#125; catch (NoSuchMethodException e) &#123; setMethod = targetClz.getSuperclass().getDeclaredMethod(setMethodName,sourceFields[i].getType()); &#125; // 调用source对象的getMethod方法 Object result = getMethod.invoke(source, new Object[] &#123;&#125;); // 调用target对象的setMethod方法 setMethod.invoke(target, result); &#125; catch (SecurityException e) &#123; e.printStackTrace(); &#125; catch (NoSuchMethodException e) &#123; e.printStackTrace(); &#125; catch (IllegalArgumentException e) &#123; e.printStackTrace(); &#125; catch (IllegalAccessException e) &#123; e.printStackTrace(); &#125; catch (InvocationTargetException e) &#123; e.printStackTrace(); &#125; &#125; else &#123; continue; &#125; &#125; &#125; &#125; 使用： 新建两个类： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990package com.lin; import java.util.Date; public class Car extends BaseBean&#123; private String name; private String id; private Boolean sellFlag; private int age; private double maxSpeed; private double minSpeed; private int driverPeople; private Date date; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public String getId() &#123; return id; &#125; public void setId(String id) &#123; this.id = id; &#125; public Boolean getSellFlag() &#123; return sellFlag; &#125; public void setSellFlag(Boolean sellFlag) &#123; this.sellFlag = sellFlag; &#125; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; this.age = age; &#125; public double getMaxSpeed() &#123; return maxSpeed; &#125; public void setMaxSpeed(double maxSpeed) &#123; this.maxSpeed = maxSpeed; &#125; public double getMinSpeed() &#123; return minSpeed; &#125; public void setMinSpeed(double minSpeed) &#123; this.minSpeed = minSpeed; &#125; public int getDriverPeople() &#123; return driverPeople; &#125; public void setDriverPeople(int driverPeople) &#123; this.driverPeople = driverPeople; &#125; public Date getDate() &#123; return date; &#125; public void setDate(Date date) &#123; this.date = date; &#125; &#125; 另一个： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798package com.lin; import java.util.Date; public class Bus extends BaseBean&#123; private String name; private String id; private Boolean sellFlag; private int age; private double maxSpeed; private double minSpeed; private long driverPeople;//和car類型不同 private int driverYear;//car沒有這個 private Date date; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public String getId() &#123; return id; &#125; public void setId(String id) &#123; this.id = id; &#125; public Boolean getSellFlag() &#123; return sellFlag; &#125; public void setSellFlag(Boolean sellFlag) &#123; this.sellFlag = sellFlag; &#125; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; this.age = age; &#125; public double getMaxSpeed() &#123; return maxSpeed; &#125; public void setMaxSpeed(double maxSpeed) &#123; this.maxSpeed = maxSpeed; &#125; public double getMinSpeed() &#123; return minSpeed; &#125; public void setMinSpeed(double minSpeed) &#123; this.minSpeed = minSpeed; &#125; public long getDriverPeople() &#123; return driverPeople; &#125; public void setDriverPeople(long driverPeople) &#123; this.driverPeople = driverPeople; &#125; public int getDriverYear() &#123; return driverYear; &#125; public void setDriverYear(int driverYear) &#123; this.driverYear = driverYear; &#125; public Date getDate() &#123; return date; &#125; public void setDate(Date date) &#123; this.date = date; &#125; &#125; 调用： 12345678910111213141516171819public static void test5() &#123; Car car = new Car(); car.setAge(12); car.setDriverPeople(4); car.setId("YU1234"); car.setMaxSpeed(13.66); car.setMinSpeed(1.09); car.setName("小车"); car.setSellFlag(false); car.setDate(new Date()); Bus bus = new Bus(); BeanCopy.copy(car,bus); System.out.println(car); System.out.println(bus); &#125; 除了两个不同的字段外，其它的都复制过去了，这在DTO、VO、DOMAIN对象转换时经常用到。 林炳文Evankaka原创作品。转载请注明出处http://blog.csdn.net/evankaka]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>反射</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java反射探索-----从类加载说起]]></title>
    <url>%2F2018%2F08%2F24%2FJava%E5%8F%8D%E5%B0%84%E6%8E%A2%E7%B4%A2-%E4%BB%8E%E7%B1%BB%E5%8A%A0%E8%BD%BD%E8%AF%B4%E8%B5%B7%2F</url>
    <content type="text"><![CDATA[&nbsp;&nbsp;&nbsp;&nbsp;摘要：本文主要讲了Java类加载的机制，这是学习反射的入门基础。 一、类加载JVM和类 &nbsp;&nbsp;&nbsp;&nbsp;当我们调用Java命令运行某个Java程序时，该命令将会启动一条Java虚拟机进程，不管该Java程序有多么复杂，该程序启动了多少个线程，它们都处于该Java虚拟机进程里。正如前面介绍的，同一个JVM的所有线程、所有变量都处于同一个进程里，它们都使用该JVM进程的内存区。当系统出现以下几种情况时，JVM进程将被终止: 1、程序运行到最后正常结束。2、程序运行到使用System.exit()或Runtime.getRuntime().exit()代码结束程序。3、程序执行过程中遇到未捕获的异常或错误而结束。3、程序所在平台强制结束了JVM进程。从上面的介绍可以看出，当Java程序运行结束时，JVM进程结束，该进程在内存中状态将会丢失。 类的生命周期 类的加载/类初始化 &nbsp;&nbsp;&nbsp;&nbsp;当程序主动使用某个类时，如果该类还未被加载到内存中，系统会通过加载、连接、初始化三个步骤来对该类进行初始化，如果没有意外，JVM将会连续完成这三个步骤，所以有时也把这三个步骤统称为类加载或类初始化。 加载：查找并加载类的二进制数据 1、通过一个类的全限定名来获取定义此类的二进制字节流。2、将这个字节流所代表的静态存储结构转化为方法区的运行时数据结构。3、在java堆中生成一个代表这个类的java.lang.Class对象，作为方法区这些数据的访问入口。 注意：将编译后的java类文件（也就是.class文件）中的二进制数据读入内存，并将其放在运行时数据区的方法区内，然后再堆区创建一个Java.lang.Class对象，用来封装类在方法区的数据结构。即加载后最终得到的是Class对象，并且更加值得注意的是：该Java.lang.Class对象是单实例的，无论这个类创建了多少个对象，他的Class对象时唯一的！ 连接：&nbsp;&nbsp;&nbsp;&nbsp;1、验证：确保被加载的类的正确性&nbsp;&nbsp;&nbsp;&nbsp;2、准备：为类的静态变量分配内存，并将其初始化为默认值&nbsp;&nbsp;&nbsp;&nbsp;3、解析：把类中的符号引用转换为直接引用。初始化：为类的静态变量赋予正确的初始值。 注意：连接和初始化阶段，其实静态变量经过了两次赋值：第一次是静态变量类型的默认值；第二次是我们真正赋给静态变量的值。 我简单画了个图，其过程如下： 类加载指的是将类的class文件读入内存，并为之创建一个java.lang.Class对象，也就是说当程序中使用任何类时，系统都会为之建立一个java.lang.Class对象。事实上，每个类是一批具有相同特征的对象的抽象(或者说概念)，而系统中所有的类，它们实际上也是对象，它们都是java.lang.Class的实例。 &nbsp;&nbsp;&nbsp;&nbsp;加载由类加载器完成，类加载器通常由JVM提供，这些类加载器也是我们前面所有程序运行的基础，JVM提供的这些类加载器通常被称为系统类加载器。除此之外，开发者可以通过继承ClassLoader基类来创建自己的类加载器。 通过使用不同的类加载器，可以从不同来源加载类的二进制数据，通常有如下几种来源: 1、从本地文件系统来加载class文件，这是绝大部分示例程序的类加载方式。 2、从JAR包中加载class文件，这种方式也是很常见的，前面介绍JDBC编程时用到的数据库驱动类就是放在JAR文件中，JVM可以从JAR文件中直接加载该class文件。 3、通过网络加载class文件。 4、把一个Java源文件动态编译、并执行加载。 类加载器通常无须等到“首次使用”该类时才加载该类，Java虚拟机规范允许系统预先加载某些类。 Java程序对类的使用方式 主动使用1、创建类的实例2、方法某个类或接口的静态变量，或者对该静态变量赋值3、调用类的静态方法4、反射（如 Class.forName(“com.itzhai.Test”)）5、初始化一个类的子类6、Java虚拟机启动时被标明为启动类的类（Main Class） 被动使用除了以上6中方式，其他对类的使用都是被动使用，都不会导致类的初始化。类的初始化时机正是java程序对类的首次主动使用，所有的Java虚拟机实现必须在每个类或接口被Java程序“首次主动使用”时才初始化它们。 对象初始化在类被装载、连接和初始化，这个类就随时都可能使用了。对象实例化和初始化是就是对象生命的起始阶段的活动，在这里我们主要讨论对象的初始化工作的相关特点。Java 编译器在编译每个类时都会为该类至少生成一个实例初始化方法–即”()” 方法。此方法与源代码中的每个构造方法相对应，如果类没有明确地声明任何构造方法，编译器则为该类生成一个默认的无参构造方法，这个默认的构造器仅仅调用父类的无参构造器，与此同时也会生成一个与默认构造方法对应的 “()” 方法.通常来说，() 方法内包括的代码内容大概为：调用另一个 () 方法；对实例变量初始化；与其对应的构造方法内的代码。如果构造方法是明确地从调用同一个类中的另一个构造方法开始，那它对应的 () 方法体内包括的内容为：一个对本类的 () 方法的调用；对应用构造方法内的所有字节码。如果构造方法不是通过调用自身类的其它构造方法开始，并且该对象不是 Object 对象，那 () 法内则包括的内容为：一个对父类 () 方法的调用；对实例变量初始化方法的字节码；最后是对应构造子的方法体字节码。如果这个类是 Object，那么它的 () 方法则不包括对父类 () 方法的调用。 二、Class.forName、实例对象.class(属性)、实例对象getClass()的区别1、相同点：通过这几种方式，得到的都是Java.lang.Class对象（这个是上面讲到的类在加载时获得的最终产物）例如： 123456789101112131415161718192021222324252627282930package com.lin; /** * 功能概要： * * @author linbingwen * @since 2015年10月20日 */public class people &#123; /** * @author linbingwen * @since 2015年10月20日 * @param args */ public static void main(String[] args) throws Exception &#123; System.out.println("..............使用不同的方式加载类..................."); System.out.println(people.class);//通过类.class获得Class对象 people a = new people(); System.out.println(a.getClass());//通过 实例名.getClass()获得Class对象 System.out.println(Class.forName("com.lin.people"));//通过Class.forName(全路径)获得Class对象 System.out.println("..............使用不同的方式创建对象..................."); System.out.println(a);//使用不同的方式创建对象 System.out.println(people.class.newInstance()); System.out.println(a.getClass().newInstance()); System.out.println(Class.forName("com.lin.people").newInstance()); &#125; &#125; 结果： 从上面可以看到不同的方式加载类。其实这一过程只发生一次！ 2、区别： 下面用一个实例来说说它们的区别 如下新建一个类 1234567891011121314package com.lin; /** * 功能概要： * * @author linbingwen * @since 2015年10月20日 */public class Cat &#123; static &#123; System.out.println("生成了一只猫"); &#125; &#125; 然后开始使用： 1234567891011121314151617181920212223242526272829303132package com.lin; /** * 功能概要： * * @author linbingwen * @since 2015年10月20日 */public class CatTest &#123; /** * @author linbingwen * @since 2015年10月20日 * @param args */ public static void main(String[] args) throws Exception&#123; System.out.println("---------------Cat.class开始------------------"); System.out.println(Cat.class);//通过类.class获得Class对象 System.out.println("---------------Cat.class结束------------------"); System.out.println("---------------Class.forName开始------------------"); System.out.println(Class.forName("com.lin.Cat"));//通过Class.forName(全路径)获得Class对象 System.out.println("---------------Class.forName结束------------------"); System.out.println("---------------cat.getClass()开始------------------"); Cat cat = new Cat(); System.out.println(cat.getClass());//通过Class.forName(全路径)获得Class对象 System.out.println("---------------cat.getClass()结束------------------"); &#125; &#125; 输出结果： 如果，将Class.forName()去掉： 如下： 1234567891011121314151617181920212223242526272829303132package com.lin; /** * 功能概要： * * @author linbingwen * @since 2015年10月20日 */public class CatTest &#123; /** * @author linbingwen * @since 2015年10月20日 * @param args */ public static void main(String[] args) throws Exception&#123; System.out.println("---------------Cat.class开始------------------"); System.out.println(Cat.class);//通过类.class获得Class对象 System.out.println("---------------Cat.class结束------------------"); // System.out.println("---------------Class.forName开始------------------");// System.out.println(Class.forName("com.lin.Cat"));//通过Class.forName(全路径)获得Class对象// System.out.println("---------------Class.forName结束------------------"); System.out.println("---------------cat.getClass()开始------------------"); Cat cat = new Cat(); System.out.println(cat.getClass());//通过Class.forName(全路径)获得Class对象 System.out.println("---------------cat.getClass()结束------------------"); &#125; &#125; 结果又变成： 所以，可以得出以下结论： 1)Class cl=Cat.class; JVM将使用类Cat的类装载器,将类A装入内存(前提是:类A还没有装入内存),不对类A做类的初始化工作.返回类A的Class的对象2)Class cl=对象引用o.getClass();返回引用o运行时真正所指的对象(因为:儿子对象的引用可能会赋给父对象的引用变量中)所属的类的Class的对象 ,如果还没装载过，会进行装载。3)Class.forName(“类名”); 装入类A,并做类的初始化(前提是:类A还没有装入内存) 三、new和newInstance()从JVM的角度看，我们使用关键字new创建一个类的时候，这个类可以没有被加载。但是使用Class对象的newInstance()方法的时候，就必须保证： 1、这个类已经加载； 2、这个类已经连接了。而完成上面两个步骤的正是Class的静态方法forName()所完成的，这个静态方法调用了启动类加载器，即加载 java API的那个加载器。 现在可以看出，Class对象的newInstance()（这种用法和Java中的工厂模式有着异曲同工之妙）实际上是把new这个方式分解为两步，即首先调用Class加载方法加载某个类，然后实例化。这样分步的好处是显而易见的。我们可以在调用class的静态加载方法forName时获得更好的灵活性，提供给了一种降耦的手段。 Class.forName().newInstance()和通过new得到对象的区别 1、使用newInstance可以解耦。使用newInstance的前提是，类已加载并且这个类已连接，这是正是class的静态方法forName（）完成的工作。newInstance实际上是把new 这个方式分解为两步，即，首先调用class的加载方法加载某个类，然后实例化。 2、newInstance: 弱类型。低效率。只能调用无参构造。 new: 强类型。相对高效。能调用任何public构造。 3、newInstance()是实现IOC、反射、面对接口编程和依赖倒置等技术方法的必然选择，new只能实现具体类的实例化，不适合于接口编程。 4、 newInstance() 一般用于动态加载类。 5、Class.forName(“”).newInstance()返回的是object 。 6、newInstance( )是一个方法，而new是一个关键字； 注:一般在通用框架里面用的就是class.forName来加载类,然后再通过反射来调用其中的方法,譬如Tomcat源码里面,这样就避免了new关键字的耦合度,还有让不同的类加载器来加载不同的类,方便提高类之间的安全性和隔离性. 林炳文Evankaka原创作品。转载请注明出处http://blog.csdn.net/evankaka]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>反射</tag>
        <tag>类加载</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java多线程学习（吐血超详细总结）]]></title>
    <url>%2F2018%2F08%2F23%2FJava%E5%A4%9A%E7%BA%BF%E7%A8%8B%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%90%90%E8%A1%80%E8%B6%85%E8%AF%A6%E7%BB%86%E6%80%BB%E7%BB%93%EF%BC%89%2F</url>
    <content type="text"><![CDATA[&nbsp;&nbsp;&nbsp;&nbsp;写在前面的话：此文只能说是java多线程的一个入门，其实Java里头线程完全可以写一本书了，但是如果最基本的你都学掌握好，又怎么能更上一个台阶呢？如果你觉得此文很简单，那推荐你看看Java并发包的的线程池（Java并发编程与技术内幕:线程池深入理解），或者看这个专栏：Java并发编程与技术内幕。你将会对Java里头的高并发场景下的线程有更加深刻的理解。 目录 一扩展javalangThread类 二实现javalangRunnable接口 三Thread和Runnable的区别 四线程状态转换 五线程调度 六常用函数说明 使用方式 为什么要用join方法 七常见线程名词解释 八线程同步 九线程数据传递 本文主要讲了java中多线程的使用方法、线程同步、线程数据传递、线程状态及相应的一些线程函数用法、概述等。在这之前，首先让我们来了解下在操作系统中进程和线程的区别： 进程：每个进程都有独立的代码和数据空间（进程上下文），进程间的切换会有较大的开销，一个进程包含1–n个线程。（进程是资源分配的最小单位） 线程：同一类线程共享代码和数据空间，每个线程有独立的运行栈和程序计数器(PC)，线程切换开销小。（线程是cpu调度的最小单位） 线程和进程一样分为五个阶段：创建、就绪、运行、阻塞、终止。 多进程是指操作系统能同时运行多个任务（程序）。 多线程是指在同一程序中有多个顺序流在执行。 在java中要想实现多线程，有两种手段，一种是继续Thread类，另外一种是实现Runable接口.(其实准确来讲，应该有三种，还有一种是实现Callable接口，并与Future、线程池结合使用，此文这里不讲这个，有兴趣看这里Java并发编程与技术内幕:Callable、Future、FutureTask、CompletionService) 一、扩展java.lang.Thread类这里继承Thread类的方法是比较常用的一种，如果说你只是想起一条线程。没有什么其它特殊的要求，那么可以使用Thread.（笔者推荐使用Runable，后头会说明为什么）。下面来看一个简单的实例1234567891011121314151617181920212223242526272829303132333435package com.multithread.learning;/** *@functon 多线程学习 *@author 林炳文 *@time 2015.3.9 */class Thread1 extends Thread&#123; private String name; public Thread1(String name) &#123; this.name=name; &#125; public void run() &#123; for (int i = 0; i &lt; 5; i++) &#123; System.out.println(name + "运行 : " + i); try &#123; sleep((int) Math.random() * 10); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125;public class Main &#123; public static void main(String[] args) &#123; Thread1 mTh1=new Thread1("A"); Thread1 mTh2=new Thread1("B"); mTh1.start(); mTh2.start(); &#125; &#125; 输出： A运行 : 0B运行 : 0A运行 : 1A运行 : 2A运行 : 3A运行 : 4B运行 : 1B运行 : 2B运行 : 3B运行 : 4 再运行一下： A运行 : 0B运行 : 0B运行 : 1B运行 : 2B运行 : 3B运行 : 4A运行 : 1A运行 : 2A运行 : 3A运行 : 4 说明： 程序启动运行main时候，java虚拟机启动一个进程，主线程main在main()调用时候被创建。随着调用MitiSay的两个对象的start方法，另外两个线程也启动了，这样，整个应用就在多线程下运行。 注意：start()方法的调用后并不是立即执行多线程代码，而是使得该线程变为可运行态（Runnable），什么时候运行是由操作系统决定的。 从程序运行的结果可以发现，多线程程序是乱序执行。因此，只有乱序执行的代码才有必要设计为多线程。 Thread.sleep()方法调用目的是不让当前线程独自霸占该进程所获取的CPU资源，以留出一定时间给其他线程执行的机会。 实际上所有的多线程代码执行顺序都是不确定的，每次执行的结果都是随机的。 但是start方法重复调用的话，会出现java.lang.IllegalThreadStateException异常。1234Thread1 mTh1=new Thread1("A");Thread1 mTh2=mTh1;mTh1.start();mTh2.start(); 输出： 123456789Exception in thread "main" java.lang.IllegalThreadStateException at java.lang.Thread.start(Unknown Source) at com.multithread.learning.Main.main(Main.java:31) A运行 : 0 A运行 : 1 A运行 : 2 A运行 : 3 A运行 : 4 二、实现java.lang.Runnable接口采用Runnable也是非常常见的一种，我们只需要重写run方法即可。下面也来看个实例。1234567891011121314151617181920212223242526272829303132333435/** *@functon 多线程学习 *@author 林炳文 *@time 2015.3.9 */package com.multithread.runnable;class Thread2 implements Runnable&#123; private String name; public Thread2(String name) &#123; this.name=name; &#125; @Override public void run() &#123; for (int i = 0; i &lt; 5; i++) &#123; System.out.println(name + "运行 : " + i); try &#123; Thread.sleep((int) Math.random() * 10); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125;public class Main &#123; public static void main(String[] args) &#123; new Thread(new Thread2("C")).start(); new Thread(new Thread2("D")).start(); &#125; &#125; 输出： C运行 : 0D运行 : 0D运行 : 1C运行 : 1D运行 : 2C运行 : 2D运行 : 3C运行 : 3D运行 : 4C运行 : 4 说明： Thread2类通过实现Runnable接口，使得该类有了多线程类的特征。run（）方法是多线程程序的一个约定。所有的多线程代码都在run方法里面。Thread类实际上也是实现了Runnable接口的类。 在启动的多线程的时候，需要先通过Thread类的构造方法Thread(Runnable target) 构造出对象，然后调用Thread对象的start()方法来运行多线程代码。 实际上所有的多线程代码都是通过运行Thread的start()方法来运行的。因此，不管是扩展Thread类还是实现Runnable接口来实现多线程，最终还是通过Thread的对象的API来控制线程的，熟悉Thread类的API是进行多线程编程的基础。 三、Thread和Runnable的区别如果一个类继承Thread，则不适合资源共享。但是如果实现了Runable接口的话，则很容易的实现资源共享。 总结： 实现Runnable接口比继承Thread类所具有的优势： 1）：适合多个相同的程序代码的线程去处理同一个资源 2）：可以避免java中的单继承的限制 3）：增加程序的健壮性，代码可以被多个线程共享，代码和数据独立 4）：线程池只能放入实现Runable或callable类线程，不能直接放入继承Thread的类 提醒一下大家：main方法其实也是一个线程。在java中所以的线程都是同时启动的，至于什么时候，哪个先执行，完全看谁先得到CPU的资源。 在**java中，每次程序运行至少启动2个线程。一个是main线程，一个是垃圾收集线程。因为每当使用java**命令执行一个类的时候，实际上都会启动一个ＪＶＭ，每一个ｊＶＭ实习在就是在操作系统中启动了一个进程。 四、线程状态转换下面的这个图非常重要！你如果看懂了这个图，那么对于多线程的理解将会更加深刻！ 1、新建状态（New）：新创建了一个线程对象。 2、就绪状态（Runnable）：线程对象创建后，其他线程调用了该对象的start()方法。该状态的线程位于可运行线程池中，变得可运行，等待获取CPU的使用权。 3、运行状态（Running）：就绪状态的线程获取了CPU，执行程序代码。 4、阻塞状态（Blocked）：阻塞状态是线程因为某种原因放弃CPU使用权，暂时停止运行。直到线程进入就绪状态，才有机会转到运行状态。阻塞的情况分三种： （一）、等待阻塞：运行的线程执行wait()方法，JVM会把该线程放入等待池中。(wait会释放持有的锁) （二）、同步阻塞：运行的线程在获取对象的同步锁时，若该同步锁被别的线程占用，则JVM会把该线程放入锁池中。 （三）、其他阻塞：运行的线程执行sleep()或join()方法，或者发出了I/O请求时，JVM会把该线程置为阻塞状态。当sleep()状态超时、join()等待线程终止或者超时、或者I/O处理完毕时，线程重新转入就绪状态。（注意,sleep是不会释放持有的锁） 5、死亡状态（Dead）：线程执行完了或者因异常退出了run()方法，该线程结束生命周期。 五、线程调度线程的调度 1、调整线程优先级：Java线程有优先级，优先级高的线程会获得较多的运行机会。 Java线程的优先级用整数表示，取值范围是1~10，Thread类有以下三个静态常量：123456//线程可以具有的最高优先级，取值为10。static int MAX_PRIORITY//线程可以具有的最低优先级，取值为1。static int MIN_PRIORITY//分配给线程的默认优先级，取值为5。static int NORM_PRIORITY Thread类的setPriority()和getPriority()方法分别用来设置和获取线程的优先级。 每个线程都有默认的优先级。主线程的默认优先级为Thread.NORM_PRIORITY。 线程的优先级有继承关系，比如A线程中创建了B线程，那么B将和A具有相同的优先级。 JVM提供了10个线程优先级，但与常见的操作系统都不能很好的映射。如果希望程序能移植到各个操作系统中，应该仅仅使用Thread类有以下三个静态常量作为优先级，这样能保证同样的优先级采用了同样的调度方式。 2、线程睡眠：Thread.sleep(long millis)方法，使线程转到阻塞状态。millis参数设定睡眠的时间，以毫秒为单位。当睡眠结束后，就转为就绪（Runnable）状态。sleep()平台移植性好。 3、线程等待：Object类中的wait()方法，导致当前的线程等待，直到其他线程调用此对象的 notify() 方法或 notifyAll() 唤醒方法。这个两个唤醒方法也是Object类中的方法，行为等价于调用 wait(0) 一样。 4、线程让步：Thread.yield() 方法，暂停当前正在执行的线程对象，把执行机会让给相同或者更高优先级的线程。 5、线程加入：join()方法，等待其他线程终止。在当前线程中调用另一个线程的join()方法，则当前线程转入阻塞状态，直到另一个进程运行结束，当前线程再由阻塞转为就绪状态。 6、线程唤醒：Object类中的notify()方法，唤醒在此对象监视器上等待的单个线程。如果所有线程都在此对象上等待，则会选择唤醒其中一个线程。选择是任意性的，并在对实现做出决定时发生。线程通过调用其中一个 wait 方法，在对象的监视器上等待。 直到当前的线程放弃此对象上的锁定，才能继续执行被唤醒的线程。被唤醒的线程将以常规方式与在该对象上主动同步的其他所有线程进行竞争；例如，唤醒的线程在作为锁定此对象的下一个线程方面没有可靠的特权或劣势。类似的方法还有一个notifyAll()，唤醒在此对象监视器上等待的所有线程。 注意：Thread中suspend()和resume()两个方法在JDK1.5中已经废除，不再介绍。因为有死锁倾向。 六、常用函数说明① sleep(long millis): 在指定的毫秒数内让当前正在执行的线程休眠（暂停执行） ② join():指等待t线程终止。 ### 使用方式。 join是Thread类的一个方法，启动线程后直接调用，即join()的作用是：“等待该线程终止”，这里需要理解的就是该线程是指的主线程等待子线程的终止。也就是在子线程调用了join()方法后面的代码，只有等到子线程结束了才能执行。 1Thread t = new AThread(); t.start(); t.join(); ### 为什么要用join()方法 在很多情况下，主线程生成并起动了子线程，如果子线程里要进行大量的耗时的运算，主线程往往将于子线程之前结束，但是如果主线程处理完其他的事务后，需要用到子线程的处理结果，也就是主线程需要等待子线程执行完成之后再结束，这个时候就要用到join()方法了。 不加join。123456789101112131415161718192021222324252627282930313233343536373839/** *@functon 多线程学习,join *@author 林炳文 *@time 2015.3.9 */package com.multithread.join;class Thread1 extends Thread&#123; private String name; public Thread1(String name) &#123; super(name); this.name=name; &#125; public void run() &#123; System.out.println(Thread.currentThread().getName() + " 线程运行开始!"); for (int i = 0; i &lt; 5; i++) &#123; System.out.println("子线程"+name + "运行 : " + i); try &#123; sleep((int) Math.random() * 10); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; System.out.println(Thread.currentThread().getName() + " 线程运行结束!"); &#125;&#125; public class Main &#123; public static void main(String[] args) &#123; System.out.println(Thread.currentThread().getName()+"主线程运行开始!"); Thread1 mTh1=new Thread1("A"); Thread1 mTh2=new Thread1("B"); mTh1.start(); mTh2.start(); System.out.println(Thread.currentThread().getName()+ "主线程运行结束!"); &#125; &#125; 输出结果：main主线程运行开始!main主线程运行结束!B 线程运行开始!子线程B运行 : 0A 线程运行开始!子线程A运行 : 0子线程B运行 : 1子线程A运行 : 1子线程A运行 : 2子线程A运行 : 3子线程A运行 : 4A 线程运行结束!子线程B运行 : 2子线程B运行 : 3子线程B运行 : 4B 线程运行结束!发现主线程比子线程早结束 加join1234567891011121314151617181920212223public class Main &#123; public static void main(String[] args) &#123; System.out.println(Thread.currentThread().getName()+"主线程运行开始!"); Thread1 mTh1=new Thread1("A"); Thread1 mTh2=new Thread1("B"); mTh1.start(); mTh2.start(); try &#123; mTh1.join(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; try &#123; mTh2.join(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(Thread.currentThread().getName()+ "主线程运行结束!"); &#125; &#125; 运行结果：main主线程运行开始!A 线程运行开始!子线程A运行 : 0B 线程运行开始!子线程B运行 : 0子线程A运行 : 1子线程B运行 : 1子线程A运行 : 2子线程B运行 : 2子线程A运行 : 3子线程B运行 : 3子线程A运行 : 4子线程B运行 : 4A 线程运行结束!主线程一定会等子线程都结束了才结束 ③ yield():暂停当前正在执行的线程对象，并执行其他线程。 Thread.yield()方法作用是：暂停当前正在执行的线程对象，并执行其他线程。 yield()应该做的是让当前运行线程回到可运行状态，以允许具有相同优先级的其他线程获得运行机会。因此，使用yield()的目的是让相同优先级的线程之间能适当的轮转执行。但是，实际中无法保证yield()达到让步目的，因为让步的线程还有可能被线程调度程序再次选中。 结论：yield()从未导致线程转到等待/睡眠/阻塞状态。在大多数情况下，yield()将导致线程从运行状态转到可运行状态，但有可能没有效果。可看上面的图。 1234567891011121314151617181920212223242526272829303132333435/** *@functon 多线程学习 yield *@author 林炳文 *@time 2015.3.9 */package com.multithread.yield;class ThreadYield extends Thread&#123; public ThreadYield(String name) &#123; super(name); &#125; @Override public void run() &#123; for (int i = 1; i &lt;= 50; i++) &#123; System.out.println("" + this.getName() + "-----" + i); // 当i为30时，该线程就会把CPU时间让掉，让其他或者自己的线程执行（也就是谁先抢到谁执行） if (i ==30) &#123; this.yield(); &#125; &#125; &#125;&#125; public class Main &#123; public static void main(String[] args) &#123; ThreadYield yt1 = new ThreadYield("张三"); ThreadYield yt2 = new ThreadYield("李四"); yt1.start(); yt2.start(); &#125; &#125; 运行结果： 第一种情况：李四（线程）当执行到30时会CPU时间让掉，这时张三（线程）抢到CPU时间并执行。 第二种情况：李四（线程）当执行到30时会CPU时间让掉，这时李四（线程）抢到CPU时间并执行。 sleep()和yield()的区别&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;sleep()和yield()的区别):sleep()使当前线程进入停滞状态，所以执行sleep()的线程在指定的时间内肯定不会被执行；yield()只是使当前线程重新回到可执行状态，所以执行yield()的线程有可能在进入到可执行状态后马上又被执行。&nbsp;&nbsp;&nbsp;sleep 方法使当前运行中的线程睡眼一段时间，进入不可运行状态，这段时间的长短是由程序设定的，yield 方法使当前线程让出 CPU 占有权，但让出的时间是不可设定的。实际上，yield()方法对应了如下操作：先检测当前是否有相同优先级的线程处于同可运行状态，如有，则把 CPU 的占有权交给此线程，否则，继续运行原来的线程。所以yield()方法称为“退让”，它把运行机会让给了同等优先级的其他线程&nbsp;&nbsp;&nbsp;&nbsp;另外，sleep 方法允许较低优先级的线程获得运行机会，但 yield() 方法执行时，当前线程仍处在可运行状态，所以，不可能让出较低优先级的线程些时获得 CPU 占有权。在一个运行系统中，如果较高优先级的线程没有调用 sleep 方法，又没有受到 I\O 阻塞，那么，较低优先级线程只能等待所有较高优先级的线程运行结束，才有机会运行。 ④setPriority(): 更改线程的优先级。123MIN_PRIORITY = 1 NORM_PRIORITY = 5 MAX_PRIORITY = 10 用法： 1234Thread4 t1 = new Thread4("t1");Thread4 t2 = new Thread4("t2");t1.setPriority(Thread.MAX_PRIORITY);t2.setPriority(Thread.MIN_PRIORITY); ⑤interrupt():不要以为它是中断某个线程！它只是线线程发送一个中断信号，让线程在无限等待时（如死锁时）能抛出抛出，从而结束线程，但是如果你吃掉了这个异常，那么这个线程还是不会中断的！ ⑥wait() Obj.wait()，与Obj.notify()必须要与synchronized(Obj)一起使用，也就是wait,与notify是针对已经获取了Obj锁进行操作，从语法角度来说就是Obj.wait(),Obj.notify必须在synchronized(Obj){…}语句块内。从功能上来说wait就是说线程在获取对象锁后，主动释放对象锁，同时本线程休眠。直到有其它线程调用对象的notify()唤醒该线程，才能继续获取对象锁，并继续执行。相应的notify()就是对对象锁的唤醒操作。但有一点需要注意的是notify()调用后，并不是马上就释放对象锁的，而是在相应的synchronized(){}语句块执行结束，自动释放锁后，JVM会在wait()对象锁的线程中随机选取一线程，赋予其对象锁，唤醒线程，继续执行。这样就提供了在线程间同步、唤醒的操作。Thread.sleep()与Object.wait()二者都可以暂停当前线程，释放CPU控制权，主要的区别在于Object.wait()在释放CPU同时，释放了对象锁的控制。 &nbsp;&nbsp;&nbsp;&nbsp;单单在概念上理解清楚了还不够，需要在实际的例子中进行测试才能更好的理解。对Object.wait()，Object.notify()的应用最经典的例子，应该是三线程打印ABC的问题了吧，这是一道比较经典的面试题，题目要求如下： &nbsp;&nbsp;&nbsp;建立三个线程，A线程打印10次A，B线程打印10次B,C线程打印10次C，要求线程同时运行，交替打印10次ABC。这个问题用Object的wait()，notify()就可以很方便的解决。代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556/** * wait用法 * @author DreamSea * @time 2015.3.9 */package com.multithread.wait;public class MyThreadPrinter2 implements Runnable &#123; private String name; private Object prev; private Object self; private MyThreadPrinter2(String name, Object prev, Object self) &#123; this.name = name; this.prev = prev; this.self = self; &#125; @Override public void run() &#123; int count = 10; while (count &gt; 0) &#123; synchronized (prev) &#123; synchronized (self) &#123; System.out.print(name); count--; self.notify(); &#125; try &#123; prev.wait(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125; public static void main(String[] args) throws Exception &#123; Object a = new Object(); Object b = new Object(); Object c = new Object(); MyThreadPrinter2 pa = new MyThreadPrinter2("A", c, a); MyThreadPrinter2 pb = new MyThreadPrinter2("B", a, b); MyThreadPrinter2 pc = new MyThreadPrinter2("C", b, c); new Thread(pa).start(); Thread.sleep(100); //确保按顺序A、B、C执行 new Thread(pb).start(); Thread.sleep(100); new Thread(pc).start(); Thread.sleep(100); &#125; &#125; 输出结果： ABCABCABCABCABCABCABCABCABCABC &nbsp;&nbsp;&nbsp;&nbsp;先来解释一下其整体思路，从大的方向上来讲，该问题为三线程间的同步唤醒操作，主要的目的就是ThreadA-&gt;ThreadB-&gt;ThreadC-&gt;ThreadA循环执行三个线程。为了控制线程执行的顺序，那么就必须要确定唤醒、等待的顺序，所以每一个线程必须同时持有两个对象锁，才能继续执行。一个对象锁是prev，就是前一个线程所持有的对象锁。还有一个就是自身对象锁。主要的思想就是，为了控制执行的顺序，必须要先持有prev锁，也就前一个线程要释放自身对象锁，再去申请自身对象锁，两者兼备时打印，之后首先调用self.notify()释放自身对象锁，唤醒下一个等待线程，再调用prev.wait()释放prev对象锁，终止当前线程，等待循环结束后再次被唤醒。运行上述代码，可以发现三个线程循环打印ABC，共10次。程序运行的主要过程就是A线程最先运行，持有C,A对象锁，后释放A,C锁，唤醒B。线程B等待A锁，再申请B锁，后打印B，再释放B，A锁，唤醒C，线程C等待B锁，再申请C锁，后打印C，再释放C,B锁，唤醒A。看起来似乎没什么问题，但如果你仔细想一下，就会发现有问题，就是初始条件，三个线程按照A,B,C的顺序来启动，按照前面的思考，A唤醒B，B唤醒C，C再唤醒A。但是这种假设依赖于JVM中线程调度、执行的顺序。&nbsp;&nbsp;&nbsp;&nbsp;wait和sleep区别 共同点：1. 他们都是在多线程的环境下，都可以在程序的调用处阻塞指定的毫秒数，并返回。2. wait()和sleep()都可以通过interrupt()方法 打断线程的暂停状态 ，从而使线程立刻抛出InterruptedException。 如果线程A希望立即结束线程B，则可以对线程B对应的Thread实例调用interrupt方法。如果此刻线程B正在wait/sleep /join，则线程B会立刻抛出InterruptedException，在catch() {} 中直接return即可安全地结束线程。 需要注意的是，InterruptedException是线程自己从内部抛出的，并不是interrupt()方法抛出的。对某一线程调用 interrupt()时，如果该线程正在执行普通的代码，那么该线程根本就不会抛出InterruptedException。但是，一旦该线程进入到 wait()/sleep()/join()后，就会立刻抛出InterruptedException 。不同点：1. Thread类的方法：sleep(),yield()等 Object的方法：wait()和notify()等2. 每个对象都有一个锁来控制同步访问。Synchronized关键字可以和对象的锁交互，来实现线程的同步。 sleep方法没有释放锁，而wait方法释放了锁，使得其他线程可以使用同步控制块或者方法。 3. wait，notify和notifyAll只能在同步控制方法或者同步控制块里面使用，而sleep可以在任何地方使用所以sleep()和wait()方法的最大区别是： sleep()睡眠时，保持对象锁，仍然占有该锁； 而wait()睡眠时，释放对象锁。 但是wait()和sleep()都可以通过interrupt()方法打断线程的暂停状态，从而使线程立刻抛出InterruptedException（但不建议使用该方法）。 sleep（）方法sleep()使当前线程进入停滞状态（阻塞当前线程），让出CUP的使用、目的是不让当前线程独自霸占该进程所获的CPU资源，以留一定时间给其他线程执行的机会; sleep()是Thread类的Static(静态)的方法；因此他不能改变对象的机锁，所以当在一个Synchronized块中调用Sleep()方法是，线程虽然休眠了，但是对象的机锁并木有被释放，其他线程无法访问这个对象（即使睡着也持有对象锁）。 在sleep()休眠时间期满后，该线程不一定会立即执行，这是因为其它线程可能正在运行而且没有被调度为放弃执行，除非此线程具有更高的优先级。wait（）方法wait()方法是Object类里的方法；当一个线程执行到wait()方法时，它就进入到一个和该对象相关的等待池中，同时失去（释放）了对象的机锁（暂时失去机锁，wait(long timeout)超时时间到后还需要返还对象锁）；其他线程可以访问； wait()使用notify或者notifyAlll或者指定睡眠时间来唤醒当前等待池中的线程。 wiat()必须放在synchronized block中，否则会在program runtime时扔出”java.lang.IllegalMonitorStateException“异常。 七、常见线程名词解释主线程：JVM调用程序main()所产生的线程。 当前线程：这个是容易混淆的概念。一般指通过Thread.currentThread()来获取的进程。 后台线程：指为其他线程提供服务的线程，也称为守护线程。JVM的垃圾回收线程就是一个后台线程。用户线程和守护线程的区别在于，是否等待主线程依赖于主线程结束而结束 前台线程：是指接受后台线程服务的线程，其实前台后台线程是联系在一起，就像傀儡和幕后操纵者一样的关系。傀儡是前台线程、幕后操纵者是后台线程。由前台线程创建的线程默认也是前台线程。可以通过isDaemon()和setDaemon()方法来判断和设置一个线程是否为后台线程。 线程类的一些常用方法： 123456789101112sleep(): 强迫一个线程睡眠Ｎ毫秒。 isAlive(): 判断一个线程是否存活。 join(): 等待线程终止。 activeCount(): 程序中活跃的线程数。 enumerate(): 枚举程序中的线程。 currentThread(): 得到当前线程。 isDaemon(): 一个线程是否为守护线程。 setDaemon(): 设置一个线程为守护线程。(用户线程和守护线程的区别在于，是否等待主线程依赖于主线程结束而结束) setName(): 为线程设置一个名称。 wait(): 强迫一个线程等待。 notify(): 通知一个线程继续运行。 setPriority(): 设置一个线程的优先级。 八、线程同步1、synchronized关键字的作用域有二种： 1）是某个对象实例内，synchronized aMethod(){}可以防止多个线程同时访问这个对象的synchronized方法（如果一个对象有多个synchronized方法，只要一个线程访问了其中的一个synchronized方法，其它线程不能同时访问这个对象中任何一个synchronized方法）。这时，不同的对象实例的synchronized方法是不相干扰的。也就是说，其它线程照样可以同时访问相同类的另一个对象实例中的synchronized方法； 2）是某个类的范围，synchronized static aStaticMethod{}防止多个线程同时访问这个类中的synchronized static 方法。它可以对类的所有对象实例起作用。 2、除了方法前用synchronized关键字，synchronized关键字还可以用于方法中的某个区块中，表示只对这个区块的资源实行互斥访问。用法是: synchronized(this){/区块/}，它的作用域是当前对象； 3、synchronized关键字是不能继承的，也就是说，基类的方法synchronized f(){} 在继承类中并不自动是synchronized f(){}，而是变成了f(){}。继承类需要你显式的指定它的某个方法为synchronized方法； Java对多线程的支持与同步机制深受大家的喜爱，似乎看起来使用了synchronized关键字就可以轻松地解决多线程共享数据同步问题。到底如何？――还得对synchronized关键字的作用进行深入了解才可定论。 总的说来，synchronized关键字可以作为函数的修饰符，也可作为函数内的语句，也就是平时说的同步方法和同步语句块。如果再细的分类，synchronized可作用于instance变量、object reference（对象引用）、static函数和class literals(类名称字面常量)身上。 在进一步阐述之前，我们需要明确几点： A．无论synchronized关键字加在方法上还是对象上，它取得的锁都是对象，而不是把一段代码或函数当作锁――而且同步方法很可能还会被其他线程的对象访问。 B．每个对象只有一个锁（lock）与之相关联。 C．实现同步是要很大的系统开销作为代价的，甚至可能造成死锁，所以尽量避免无谓的同步控制。 接着来讨论synchronized用到不同地方对代码产生的影响： 假设P1、P2是同一个类的不同对象，这个类中定义了以下几种情况的同步块或同步方法，P1、P2就都可以调用它们。 1． 把synchronized当作函数修饰符时，示例代码如下： 123Public synchronized void methodAAA() &#123; //….&#125; 这也就是同步方法，那这时synchronized锁定的是哪个对象呢？它锁定的是调用这个同步方法对象。也就是说，当一个对象P1在不同的线程中执行这个同步方法时，它们之间会形成互斥，达到同步的效果。但是这个对象所属的Class所产生的另一对象P2却可以任意调用这个被加了synchronized关键字的方法。 上边的示例代码等同于如下代码： 12345public void methodAAA() &#123; synchronized (this) &#123; // (1) //….. &#125;&#125; (1)处的this指的是什么呢？它指的就是调用这个方法的对象，如P1。可见同步方法实质是将synchronized作用于object reference。――那个拿到了P1对象锁的线程，才可以调用P1的同步方法，而对P2而言，P1这个锁与它毫不相干，程序也可能在这种情形下摆脱同步机制的控制，造成数据混乱：（ 2．同步块，示例代码如下： 12345public void method3(SomeObject so) &#123; synchronized(so) &#123; //….. &#125;&#125; 这时，锁就是so这个对象，谁拿到这个锁谁就可以运行它所控制的那段代码。当有一个明确的对象作为锁时，就可以这样写程序，但当没有明确的对象作为锁，只是想让一段代码同步时，可以创建一个特殊的instance变量（它得是一个对象）来充当锁： 12345678class Foo implements Runnable &#123; private byte[] lock = new byte[0]; // 特殊的instance变量 Public void methodA() &#123; synchronized(lock) &#123; //… &#125; &#125; //…..&#125; 注：零长度的byte数组对象创建起来将比任何对象都经济――查看编译后的字节码：生成零长度的byte[]对象只需3条操作码，而Object lock = new Object()则需要7行操作码。 3．将synchronized作用于static 函数，示例代码如下： 12345678Class Foo &#123; public synchronized static void methodAAA() &#123; // 同步的static 函数 //…. &#125; public void methodBBB() &#123; synchronized(Foo.class) // class literal(类名称字面常量) &#125;&#125; &nbsp;&nbsp;代码中的methodBBB()方法是把class literal作为锁的情况，它和同步的static函数产生的效果是一样的，取得的锁很特别，是当前调用这个方法的对象所属的类（Class，而不再是由这个Class产生的某个具体对象了）。 记得在《Effective Java》一书中看到过将 Foo.class和 P1.getClass()用于作同步锁还不一样，不能用P1.getClass()来达到锁这个Class的目的。P1指的是由Foo类产生的对象。 可以推断：如果一个类中定义了一个synchronized的static函数A，也定义了一个synchronized 的instance函数B，那么这个类的同一对象Obj在多线程中分别访问A和B两个方法时，不会构成同步，因为它们的锁都不一样。A方法的锁是Obj这个对象，而B的锁是Obj所属的那个Class。 总结一下： 1、线程同步的目的是为了保护多个线程反问一个资源时对资源的破坏。2、线程同步方法是通过锁来实现，每个对象都有切仅有一个锁，这个锁与一个特定的对象关联，线程一旦获取了对象锁，其他访问该对象的线程就无法再访问该对象的其他非同步方法3、对于静态同步方法，锁是针对这个类的，锁对象是该类的Class对象。静态和非静态方法的锁互不干预。一个线程获得锁，当在一个同步方法中访问另外对象上的同步方法时，会获取这两个对象锁。4、对于同步，要时刻清醒在哪个对象上同步，这是关键。5、编写线程安全的类，需要时刻注意对多个线程竞争访问资源的逻辑和安全做出正确的判断，对“原子”操作做出分析，并保证原子操作期间别的线程无法访问竞争资源。6、当多个线程等待一个对象锁时，没有获取到锁的线程将发生阻塞。7、死锁是线程间相互等待锁锁造成的，在实际中发生的概率非常的小。真让你写个死锁程序，不一定好使，呵呵。但是，一旦程序发生死锁，程序将死掉。 九、线程数据传递在传统的同步开发模式下，当我们调用一个函数时，通过这个函数的参数将数据传入，并通过这个函数的返回值来返回最终的计算结果。但在多线程的异步开发模式下，数据的传递和返回和同步开发模式有很大的区别。由于线程的运行和结束是不可预料的，因此，在传递和返回数据时就无法象函数一样通过函数参数和return语句来返回数据。 9.1、通过构造方法传递数据在创建线程时，必须要建立一个Thread类的或其子类的实例。因此，我们不难想到在调用start方法之前通过线程类的构造方法将数据传入线程。并将传入的数据使用类变量保存起来，以便线程使用(其实就是在run方法中使用)。下面的代码演示了如何通过构造方法来传递数据： 123456789101112131415 package mythread; public class MyThread1 extends Thread &#123; private String name; public MyThread1(String name) &#123; this.name = name; &#125; public void run() &#123; System.out.println("hello " + name); &#125; public static void main(String[] args) &#123; Thread thread = new MyThread1("world"); thread.start(); &#125; &#125; 由于这种方法是在创建线程对象的同时传递数据的，因此，在线程运行之前这些数据就就已经到位了，这样就不会造成数据在线程运行后才传入的现象。如果要传递更复杂的数据，可以使用集合、类等数据结构。使用构造方法来传递数据虽然比较安全，但如果要传递的数据比较多时，就会造成很多不便。由于Java没有默认参数，要想实现类似默认参数的效果，就得使用重载，这样不但使构造方法本身过于复杂，又会使构造方法在数量上大增。因此，要想避免这种情况，就得通过类方法或类变量来传递数据。 9.2、通过变量和方法传递数据向对象中传入数据一般有两次机会，第一次机会是在建立对象时通过构造方法将数据传入，另外一次机会就是在类中定义一系列的public的方法或变量（也可称之为字段）。然后在建立完对象后，通过对象实例逐个赋值。下面的代码是对MyThread1类的改版，使用了一个setName方法来设置 name变量： 123456789101112131415161718192021 package mythread; public class MyThread2 implements Runnable &#123; private String name; public void setName(String name) &#123; this.name = name; &#125; public void run() &#123; System.out.println("hello " + name); &#125; public static void main(String[] args) &#123; MyThread2 myThread = new MyThread2(); myThread.setName("world"); Thread thread = new Thread(myThread); thread.start(); &#125; &#125; 9.3、通过回调函数传递数据 上面讨论的两种向线程中传递数据的方法是最常用的。但这两种方法都是main方法中主动将数据传入线程类的。这对于线程来说，是被动接收这些数据的。然而，在有些应用中需要在线程运行的过程中动态地获取数据，如在下面代码的run方法中产生了3个随机数，然后通过Work类的process方法求这三个随机数的和，并通过Data类的value将结果返回。从这个例子可以看出，在返回value之前，必须要得到三个随机数。也就是说，这个 value是无法事先就传入线程类的。 12345678910111213141516171819202122232425262728293031323334 package mythread; class Data &#123; public int value = 0; &#125; class Work &#123; public void process(Data data, Integer numbers) &#123; for (int n : numbers) &#123; data.value += n; &#125; &#125; &#125;public class MyThread3 extends Thread &#123; private Work work; public MyThread3(Work work) &#123; this.work = work; &#125; public void run() &#123; java.util.Random random = new java.util.Random(); Data data = new Data(); int n1 = random.nextInt(1000); int n2 = random.nextInt(2000); int n3 = random.nextInt(3000); work.process(data, n1, n2, n3); // 使用回调函数 System.out.println(String.valueOf(n1) + "+" + String.valueOf(n2) + "+" + String.valueOf(n3) + "=" + data.value); &#125; public static void main(String[] args) &#123; Thread thread = new MyThread3(new Work()); thread.start(); &#125; &#125; &nbsp;&nbsp;好了，Java多线程的基础知识就讲到这里了，有兴趣研究多线程的推荐直接看java的源码，你将会得到很大的提升！ 林炳文Evankaka原创作品。转载请注明出处http://blog.csdn.net/evankaka]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>多线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JavaScript工作原理：V8引擎和5招优化]]></title>
    <url>%2F2018%2F08%2F23%2FJavaScript%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86%EF%BC%9AV8%E5%BC%95%E6%93%8E%E5%92%8C5%E6%8B%9B%E4%BC%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[概述JavaScript引擎是执行JavaScript代码的程序或解释器。JavaScript引擎可以作为标准解释器或即时编译器来实现，该编译器以某种形式将JavaScript编译为字节码。 流行的JavaScript引擎： V8：开源，Google开发，C++，Chrome浏览器 Rhino：开源，Mozilla开发，Java SpiderMonkey：第一个JavaScript引擎，网景浏览器（之前）和Firefox（现在） JavaScriptCore：开源，苹果Safari浏览器 - Chakra（JSscript9）：Internet Explorer浏览器 Chakra（JavaScript）：Microsoft Edge浏览器 V8起源V8引擎是由Google构建的，以C++开发并且开源，与其它的引擎不同的是，V8还是Node.js的运行时环境。 V8最初设计用于提高浏览器内部JavaScript执行的性能。为了获得速度，V8将JavaScript代码转换为更高效的机器代码，而不是使用解释器。它通过实现JIT（Just-In-Time）编译器（如SpiderMonkey或Rhino（Mozilla）等许多现代JavaScript引擎）将JavaScript代码编译为机器代码。 这里的主要区别在于V8不生成字节码或任何中间代码。 V8曾经的两个编译器在V8引擎的v5.9版本出来之前，V8有两个编译器：full-codegen：一个简单而且速度非常快的编译器，可以生成简单且相对较慢的机器代码。Crankshaft：一种更复杂（Just-In-Time）的优化编译器，可以生成高度优化的代码。 V8引擎还在内部使用多个线程： 主线程完成预定的任务：获取你的代码，编译它然后执行它 一个单独的线程用于编译，当这个单独的线程优化代码时，主线程可以继续执行 一个Profiler线程，它会告诉运行时我们花了很多时间，使得Crankshaft能够优化它们 一些线程处理垃圾处理器扫描 当第一次执行JavaScript代码时，V8利用full-codegen，直接将解析的JavaScript翻译成机器代码而无需任何转换。这使它可以非常快速地开始执行机器代码。请注意，V8不使用中间字节码表示法，不需要解释器。 当您的代码运行一段时间后，Profiler线程已经收集了足够的数据以确定哪种方法应该进行优化。 接下来，Crankshaft优化从另一个线程开始。它将JavaScript抽象语法树翻译为称为Hydrogen的高级静态单分配（SSA）表示，并尝试优化该hydrogen图。 大多数优化都是在这个级别完成的。 优化：内联第一次优化是提前尽可能多地嵌入代码。 内联是将被调用函数的主体替换为调用网站（调用该函数的代码行）的过程。 这个简单的步骤可以让以下优化变得更有意义。 优化：隐藏的类JavaScript是一种基于原型的语言：没有类，对象的创建是通过克隆实现的。JavaScript也是一种动态编程语言，它意味着属性可以在实例化后轻松添加或从对象中移除。 大多数JavaScript解释器使用字典式结构（基于哈希函数）来存储对象属性值在内存中的位置。这种结构使得检索JavaScript中的属性的值比在Java或C＃等非动态编程语言中的计算更昂贵。在Java中，所有对象属性都是在编译之前由固定的对象布局确定的，并且不能在运行时动态添加或删除（当然，C＃的动态类型是另一个主题）。因此，属性的值（或指向这些属性的指针）可以作为连续缓冲区存储在内存中，每个值之间都有一个固定偏移量。偏移量的长度可以根据属性类型轻松确定，但在运行时可以更改属性类型的JavaScript中不可行。 由于使用字典查找内存中对象属性的位置效率非常低，因此V8使用不同的方法：隐藏类。隐藏类的工作方式与Java等语言中使用的固定对象布局（类）类似，除了它们是在运行时创建的。现在，让我们看看他们实际的样子： 12345function Point(x, y) &#123; this.x = x; this.y = y;&#125;var p1 = new Point(1, 2); 当“new Point(1, 2)”被执行时， V8引擎会创建一个名为C0的隐藏类。 由于Point还未定义任何属性，因此“C0”为空。 一旦执行了第一条语句“this.x = x”（在“Point”函数内部），V8将创建第二个隐藏类“C1”，它基于“C0”。“C1”描述了可以找到属性x的存储器中的位置（相对于对象指针）。在这种情况下，“x”存储在偏移量0处，这意味着在内存中将点对象视为连续缓冲区时，第一个偏移量将对应于属性“x”。 V8还将用“类别转换”更新“C0”，该类别转换指出如果将属性“x”添加到点对象，隐藏类应从“C0”切换到“C1”。 下面的点对象的隐藏类现在是“C1”。 每次将新属性添加到对象时，旧的隐藏类都会使用到新隐藏类的转换路径进行更新。隐藏类转换非常重要，因为它们允许隐藏类在以相同方式创建的对象之间共享。如果两个对象共享一个隐藏类并向它们添加了相同的属性，则转换将确保两个对象都接收到相同的新隐藏类以及随附的所有优化代码。 当执行语句“this.y = y”（同样，在“this.x = x”语句之后的Point函数内部）时，将重复此过程。 创建一个名为“C2”的新隐藏类，将类转换添加到“C1”，指出如果将属性“y”添加到Point对象（已包含属性“x”），则隐藏类应更改为 “C2”，点对象的隐藏类更新为“C2”。 隐藏类转换取决于将属性添加到对象的顺序。 看看下面的代码片段：123456789101112function Point(x, y) &#123; this.x = x; this.y = y;&#125; var p1 = new Point(1, 2);p1.a = 5;p1.b = 6; var p2 = new Point(3, 5);p2.b = 7;p2.a = 8; 现在，您可能认为对于p1和p2，将使用相同的隐藏类和转换。事实上却不是。对于“p1”，首先添加属性“a”，然后添加属性“b”。然而，对于“p2”，首先分配“b”，然后是“a”。 因此，由于不同的转换路径，“p1”和“p2”以不同的隐藏类结束。在这种情况下，以相同顺序初始化动态属性好得多，以便隐藏的类可以重用。 内联缓存V8利用另一种技术来优化称为内联缓存的动态类型化语言。内联缓存依赖于观察到对相同方法的重复调用倾向于发生在相同类型的对象上。在这里可以找到关于内联缓存的深入解释。 我们将讨论内联缓存的一般概念（如果您没有时间通过​​上面的深入解释）。 那么它是怎样工作的？ V8维护一个对象类型的缓存，这些对象在最近的方法调用中作为参数传递，并使用这些信息来预测将来作为参数传递的对象的类型。如果V8能够对传递给方法的对象的类型做出很好的假设，那么它可以绕过确定如何访问对象属性的过程，而是使用以前查找存储的信息到对象的隐藏课程。 那么隐藏类和内联缓存的概念如何相关？无论何时在特定对象上调用方法，V8引擎都必须执行对该对象的隐藏类的查找，以确定访问特定属性的偏移量。在相同隐藏类的两次成功调用之后，V8省略了隐藏类查找，并简单地将该属性的偏移量添加到对象指针本身。对于该方法的所有未来调用，V8引擎都假定隐藏的类没有更改，并使用从以前的查找存储的偏移量直接跳转到特定属性的内存地址。这大大提高了执行速度。 内联缓存也是为什么相同类型的对象共享隐藏类非常重要的原因。如果您创建两个具有相同类型和不同隐藏类的对象（就像我们之前的示例中那样），V8将无法使用内联缓存，因为即使这两个对象的类型相同，它们对应的隐藏类为其属性分配不同的偏移量。 编译为机器码一旦Hydrogen图被优化，Crankshaft将其降低到称为Lithium的较低级表示。大部分的Lithium实施都是特定于架构的。寄存器分配发生在这个级别。 最终，Lithium被编译成机器码。然后发生其他事情，称为OSR：堆栈替换。在我们开始编译和优化那些耗时较长的方法之前，我们可能会运行它。V8不会忘记它刚刚缓慢执行的内容，以再次优化版本开始。相反，它会转换我们拥有的所有上下文（堆栈，寄存器），以便我们可以在执行过程中切换到优化版本。这是一项非常复杂的任务，考虑到除了其他优化之外，V8最初还是将代码内联。 V8不是唯一能够做到的引擎。 有一种叫做去最佳化的保护措施可以做出相反的转变，并在引擎的假设不再成立的情况下恢复到非优化的代码。 垃圾收集对于垃圾收集，V8采用了传统的标记清除方式来清理老一代。标记阶段应该停止JavaScript执行。为了控制GC成本并使执行更加稳定，V8使用增量标记：不是遍历整个堆，而是试图标记每个可能的对象，它只走过堆的一部分，然后恢复正常执行。下一个GC停止将从先前堆走过的地方继续。这允许在正常执行期间非常短的暂停。如前所述，扫描阶段由单独的线程处理。 Ignition和TurboFan随着2017年早些时候发布V8 5.9，引入了新的执行流程。这个新的管道在实际的JavaScript应用程序中实现了更大的性能改进和显着的内存节省。 新的执行流程建立在Ignition，V8的解释器和TurboFan，V8的最新优化编译器之上。 您可以查看V8团队关于此主题的博客文章。 自从V8.5版本问世以来，V8团队一直在努力跟上新的JavaScript语言特性，而V8团队已经不再使用V8版本的全代码和Crankshaft（自2010年以来服务于V8的技术）。这些功能需要进行优化。 这意味着整体V8将有更简单和更可维护的架构。 这些改进仅仅是一个开始。 新的Ignition和TurboFan管道为进一步优化铺平了道路，这将在未来几年提升JavaScript性能并缩小V8在Chrome和Node.js中的占用空间。 最后，这里有一些关于如何编写优化的，更好的JavaScript的技巧和窍门。 您可以轻松地从上述内容中获取这些内容，但是，为了方便起见，以下是摘要： 如何编写优化的JavaScript 对象属性的顺序：始终以相同的顺序实例化对象属性，以便可以共享隐藏类和随后优化的代码。 动态属性：在实例化之后向对象添加属性将强制隐藏类更改，并减慢为先前隐藏类优化的所有方法。相反，在其构造函数中分配所有对象的属性。 方法：重复执行相同方法的代码将比仅执行一次（由于内联缓存）执行许多不同方法的代码运行得更快。 数组：避免稀疏数组，其中的键不是增量数字。稀疏数组中没有每个元素都是哈希表。这种阵列中的元素访问费用较高。另外，尽量避免预分配大型数组。随着你的成长，成长会更好。最后，不要删除数组中的元素。它使密钥稀疏。 标记值：V8用32位来表示对象和数字。由于它的31位，它使用1个bit来知道它是一个对象（flag = 1）还是一个称为SMI（SMall Integer）的整数（flag = 0）。然后，如果数字值大于31位，V8会将该数字框起来，将其变成双精度值并创建一个新对象以将该数字放入其中。尝试尽可能使用31位有符号数字以避免将昂贵的装箱操作转换为JS对象。 此文摘自《JavaScript工作原理（二）：V8引擎和5招优化》 作者：xupea]]></content>
      <categories>
        <category>前端</category>
      </categories>
      <tags>
        <tag>js</tag>
        <tag>V8</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SQL执行效率总结]]></title>
    <url>%2F2018%2F08%2F22%2FSQL%E6%89%A7%E8%A1%8C%E6%95%88%E7%8E%87%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[1.关于SQL查询效率，100w数据，查询只要1秒，与您分享: 机器情况 p4: 2.4 内存: 1 G os: windows 2003 数据库: ms sql server 2000 目的: 查询性能测试,比较两种查询的性能 SQL查询效率 step by step -- setp 1. – 建表 create table t_userinfo ( userid int identity(1,1) primary key nonclustered, nick varchar(50) not null default &#39;&#39;, classid int not null default 0, writetime datetime not null default getdate() ) go -- 建索引 create clustered index ix_userinfo_classid on t_userinfo(classid) go -- step 2.declare @i int declare @k int declare @nick varchar(10) set @i = 1 while @i&lt;1000000 begin set @k = @i % 10 set @nick = convert(varchar,@i) insert into t_userinfo(nick,classid,writetime) values(@nick,@k,getdate()) set @i = @i + 1 end -- 耗时 08:27 ，需要耐心等待 -- step 3. select top 20 userid,nick,classid,writetime from t_userinfo where userid not in ( select top 900000 userid from t_userinfoorder by userid asc ) -- 耗时 8 秒 ,够长的 -- step 4. select a.userid,b.nick,b.classid,b.writetime from ( select top 20 a.userid from ( select top 900020 userid from t_userinfo order by userid asc ) a order by a.userid desc ) a inner join t_userinfo b on a.userid = b.userid order by a.userid asc -- 耗时 1 秒，太快了吧，不可以思议 -- step 5 where 查询 select top 20 userid,nick,classid,writetime from t_userinfo where classid = 1 and userid not in ( select top 90000 userid from t_userinfo where classid = 1 order by userid asc ) – 耗时 2 秒 -- step 6 where 查询 select a.userid,b.nick,b.classid,b.writetime from ( select top 20 a.userid from ( select top 90000 userid from t_userinfo where classid = 1 order by userid asc ) a order by a.userid desc ) a inner join t_userinfo b on a.userid = b.userid order by a.userid asc -- 查询分析器显示不到 1 秒. 查询效率分析： 子查询为确保消除重复值，必须为外部查询的每个结果都处理嵌套查询。在这种情况下可以考虑用联接查询来取代。 如果要用子查询，那就用EXISTS替代IN、用NOT EXISTS替代NOT IN。因为EXISTS引入的子查询只是测试是否存在符合子查询中指定条件的行，效率较高。无论在哪种情况下,NOT IN都是最低效的。因为它对子查询中的表执行了一个全表遍历。 建立合理的索引,避免扫描多余数据，避免表扫描！ 几百万条数据，照样几十毫秒完成查询,对查询进行优化，应尽量避免全表扫描，首先应考虑在 where 及 order by 涉及的列上建立索引。 2.应尽量避免在 where 子句中对字段进行 null 值判断，否则将导致引擎放弃使用索引而进行全表扫描，如： select id from t where num is null 可以在num上设置默认值0，确保表中num列没有null值，然后这样查询： select id from t where num=0 3.应尽量避免在 where 子句中使用!=或&lt;&gt;操作符，否则将引擎放弃使用索引而进行全表扫描。 4.应尽量避免在 where 子句中使用 or 来连接条件，否则将导致引擎放弃使用索引而进行全表扫描，如： select id from t where num=10 or num=20 可以这样查询： select id from t where num=10 union all select id from t where num=20 5.in 和 not in 也要慎用，否则会导致全表扫描，如： select id from t where num in(1,2,3) 对于连续的数值，能用 between 就不要用 in 了： select id from t where num between 1 and 3 6.下面的查询也将导致全表扫描： select id from t where name like &#39;%c%&#39; 若要提高效率，可以考虑全文检索。 7.如果在 where 子句中使用参数，也会导致全表扫描。因为SQL只有在运行时才会解析局部变量，但优化程序不能将访问计划的选择推迟到运行时；它必须在编译时进行选择。然 而，如果在编译时建立访问计划，变量的值还是未知的，因而无法作为索引选择的输入项。如下面语句将进行全表扫描： select id from t where[num=@num](mailto:num=@num) 可以改为强制查询使用索引： select id from t with(index(索引名)) where[num=@num](mailto:num=@num) 8.应尽量避免在 where 子句中对字段进行表达式操作，这将导致引擎放弃使用索引而进行全表扫描。如： select id from t where num/2=100 应改为: select id from t where num=100*2 9.应尽量避免在where子句中对字段进行函数操作，这将导致引擎放弃使用索引而进行全表扫描。如： select id from t where substring(name,1,3)=&#39;abc&#39;--name以abc开头的id select id from t where datediff(day,createdate,&#39;2005-11-30&#39;)=0--‘2005-11-30’生成的id 应改为: select id from t where name like &#39;abc%&#39; select id from t where createdate&gt;=&#39;2005-11-30&#39; and createdate&lt;&#39;2005-12-1&#39; 10.不要在 where 子句中的“=”左边进行函数、算术运算或其他表达式运算，否则系统将可能无法正确使用索引。 11.在使用索引字段作为条件时，如果该索引是复合索引，那么必须使用到该索引中的第一个字段作为条件时才能保证系统使用该索引，否则该索引将不会被使用，并且应尽可能的让字段顺序与索引顺序相一致。 12.不要写一些没有意义的查询，如需要生成一个空表结构： select col1,col2 into #t from t where 1=0 这类代码不会返回任何结果集，但是会消耗系统资源的，应改成这样：create table #t(...) 13.很多时候用 exists 代替 in 是一个好的选择： select num from a where num in(select num from b) 用下面的语句替换： select num from a where exists(select 1 from b where num=a.num) 14.并不是所有索引对查询都有效，SQL是根据表中数据来进行查询优化的，当索引列有大量数据重复时，SQL查询可能不会去利用索引，如一表中有字段sex，male、female几乎各一半，那么即使在sex上建了索引也对查询效率起不了作用。 15.索引并不是越多越好，索引固然可以提高相应的 select 的效率，但同时也降低了 insert 及 update 的效率，因为 insert 或 update 时有可能会重建索引，所以怎样建索引需要慎重考虑，视具体情况而定。一个表的索引数最好不要超过6个，若太多则应考虑一些不常使用到的列上建的索引是否有 必要。 16.应尽可能的避免更新 clustered 索引数据列，因为 clustered 索引数据列的顺序就是表记录的物理存储顺序，一旦该列值改变将导致整个表记录的顺序的调整，会耗费相当大的资源。若应用系统需要频繁更新 clustered 索引数据列，那么需要考虑是否应将该索引建为 clustered 索引。 17.尽量使用数字型字段，若只含数值信息的字段尽量不要设计为字符型，这会降低查询和连接的性能，并会增加存储开销。这是因为引擎在处理查询和连接时会逐个比较字符串中每一个字符，而对于数字型而言只需要比较一次就够了。 18.尽可能的使用 varchar/nvarchar 代替 char/nchar ，因为首先变长字段存储空间小，可以节省存储空间，其次对于查询来说，在一个相对较小的字段内搜索效率显然要高些。 19.任何地方都不要使用 select * from t ，用具体的字段列表代替“*”，不要返回用不到的任何字段。 20.尽量使用表变量来代替临时表。如果表变量包含大量数据，请注意索引非常有限（只有主键索引）。 21.避免频繁创建和删除临时表，以减少系统表资源的消耗。 22.临时表并不是不可使用，适当地使用它们可以使某些例程更有效，例如，当需要重复引用大型表或常用表中的某个数据集时。但是，对于一次性事件，最好使用导出表。 23.在新建临时表时，如果一次性插入数据量很大，那么可以使用 select into 代替 create table，避免造成大量 log ，以提高速度；如果数据量不大，为了缓和系统表的资源，应先create table，然后insert。 24.如果使用到了临时表，在存储过程的最后务必将所有的临时表显式删除，先 truncate table ，然后 drop table ，这样可以避免系统表的较长时间锁定。 25.尽量避免使用游标，因为游标的效率较差，如果游标操作的数据超过1万行，那么就应该考虑改写。 26.使用基于游标的方法或临时表方法之前，应先寻找基于集的解决方案来解决问题，基于集的方法通常更有效。 27.与临时表一样，游标并不是不可使用。对小型数据集使用 FAST_FORWARD 游标通常要优于其他逐行处理方法，尤其是在必须引用几个表才能获得所需的数据时。在结果集中包括“合计”的例程通常要比使用游标执行的速度快。如果开发时 间允许，基于游标的方法和基于集的方法都可以尝试一下，看哪一种方法的效果更好。 28.在所有的存储过程和触发器的开始处设置 SET NOCOUNT ON ，在结束时设置 SET NOCOUNT OFF 。无需在执行存储过程和触发器的每个语句后向客户端发送 DONE_IN_PROC 消息。 29.尽量避免大事务操作，提高系统并发能力。 30.尽量避免向客户端返回大数据量，若数据量过大，应该考虑相应需求是否合理 1、避免将字段设为“允许为空”2、数据表设计要规范3、深入分析数据操作所要对数据库进行的操作4、尽量不要使用临时表5、多多使用事务6、尽量不要使用游标7、避免死锁8、要注意读写锁的使用9、不要打开大的数据集10、不要使用服务器端游标11、在程序编码时使用大数据量的数据库12、不要给“性别”列创建索引13、注意超时问题14、不要使用Select *15、在细节表中插入纪录时，不要在主表执行Select MAX(ID)16、尽量不要使用TEXT数据类型17、使用参数查询18、不要使用Insert导入大批的数据19、学会分析查询20、使用参照完整性21、用INNER JOIN 和LEFT JOIN代替Where 提高SQL查询效率（要点与技巧）：技巧一问题类型：ACCESS数据库字段中含有日文片假名或其它不明字符时查询会提示内存溢出。 解决方法：修改查询语句 sql=&quot;select * from tablename where column like &#39;%&quot;&amp;word&amp;&quot;%&#39;&quot; 改为 sql=&quot;select * from tablename&quot; rs.filter = &quot; column like &#39;%&quot;&amp;word&amp;&quot;%&#39;&quot; 技巧二问题类型：如何用简易的办法实现类似百度的多关键词查询（多关键词用空格或其它符号间隔）。 解决方法：12345//用空格分割查询字符串 ck=split(word," ")//得到分割后的数量 sck=UBound(ck) sql="select * tablename where" 在一个字段中查询 For i = 0 To sck SQL = SQL &amp; tempJoinWord &amp; "(" &amp; _ "column like '"&amp;ck(i)&amp;"%')" tempJoinWord = " and " Next 在二个字段中同时查询 For i = 0 To sck SQL = SQL &amp; tempJoinWord &amp; "(" &amp; _ "column like '"&amp;ck(i)&amp;"%' or " &amp; _ "column1 like '"&amp;ck(i)&amp;"%')" tempJoinWord = " and " Next 技巧三大大提高查询效率的几种技巧 尽量不要使用 or，使用or会引起全表扫描，将大大降低查询效率。 经过实践验证，charindex()并不比前面加%的like更能提高查询效率，并且charindex()会使索引失去作用（指sqlserver数据库） column like ‘%”&amp;word&amp;”%’ 会使索引不起作用column like ‘“&amp;word&amp;”%’ 会使索引起作用（去掉前面的%符号）（指sqlserver数据库） ‘%”&amp;word&amp;”%’ 与’”&amp;word&amp;”%’ 在查询时的区别：比如你的字段内容为 一个容易受伤的女人‘%”&amp;word&amp;”%’ ：会通配所有字符串，不论查“受伤”还是查“一个”，都会显示结果。‘“&amp;word&amp;”%’ ：只通配前面的字符串，例如查“受伤”是没有结果的，只有查“一个”，才会显示结果。 字段提取要按照“需多少、提多少”的原则，避免“select *”，尽量使用“select 字段1,字段2,字段3……..”。实践证明：每少提取一个字段，数据的提取速度就会有相应的提升。提升的速度还要看您舍弃的字段的大小来判断。 order by按聚集索引列排序效率最高。一个sqlserver数据表只能建立一个聚集索引，一般默认为ID，也可以改为其它的字段。 为你的表建立适当的索引，建立索引可以使你的查询速度提高几十几百倍。（指sqlserver数据库） 以下是建立索引与不建立索引的一个查询效率分析：Sqlserver索引与查询效率分析。表 News字段Id：自动编号Title：文章标题Author：作者Content：内容Star：优先级Addtime：时间记录：100万条测试机器：P4 2.8/1G内存/IDE硬盘 方案1：主键Id，默认为聚集索引，不建立其它非聚集索引select * from News where Title like &#39;%&quot;&amp;word&amp;&quot;%&#39; or Author like &#39;%&quot;&amp;word&amp;&quot;%&#39; order by Id desc从字段Title和Author中模糊检索，按Id排序查询时间：50秒 方案2：主键Id，默认为聚集索引在Title、Author、Star上建立非聚集索引select * from News where Title like &#39;&quot;&amp;word&amp;&quot;%&#39; or Author like &#39;&quot;&amp;word&amp;&quot;%&#39; order by Id desc从字段Title和Author中模糊检索，按Id排序查询时间：2 - 2.5秒 方案3：主键Id，默认为聚集索引在Title、Author、Star上建立非聚集索引select * from News where Title like &#39;&quot;&amp;word&amp;&quot;%&#39; or Author like &#39;&quot;&amp;word&amp;&quot;%&#39; order by Star desc从字段Title和Author中模糊检索，按Star排序查询时间：2 秒 方案4：主键Id，默认为聚集索引在Title、Author、Star上建立非聚集索引select * from News where Title like &#39;&quot;&amp;word&amp;&quot;%&#39; or Author like &#39;&quot;&amp;word&amp;&quot;%&#39;从字段Title和Author中模糊检索，不排序查询时间：1.8 - 2 秒 方案5：主键Id，默认为聚集索引在Title、Author、Star上建立非聚集索引select * from News where Title like &#39;&quot;&amp;word&amp;&quot;%&#39;或select * from News where Author like &#39;&quot;&amp;word&amp;&quot;%&#39;从字段Title 或 Author中检索，不排序查询时间：1秒 如何提高SQL语言的查询效率?问：请问我如何才能提高SQL语言的查询效率呢？答：这得从头说起：由于SQL是面向结果而不是面向过程的查询语言，所以一般支持SQL语言的大型关系型数据库都使用一个基于查询成本的优化器，为即时查询提供一个最佳的执行策略。对于优化器，输入是一条查询语句，输出是一个执行策略。一条SQL查询语句可以有多种执行策略，优化器将估计出全部执行方法中所需时间最少的所谓成本最低的那一种方法。所有优化都是基于用记所使用的查询语句中的where子句，优化器对where子句中的优化主要用搜索参数(Serach Argument)。搜索参数的核心思想就是数据库使用表中字段的索引来查询数据，而不必直接查询记录中的数据。带有 =、&lt;、&lt;=、&gt;、&gt;= 等操作符的条件语句可以直接使用索引，如下列是搜索参数：emp_id = “10001” 或 salary &gt; 3000 或 a =1 and c = 7而下列则不是搜索参数：salary = emp_salary 或 dep_id != 10 或 salary * 12 &gt;= 3000 或 a=1 or c=7应当尽可能提供一些冗余的搜索参数，使优化器有更多的选择余地。请看以下3种方法： 第一种方法：1234select employee.emp_name,department.dep_name from department,employee where (employee.dep_id = department.dep_id) and (department.dep_code="01") and (employee.dep_code="01"); 它的搜索分析结果如下：Estimate 2 I/O operationsScan department using primary keyfor rows where dep_code equals “01”Estimate getting here 1 timesScan employee sequentiallyEstimate getting here 5 times 第二种方法：123select employee.emp_name,department.dep_name from department,employeewhere (employee.dep_id=department.dep_id) and (department.dep_code="01"); 它的搜索分析结果如下：Estimate 2 I/O operationsScan department using primary keyfor rows where dep_code equals “01”Estimate getting here 1 timesScan employee sequentiallyEstimate getting here 5 times 第一种方法与第二种运行效率相同，但第一种方法最好，因为它为优化器提供了更多的选择机会。 第三种方法：123select employee.emp_name,department.dep_name from department,employee where (employee.dep_id = department.dep_id) and (employee.dep_code="01"); 这种方法最不好，因为它无法使用索引，也就是无法优化…… 使用SQL语句时应注意以下几点：1、避免使用不兼容的数据类型。例如，Float和Integer，Char和Varchar，Binary和Long Binary不兼容的。数据类型的不兼容可能使优化器无法执行一些本可以进行的优化操作。例如：select emp_name form employee where salary &gt; 3000;在此语句中若salary是Float类型的，则优化器很难对其进行优化，因为3000是个整数，我们应在编程时使用3000.0而不要等运行时让DBMS进行转化。2、尽量不要使用表达式，因它在编绎时是无法得到的，所以SQL只能使用其平均密度来估计将要命中的记录数。3、避免对搜索参数使用其他的数学操作符。如：select emp_name from employee where salary * 12 &gt; 3000;`应改为：select emp_name from employee where salary &gt; 250; 4、避免使用 != 或 &lt;&gt; 等这样的操作符，因为它会使系统无法使用索引，而只能直接搜索表中的数据。 ORACAL中的应用一个1600万数据表－－短信上行表TBL_SMS_MO结构：12345678910111213141516171819202122232425262728293031323334353637CREATE TABLE TBL_SMS_MO ( SMS_ID NUMBER, MO_ID VARCHAR2(50), MOBILE VARCHAR2(11), SPNUMBER VARCHAR2(20), MESSAGE VARCHAR2(150), TRADE_CODE VARCHAR2(20), LINK_ID VARCHAR2(50), GATEWAY_ID NUMBER, GATEWAY_PORT NUMBER, MO_TIME DATE DEFAULT SYSDATE ); CREATE INDEX IDX_MO_DATE ON TBL_SMS_MO (MO_TIME) PCTFREE 10 INITRANS 2 MAXTRANS 255 STORAGE ( INITIAL 1M NEXT 1M MINEXTENTS 1 MAXEXTENTS UNLIMITED PCTINCREASE 0 ); CREATE INDEX IDX_MO_MOBILE ON TBL_SMS_MO (MOBILE) PCTFREE 10 INITRANS 2 MAXTRANS 255 STORAGE ( INITIAL 64K NEXT 1M MINEXTENTS 1 MAXEXTENTS UNLIMITED PCTINCREASE 0 ); 问题：从表中查询某时间段内某手机发送的短消息，如下SQL语句：12345SELECT MOBILE,MESSAGE,TRADE_CODE,MO_TIME FROM TBL_SMS_MO WHERE MOBILE='130XXXXXXXX' AND MO_TIME BETWEEN TO_DATE('2006-04-01','YYYY-MM-DD HH24:MI:SS') AND TO_DATE('2006-04-07','YYYY-MM-DD HH24:MI:SS') ORDER BY MO_TIME DESC 返回结果大约需要10分钟，应用于网页查询，简直难以忍受。分析：在PL/SQL Developer，点击“Explain Plan”按钮（或F5键），对SQL进行分析，发现缺省使用的索引是IDX_MO_DATE。问题可能出在这里，因为相对于总数量1600万数据来说， 都mobile的数据是很少的，如果使用ID_MO_MOBILE比较容易锁定数据。如下优化：12345SELECT MOBILE,MESSAGE,TRADE_CODE,MO_TIME FROM TBL_SMS_MO WHERE MOBILE='130XXXXXXXX' AND MO_TIME BETWEEN TO_DATE('2006-04-01','YYYY-MM-DD HH24:MI:SS') AND TO_DATE('2006-04-07','YYYY-MM-DD HH24:MI:SS') ORDER BY MO_TIME DESC 测试：按F8运行这个SQL，哇～… … 2.360s，这就是差别。用索引提高SQL Server性能特别说明 在微软的SQL Server系统中通过有效的使用索引可以提高数据库的查询性能，但是性能的提高取决于数据库的实现。在本文中将会告诉你如何实现索引并有效的提高数据库的性能。 在关系型数据库中使用索引能够提高数据库性能，这一点是非常明显的。用的索引越多，从数据库系统中得到数据的速度就越快。然而，需要注意的是，用的索引 越多，向数据库系统中插入新数据所花费的时间就越多。在本文中，你将了解到微软的SQL Server数据库所支持的各种不同类型的索引，在这里你将了解到如何使用不同的方法来实现索引，通过这些不同的实现方法，你在数据库的读性能方面得到的 远比在数据库的整体性能方面的损失要多得多。 索引的定义 索引是数据库的工具，通过使用索引，在数据库中获取数据的时 候，就可以不用扫描数据库中的所有数据记录，这样能够提高系统获取数据的性能。使用索引可以改变数据的组织方式，使得所有的数据都是按照相似的结构来组织 的，这样就可以很容易地实现数据的检索访问。索引是按照列来创建的，这样就可以根据索引列中的值来帮助数据库找到相应的数据。 索引的类型 微软的SQL Server 支持两种类型的索引：clustered 索引和nonclustered索引。Clustered 索引在数据表中按照物理顺序存储数据。因为在表中只有一个物理顺序，所以在每个表中只能有一个clustered索引。在查找某个范围内的数据 时，Clustered索引是一种非常有效的索引，因为这些数据在存储的时候已经按照物理顺序排好序了。 Nonclustered索引不会影响到下面的物理存储，但是它是由数据行指针构成的。如果已经存在一个clustered索引，在 nonclustered中的索引指针将包含clustered索引的位置参考。这些索引比数据更紧促，而且对这些索引的扫描速度比对实际的数据表扫描要 快得多。 如何实现索引 数据库可以自动创建某些索引。例如，微软的SQL Server系统通过自动创建唯一索引来强制实现UNIQUE约束，这样可以确保在数据库中不会插入重复数据。也可以使用CREATE INDEX语句或者通过SQL Server Enterprise Manager来创建其他索引，SQL Server Enterprise Manager还有一个索引创建模板来指导你如何创建索引。 得到更好的性能 虽然索引可以带来性能上的优势，但是同时 也将带来一定的代价。虽然SQL Server系统允许你在每个数据表中创建多达256个nonclustered索引，但是建议不要使用这么多的索引。因为索引需要在内存和物理磁盘驱动 器上使用更多的存储空间。在执行插入声明的过程中可能会在一定程度上导致系统性能的下降，因为在插入数据的时候是需要根据索引的顺序插入，而不是在第一个 可用的位置直接插入数据，这样一来，存在的索引越多将导致插入或者更新声明所需要的时间就越多。 在使用SQL Server系统创建索引的时候，建议参照下面的创建准则来实现： 正确的选择数据类型 在索引中使用某些数据类型可以提高数据库系统的效率，例如，Int，bigint， smallint，和tinyint等这些数据类型都非常适合于用在索引中，因为他们都占用相同大小的空间并且可以很容易地实现比较操作。其他的数据类型 如char和varchar的效率都非常低，因为这些数据类型都不适合于执行数学操作，并且执行比较操作的时间都比上面提到数据类型要长。 确保在使用的过程中正确的利用索引值 在执行查询操作时，可能所使用的列只是clustered的一部分，这时尤其要注意的是如何使用这些数据。当用这些数据列作为参数调用函数时，这些函数 可能会使现有的排序优势失效。例如，使用日期值作为索引，而为了实现比较操作，可能需要将这个日期值转换为字符串，这样将导致在查询过程中无法用到这个日 期索引值。 在创建多列索引时，需要注意列的顺序 数据库将根据第一列索引的值来排列记录，然后进一步根据第二列的值来排序，依次排序直到最后一个索引排序完毕。哪一列唯一数据值较少，哪一列就应该为第一个索引，这样可以确保数据可以通过索引进一步交叉排序。 在clustered索引中限制列的数量 在clustered索引中用到的列越多，在nonclustered索引中包含的clustered索引参考位置就越多，需要存储的数据也就越多。这样将增加包含索引的数据表的大小，并且将增加基于索引的搜索时间。 避免频繁更新clustered索引数据列 由于nonclustered 索引依赖于clustered 索引，所以如果构成clustered 索引的数据列频繁更新，将导致在nonclustered中存储的行定位器也将随之频繁更新。对于所有与这些列相关的查询来说，如果发生记录被锁定的情况 时，这将可能导致性能成本的增加。 分开操作（如果可能的话） 对于一个表来说，如果需要进行频繁的执行插入、更新操作，同时还有大量读操作的话，在可能的情况下尝试将这个表分开操作。所有的插入和更新操作可以在一个没有索引的表中操作，然后将其复制到另外一个表中，在这个表里有大量的索引可以优化读数据的能力。 适当的重建索引 Nonclustered索引包含clustered索引的指针，这样一来Nonclustered索引将从属于clustered 索引。当重建clustered索引时，首先是丢弃原来的索引，然后再使用CREATE INDEX 来创建索引，或者在使用CREATE INDEX 声明的同时将DROP_EXISTING 子句作为重建索引的一部分。将丢弃和创建分为几步将会导致多次重建nonclustered 索引，而不象使用DROP_EXISTING 子句那样，只重建一次nonclustered 索引。 明智的使用填充因子 数据存储在那些具有固定大小的连续内存页面内。随着新的记录行的加入，数据内存页将逐渐被填满，系统就必须执行数据页的拆分工作，通过这个拆分工作将部 分数据转移到下一个新的页面当中。这样的拆分之后，将加重系统的负担，并且会导致存储的数据支离破碎。填充因子可以维护数据之间的缺口，一般在创建索引的 时候，该索引的填充因子就已经被设置好了。这样一来，可以减少插入数据所引起的页面分裂的次数。因为只是在创建索引的时候才维护空间的大小，在增加数据或 者更新数据时不会去维护空间的大小。因此，要想能够充分的利用填充因子，就必须周期性的重建索引。由填充因子所造成的缺口将导致读性能的下降，因为随着数 据库的扩张，越来越多的磁盘存取工作需要读取数据。所以，在读的次数超过写的次数的时候，很重要的一点是考虑使用填充因子还是使用缺省方式合适。 管理层的决策 通过有效的使用索引，可以在微软的SQL Server系统中实现很好的查询功能，但是使用索引的效率取决于几种不同的实现决策。在索引的性能平衡方面，要做出正确的数据库管理决策意味着需要在良 好的性能和困境中抉择。在特定的情况下，本文给出的一些建议将有助于你做出正确的决策]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>SQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Oracle 分组统计，按照天、月份周和自然周、月、季度和年]]></title>
    <url>%2F2018%2F08%2F21%2FOracle-%E5%88%86%E7%BB%84%E7%BB%9F%E8%AE%A1%EF%BC%8C%E6%8C%89%E7%85%A7%E5%A4%A9%E3%80%81%E6%9C%88%E4%BB%BD%E5%91%A8%E5%92%8C%E8%87%AA%E7%84%B6%E5%91%A8%E3%80%81%E6%9C%88%E3%80%81%E5%AD%A3%E5%BA%A6%E5%92%8C%E5%B9%B4%2F</url>
    <content type="text"><![CDATA[做报表统计时会经常用到 周，月，季度，年进行分组统计，所以结合网络搜索推荐的sql，总结如下 123456-- 按天统计select to_char(t.CREATED+15/24, 'YYYY-MM-DD') as 天,sum(1) as 数量from TB_EXT_TRADE tWHERE t.TID LIKE 'SC%' OR t.TID LIKE 'WSC%'group by to_char(t.CREATED+15/24, 'YYYY-MM-DD') --trunc(t.CREATED, 'DD') 1234567-- 按自然周的日期统计 select to_char(next_day(t.CREATED+15/24 - 7,2),'YYYY-MM-DD') AS 周,sum(1) as 数量from TB_EXT_TRADE tWHERE t.TID LIKE 'SC%' OR t.TID LIKE 'WSC%'group by to_char(next_day(t.CREATED+15/24 - 7,2),'YYYY-MM-DD')ORDER BY 周; 1234567-- 按自然周统计 select to_char(t.CREATED,'iw') AS 周,sum(1) as 数量from TB_EXT_TRADE tWHERE t.TID LIKE 'SC%' OR t.TID LIKE 'WSC%'group by to_char(t.CREATED,'iw')ORDER BY 周; 12345678-- 按自然月统计 select to_char(t.CREATED,'YYYY-MM') as 月份,sum(1) as 数量from TB_EXT_TRADE tWHERE t.TID LIKE 'SC%' OR t.TID LIKE 'WSC%'GROUP BY to_char(t.CREATED,'YYYY-MM') -- to_char(t.CREATED+15/24,'yyyy-mm') 不大准确ORDER BY 月份; 1234567-- 按季统计 select to_char(t.CREATED,'q') 季度,sum(1) as 数量from TB_EXT_TRADE tWHERE t.TID LIKE 'SC%' OR t.TID LIKE 'WSC%'group by to_char(t.CREATED,'q')ORDER BY 季度 NULLS LAST; 1234567--按年统计 select to_char(t.CREATED,'yyyy') AS 年度,sum(1) as 数量from TB_EXT_TRADE tWHERE t.TID LIKE 'SC%' OR t.TID LIKE 'WSC%'group by to_char(t.CREATED,'yyyy')ORDER BY 年度;]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>SQL</tag>
        <tag>Oracle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring任务调度之Quartz]]></title>
    <url>%2F2018%2F08%2F21%2FSpring%E4%BB%BB%E5%8A%A1%E8%B0%83%E5%BA%A6%E4%B9%8BQuartz%2F</url>
    <content type="text"><![CDATA[Quartz是OpenSymphony开源组织在Job scheduling领域又一个开源项目，是完全由java开发的一个开源的任务日程管理系统，“任务进度管理器”就是一个在预先确定（被纳入日程）的时间到达时，负责执行（或者通知）其他软件组件的系统。Quartz用一个小Java库发布文件（.jar文件），这个库文件包含了所有Quartz核心功能。这些功能的主要接口(API)是Scheduler接口。它提供了简单的操作，例如：将任务纳入日程或者从日程中取消，开始/停止/暂停日程进度。 一、Quartz作业类的继承方式来讲，可以分为两类： 作业类需要继承自特定的作业类基类，如Quartz中需要继承自org.springframework.scheduling.quartz.QuartzJobBean；java.util.Timer中需要继承自java.util.TimerTask。 作业类即普通的java类，不需要继承自任何基类。 注:推荐使用第二种方式，因为这样所以的类都是普通类，不需要事先区别对待。 从任务调度的触发时机来分，这里主要是针对作业使用的触发器，主要有以下两种： 每隔指定时间则触发一次，在Quartz中对应的触发器为：org.springframework.scheduling.quartz.SimpleTriggerBean 每到指定时间则触发一次，在Quartz中对应的调度器为：org.springframework.scheduling.quartz.CronTriggerBean 注：并非每种任务都可以使用这两种触发器，如java.util.TimerTask任务就只能使用第一种。Quartz和spring task都可以支持这两种触发条件。 第一种，作业类继承自特定的基类：org.springframework.scheduling.quartz.QuartzJobBean第一步：定义作业类12345678910111213141516171819202122import org.quartz.JobExecutionContext; import org.quartz.JobExecutionException; import org.springframework.scheduling.quartz.QuartzJobBean; public class Job1 extends QuartzJobBean &#123; private int timeout; private static int i = 0; //调度工厂实例化后，经过timeout时间开始执行调度 public void setTimeout(int timeout) &#123; this.timeout = timeout; &#125; /** * 要调度的具体任务 */ @Override protected void executeInternal(JobExecutionContext context) throws JobExecutionException &#123; System.out.println("定时任务执行中…"); &#125; &#125; 第二步：spring配置文件中配置作业类JobDetailBean12345678&lt;bean name="job1" class="org.springframework.scheduling.quartz.JobDetailBean"&gt; &lt;property name="jobClass" value="com.gy.Job1" /&gt; &lt;property name="jobDataAsMap"&gt; &lt;map&gt; &lt;entry key="timeout" value="0" /&gt; &lt;/map&gt; &lt;/property&gt; &lt;/bean&gt; 说明：org.springframework.scheduling.quartz.JobDetailBean有两个属性，jobClass属性即我们在java代码中定义的任务类，jobDataAsMap属性即该任务类中需要注入的属性值。 第三步：配置作业调度的触发方式（触发器） Quartz的作业触发器有两种，分别是 org.springframework.scheduling.quartz.SimpleTriggerBean org.springframework.scheduling.quartz.CronTriggerBean 第一种 SimpleTriggerBean，只支持按照一定频度调用任务，如每隔30分钟运行一次。配置方式如下：12345&lt;bean id="simpleTrigger" class="org.springframework.scheduling.quartz.SimpleTriggerBean"&gt; &lt;property name="jobDetail" ref="job1" /&gt; &lt;property name="startDelay" value="0" /&gt;&lt;!--调度工厂实例化后，经过0秒开始执行调度 --&gt; &lt;property name="repeatInterval" value="2000" /&gt;&lt;!--每2秒调度一次 --&gt; &lt;/bean&gt; 第二种 CronTriggerBean，支持到指定时间运行一次，如每天12:00运行一次等。配置方式如下：12345&lt;bean id="cronTrigger" class="org.springframework.scheduling.quartz.CronTriggerBean"&gt; &lt;property name="jobDetail" ref="job1" /&gt; &lt;!--每天12:00运行一次--&gt; &lt;property name="cronExpression" value="0 0 12 * * ?" /&gt; &lt;/bean&gt; 第四步：配置调度工厂1234567&lt;bean class="org.springframework.scheduling.quartz.SchedulerFactoryBean"&gt; &lt;property name="triggers"&gt; &lt;list&gt; &lt;ref bean="cronTrigger" /&gt; &lt;/list&gt; &lt;/property&gt; &lt;/bean&gt; 说明：该参数指定的就是之前配置的触发器的名字。 第五步：启动你的应用即可，即将工程部署至tomcat或其他容器。 第二种，作业类不继承特定基类。Spring能够支持这种方式，归功于两个类： org.springframework.scheduling.timer.MethodInvokingTimerTaskFactoryBean org.springframework.scheduling.quartz.MethodInvokingJobDetailFactoryBean 这两个类分别对应spring支持的两种实现任务调度的方式，即前文提到到java自带的timer task方式和Quartz方式。这里我只写MethodInvokingJobDetailFactoryBean的用法，使用该类的好处是,我们的任务类不再需要继承自任何类，而是普通的pojo。 第一步：编写任务类12345public class Job2 &#123; public void doJob2() &#123; System.out.println("不继承QuartzJobBean方式-调度进行中…"); &#125; &#125; 说明：可以看出，这就是一个普通的类，并且有一个方法。 第二步：配置作业类1234567&lt;bean id="job2" class="org.springframework.scheduling.quartz.MethodInvokingJobDetailFactoryBean"&gt; &lt;property name="targetObject"&gt; &lt;bean class="com.gy.Job2" /&gt; &lt;/property&gt; &lt;property name="targetMethod" value="doJob2" /&gt; &lt;property name="concurrent" value="false" /&gt;&lt;!-- 作业不并发调度 --&gt; &lt;/bean&gt; 说明：这一步是关键步骤，声明一个MethodInvokingJobDetailFactoryBean，有两个关键属性：targetObject指定任务类，targetMethod指定运行的方法。往下的步骤就与方法一相同了，为了完整，同样贴出。 第三步：配置作业调度的触发方式（触发器） Quartz的作业触发器有两种，分别是 org.springframework.scheduling.quartz.SimpleTriggerBean org.springframework.scheduling.quartz.CronTriggerBean 第一种SimpleTriggerBean，只支持按照一定频度调用任务，如每隔30分钟运行一次。配置方式如下：12345&lt;bean id="simpleTrigger" class="org.springframework.scheduling.quartz.SimpleTriggerBean"&gt; &lt;property name="jobDetail" ref="job2" /&gt; &lt;property name="startDelay" value="0" /&gt;&lt;!-- 调度工厂实例化后，经过0秒开始执行调度 --&gt; &lt;property name="repeatInterval" value="2000" /&gt;&lt;!-- 每2秒调度一次 --&gt; &lt;/bean&gt; 第二种CronTriggerBean，支持到指定时间运行一次，如每天12:00运行一次等。配置方式如下：12345&lt;bean id="cronTrigger" class="org.springframework.scheduling.quartz.CronTriggerBean"&gt; &lt;property name="jobDetail" ref="job2" /&gt; &lt;!--每天12:00运行一次 --&gt; &lt;property name="cronExpression" value="0 0 12 * * ?" /&gt; &lt;/bean&gt; 以上两种调度方式根据实际情况，任选一种即可。 第四步：配置调度工厂1234567&lt;bean class="org.springframework.scheduling.quartz.SchedulerFactoryBean"&gt; &lt;property name="triggers"&gt; &lt;list&gt; &lt;ref bean="cronTrigger" /&gt; &lt;/list&gt; &lt;/property&gt; &lt;/bean&gt; 说明：该参数指定的就是之前配置的触发器的名字。 第五步：启动你的应用即可，即将工程部署至tomcat或其他容器。到此，spring中Quartz的基本配置就介绍完了，当然了，使用之前，要导入相应的spring的包与Quartz的包，这些就不消多说了。 参考《官网》《Quartz任务调度快速入门》《深入解读Quartz的原理》《Quartz学习-阿飞(dufyun)》《Spring任务调度之Quartz-独具匠心》《基于Quartz开发企业级任务调度应用》《Quartz 数据库表含义解释》《Quartz源码分析》《Quartz系列》]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>Spring</tag>
        <tag>Quartz</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring+Quartz的版本问题]]></title>
    <url>%2F2018%2F08%2F21%2FSpring-Quartz%E7%9A%84%E7%89%88%E6%9C%AC%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[1Caused by: java.lang.IncompatibleClassChangeError: class org.springframework.scheduling.quartz.CronTriggerBean has interface org.quartz.CronTrigger as superclass 原因是Spring 3.0版本中内置的Quartz版本是&lt;2.0的，在使用最新的Quartz包(&gt;2.0)之后，接口不兼容。 解决办法有三种： 1.降低Quartz版本，降到1.X去。 2.升级Spring版本到3.1+，根据Spring的建议，将原来的TriggerBean替换成TriggerFactoryBean，例如CronTriggerBean 就可以替换成 CronTriggerFactoryBean。替换之后问题解决。 3.如果不在xml配置文件中引用 Spring 3.0 是支持 Quartz2.2.1(目前最新版本)，直接在程序中调用即可。]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>Spring</tag>
        <tag>Quartz</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python骚操作：微信远程控制电脑]]></title>
    <url>%2F2018%2F08%2F20%2FPython%E9%AA%9A%E6%93%8D%E4%BD%9C%EF%BC%9A%E5%BE%AE%E4%BF%A1%E8%BF%9C%E7%A8%8B%E6%8E%A7%E5%88%B6%E7%94%B5%E8%84%91%2F</url>
    <content type="text"><![CDATA[今天带给大家一个非常有意思的 python 程序，基于 itchat 实现微信控制电脑。你可以通过在微信发送命令，来拍摄当前电脑的使用者，然后图片会发送到你的微信上。甚至你可以发送命令来远程关闭电脑。 应用场景 你可爱又迷人的女朋友，在看到这篇教程之后，非常的开心。在你的电脑上部署了这个脚本，并且在你不知情的情况下，默默的登录上。随后跟你说我出去跟闺蜜逛街啦，今天就不陪你了。要记得不准吃鸡。 你心想老子终于可以放松一天了！开心的吃鸡！口上说着：“好的！亲爱的玩得开心！”等着女朋友出门以后，你就开启了吃鸡模式，在绝地求生里大开杀戒。 你的女朋友早已对你了如指掌，通过脚本，先让电脑截图留下现场证据，随后再打电话质问你是否在吃鸡，你如果撒谎就把电脑远程关机。 最后你想了下不对我没有女朋友啊，随后你转头微笑地看着你的室友。 程序思路 此次程序使用的环境是 python3.7 + windows7，在运行程序之前请先确保你已经安装好了 opencv-python 和 matplotlib。通过 pip install 即可安装。 12pip install opencv-pythonpip install matplotlib 程序主要是通过使用 itchat 库来登录到微信网页端，然后通过 itchat 来发送消息和接收消息。并通过 opencv 来调用电脑的摄像头，把当前使用电脑的用户拍照下来，发送到你的微信上。至于远程关机是通过调用 os 库，发送 cmd 命名即可实现。 程序源码 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758#!/usr/bin/env python# -*- coding: utf-8 -*-# @Time : 2018/8/20 11:12# @Author : yfzhou# @Site : # @File : wechat_control_computer.py# @Software: PyCharm# Life is short, I use python.import itchatimport osimport timeimport cv2sendMsg = u"&#123;消息助手&#125;：暂时无法回复"usageMsg = u"使用方法：\n1.运行CMD命令：cmd xxx (xxx为命令)\n" \ u"-例如关机命令:\ncmd shutdown -s -t 0 \n" \ u"2.获取当前电脑用户：cap\n3.启用消息助手(默认关闭)：ast\n" \ u"4.关闭消息助手：astc"flag = 0 # 消息助手开关nowTime = time.localtime()filename = str(nowTime.tm_mday) + str(nowTime.tm_hour) + str(nowTime.tm_min) + str(nowTime.tm_sec) + ".txt"myfile = open(filename, 'w')@itchat.msg_register('Text')def text_reply(msg): global flag message = msg['Text'] fromName = msg['FromUserName'] toName = msg['ToUserName'] if toName == "filehelper": if message == "cap": cap = cv2.VideoCapture(0) ret, img = cap.read() cv2.imwrite("weixinTemp.jpg", img) itchat.send('@img@%s' % u'weixinTemp.jpg', 'filehelper') cap.release() if message[0:3] == "cmd": os.system(message.strip(message[0:4])) if message == "ast": flag = 1 itchat.send("消息助手已开启", "filehelper") if message == "astc": flag = 0 itchat.send("消息助手已关闭", "filehelper") elif flag == 1: itchat.send(sendMsg, fromName) myfile.write(message) myfile.write("\n") myfile.flush()if __name__ == '__main__': itchat.auto_login() itchat.send(usageMsg, "filehelper") itchat.run() 程序并不复杂，定义了一些发送的消息，然后通过调用 itchat 和 cv2 相关库的操作，即可实现。关于 itchat 库的一些操作，可以去网上找相关的文档。 使用教程 获取源代码，然后在你的电脑上运行。随后会弹出一个微信网页登录的二维码。 使用你的手机微信扫描登录，等待一会儿，微信文件助手就会收到相应操作信息。 发送消息「cmd shutdown -s -t 0」即可让当前的电脑关闭。 发送消息「cap」即可调用电脑摄像头拍摄当前用户，然后把图片发送到微信上。 当然 cmd 命名还可以做更多有趣的事，大家可以自己去网上搜索下。]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python中深拷贝与浅拷贝的区别]]></title>
    <url>%2F2018%2F08%2F18%2FPython%E4%B8%AD%E6%B7%B1%E6%8B%B7%E8%B4%9D%E4%B8%8E%E6%B5%85%E6%8B%B7%E8%B4%9D%E7%9A%84%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[定义：在Python中对象的赋值其实就是对象的引用。当创建一个对象，把它赋值给另一个变量的时候，python并没有拷贝这个对象，只是拷贝了这个对象的引用而已。 浅拷贝：拷贝了最外围的对象本身，内部的元素都只是拷贝了一个引用而已。也就是，把对象复制一遍，但是该对象中引用的其他对象我不复制 深拷贝：外围和内部元素都进行了拷贝对象本身，而不是引用。也就是，把对象复制一遍，并且该对象中引用的其他对象我也复制。 几个术语的解释：1，变量：是一个系统表的元素，拥有指向对象的连接空间2，对象：被分配的一块内存，存储其所代表的值3，引用：是自动形成的从变量到对象的指针4，注意：类型（int类型，long类型(python3已去除long类型，只剩下int类型的数据)）属于对象，不是变量5，不可变对象：一旦创建就不可修改的对象，包括字符串、元组、数字6，可变对象：可以修改的对象，包括列表、字典。 应用的范围：1，切片可以应用于：列表、元组、字符串，但不能应用于字典。2，深浅拷贝，既可应用序列（列表、元组、字符串），也可应用字典。 深浅拷贝的作用：1，减少内存的使用2，以后在做数据的清洗、修改或者入库的时候，对原数据进行复制一份，以防数据修改之后，找不到原数据。 对于不可变对象的深浅拷贝：不可变对象类型，没有被拷贝的说法，即便是用深拷贝，查看id的话也是一样的，如果对其重新赋值，也只是新创建一个对象，替换掉旧的而已。一句话就是，不可变类型，不管是深拷贝还是浅拷贝，地址值和拷贝后的值都是一样的。 1234567891011121314151617181920212223242526272829303132333435a=(1,2,3)print("=====第一种=号浅拷贝=====")b=aprint(a)print(b)print(id(a))print(id(b))print("=====另一种copy浅拷贝===")b=copy.copy(a)print(a)print(b)print(id(a))print(id(b))print("=====深拷贝=====")b=copy.deepcopy(a)print(a)print(b)print(id(a))print(id(b))# 结果如下：=====浅拷贝=====(1, 2, 3)(1, 2, 3)28145223359522814522335952=====另一种浅拷贝===(1, 2, 3)(1, 2, 3)28145223359522814522335952=====深拷贝=====(1, 2, 3)(1, 2, 3)28145223359522814522335952 对于可变对象深浅拷贝:=浅拷贝：值相等，地址相等copy浅拷贝：值相等，地址不相等deepcopy深拷贝：值相等，地址不相等 1234567891011121314151617181920212223242526272829303132333435a=[1,2,3]print("=====第一种=号浅拷贝=====")b=aprint(a)print(b)print(id(a))print(id(b))print("=====另一种copy浅拷贝===")b=copy.copy(a)print(a)print(b)print(id(a))print(id(b))print("=====深拷贝=====")b=copy.deepcopy(a)print(a)print(b)print(id(a))print(id(b))#结果如下：=====浅拷贝=====[1, 2, 3][1, 2, 3]20076963215442007696321544=====另一种copy浅拷贝===[1, 2, 3][1, 2, 3]20076963215442007695909960=====深拷贝=====[1, 2, 3][1, 2, 3]20076963215442007696319560 总结：1，深浅拷贝都是对源对象的复制，占用不同的内存空间。2，不可变类型的对象，对于深浅拷贝毫无影响，最终的地址值和值都是相等的。3，可变类型：=浅拷贝： 值相等，地址相等copy浅拷贝：值相等，地址不相等deepcopy深拷贝：值相等，地址不相等]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo-Next搭建个人博客（使用图床）]]></title>
    <url>%2F2018%2F08%2F17%2FHexo-Next%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%EF%BC%88%E4%BD%BF%E7%94%A8%E5%9B%BE%E5%BA%8A%EF%BC%89%2F</url>
    <content type="text"><![CDATA[图片作为互联网服务中最基础的资源之一，随着互联网基础服务越来越专业化，图片的存储、处理、分发也发展成了一项独立的基础服务。试想一下，如果每家互联网公司都要花费大量人力物力去做图片相关的技术研发，哪还有时间去做自己的业务。专业的事情还是要交给专业的人来做。 图床，也就是专门提供存储图片的地方，我们只要通过图床提供的 API 接口，把图片上传上去，就可以通过外链访问了，根本不用操心图片是怎么存的，硬盘空间不够了，硬盘坏了，访问速度比较慢等等问题，这些图床都会帮我们搞定，他们会用各种技术帮我们做图片相关的优化和服务，比如多机互备、CDN 加速、图片处理、图片鉴黄、文本识别等等。 当然，图床也是有缺点的，当所有人都把图片存在同一个图床上，万一有一天图床真挂了，那所有图片就都无法访问了，虽然这种情况的概率很低，但并不等于不会发生。我就经历过云服务商机房被雷劈，网站都挂掉的情况。支付宝光缆不也被挖断过吗？不过，对于我们个人用户来说，要求也没那么高，图床已经完全能满足我们的需求了。 目前图床可以分为两种，一种是公共图床，一种是自建图床。公共图床也就是利用公共服务的图片上传接口，来提供图片外链的服务，比如新浪微博。自建图床，也就是利用各大云服务商提供的存储空间或者自己在 VPS 上使用开源软件来搭建图床，存储图片，生成外链提供访问，比如七牛、Lychee 开源自建图床方案。 公共图床微博图床由于微博本身就是面向公众提供服务，每个人发微博基本都得带上几张图片，以微博的体量，每天的新增图片数也不是个小数字。但是微博对于图片上传服务也没有接口说明文档，上传的接口还是在开发者们从微博产品里找出来的，可能微博只希望上传的图片仅仅用于微博产品本身吧。 微博图床的特点是免费，没有容量限制，全网 CDN 加速，支持 HTTPS，到哪里都很快。但是免费的服务也有不足的地方，上传的图片会被转成 jpg，图片中可能加上了肉眼难以识别的水印，另外微博的图片鉴别服务也可能会随时删除你的图片。 相关链接： 微博图床上传地址：从这里直接上传图片比较麻烦，你可以使用下面介绍的一些图床工具，上传起来更方便。 微博图床 API：用浏览器当然是不能访问的，只提供图片上传。 Imgur 图床 Imgur API Imgur 是一家国外老牌的图片存储服务商，国外速度很快，口碑不错，支持 HTTPS。但是国内速度很不稳定，所以追求国内速度的同学慎用。 相关链接： Imgur API SM.MS 图床 sm.ms 图床 SM.MS 是由 V2EX @Showfom 自建的，无外链限制，无流量限制的图床，支持 HTTPS，速度不错，已经运行两年多了。 相关链接： sm.ms API 其它公共图床还有很多，一搜一大把，不过大部分规模都比较小，要不就是国内访问速度不理想，使用前最好先了解一下。 目前自建图床方案有两种，一种是利用云服务商提供的存储服务来作为图床，通过 API 来管理图片，另一种是在 VPS 上安装开源的图片或文件管理程序，只要能提供外链，基本都可以作为图床来用。 自建图床：云服务七牛 七牛云 七牛 作为国内领先的云服务商，全网 CDN 加速，全国访问速度都不错，API 很详细，对开发者比较友好。免费用户提供 10GB 存储空间，国内和海外分别提供 10 GB 的 HTTP 免费流量，七牛的 HTTPS 流量是收费的，没有免费额度。此外，七牛还提供了针对图片的各种服务，包括图片裁剪，压缩，鉴黄等等衍生服务。如果你觉得图片尺寸太大，可以在外链后面添加参数，访问的时候七牛会自动根据你的参数对图片进行处理。 目前我的图片都存在七牛上，使用 HTTPS 外链，每个月支出也就几块钱，就为了博客上那个小绿锁 😆。 相关链接： 七牛云 API 文档 七牛价格 又拍云 又拍云 又拍云 也算是国内比较有名的云服务商了，国内拥有 200+ 的自建 CDN 节点，国内速度也不错，API 很详细，不过对于普通用户没有免费额度，目前实行的是「按照用户每日实际消耗的 CDN 流量，实行 1:1 的存储空间费免费使用」。 相关链接： 又拍云 API 文档 又拍云价格 阿里云 OSS 阿里云 OSS 阿里云 OSS（Object Storage Service），即阿里云对象存储服务，也可以作为图床，速度国内国外都不错，SDK 和 API 都很完善，收费也不算太贵，就是计费方案太复杂，目前费用包括：存储费用+流量费用+接口调用费用+数据处理，而且还分时段，地区，阶梯计费。可以选择包年包月和按量付费，具体价格和文档可以查看下面的官网介绍。 相关链接： 阿里云 OSS 文档 阿里云 OSS 价格 自建图床：开源方案如果你有 VPS，并且网络速度 OK 的话，自建图床也是一个不错的选择。 Lychee Lychee Lychee 是一个开源免费的基于 PHP 的图片管理系统，支持 Docker 部署，可以直接当做图床来用，Lychee 还支持很多扩展。 树洞外链 树洞外链 树洞外链 是一款免费开源的 PHP 外链网盘系统，界面简洁友好，支持七牛、本地、远程、阿里云OSS、又拍云五种储存方式，支持多用户系统，多上传方案策略。 相关链接： 演示站点 上传工具对普通用户来说，直接使用图床 API 很麻烦，我们可以借助一些工具方便的上传图片，下面就根据 macOS、Windows、Web 分别推荐几款工具。 iPic iPicMac 相关文章 下载 Mac iPic 是 macOS 上口碑最好的图床工具，支持 微博图床、七牛、阿里云 OSS、又拍云、Imgur、Flickr 等常见图床，支持拖拽、快捷键、剪贴板上传，支持上传前压缩，上传完毕自动生成 Markdown 并拷贝到剪贴板。如果你想迁移图床，开发者 @jason 还做了一款 图床迁移工具 iPic Mover 来帮助你。此外，简洁优雅的 Markdown 工具 Typora也内嵌了 iPic 的上传服务，如果你也使用 Typora 的话，能感觉到这俩工具简直是绝配。 MWeb MWebiOS 相关文章 下载 iOS 通用 Mac MWeb 中的图床支持 如果你只是码字的时候才用到图床，那可能 MWeb 也能满足你的需求，MWeb 支持七牛、imgur、Google Photos，还支持自定义图床，写作的时候只需要将图片拖进来，写作完成一键上传所有图片，也很方便。 Dropzone 3 Dropzone 3Mac 相关文章 下载 Mac Dropzone 3 也可以通过 七牛插件 来支持上传图片，和 MWeb 类似，具体教程可以看 这里。 MPic MPic MPic 目测是 Windows 上唯一的图床工具了，目前只支持七牛，把图片拖拽到软件窗口中就能上传。 Web使用 Web 技术开发的图床工具一抓一大把，大部分都基于七牛和微博图床 API，这里就介绍两个体验不错的吧： 极简图床：默认公共图床使用 sm.ms、微博图床，可以自定义支持七牛，界面简洁美观，支持 Chrome 插件，注册后还可以同步上传历史。 微博图床 Chroem 扩展：开源的图床工具，只支持微博图床，使用起来也很方便，可以批量上传，管理上传历史。 脚本如果你对上面推荐的产品不满意，并且你会折腾的话，可以使用这个脚本来完成图片上传：Markdown 图片实用工具 该脚本使用 Python 版的七牛 SDK 来实现上传功能，你可以按照相关介绍，搭配 Alfred 来快速完成图片上传。 图床服务最重要的是稳定性，大厂的云服务也都比较有保障，大家只要考虑下价格和易用性就可以了。就我个人而言，我首先推荐七牛，它的价格比较厚道，免费用户也有一定额度，数据可以自己掌控，另外各大平台的图床工具也基本都支持，易用性很高。其次推荐微博图床，对于不是很重要的图片，都可以存到微博图床，毕竟流量存储都免费，速度也不错。至于图床工具，就看自己的喜好了，只要顺手就行。但是不论选择哪一个服务或者工具，我觉得首先要自己可以掌控数据。 总之，适合自己的才是最好的。如果你还有其它好用的工具或者图床服务，欢迎留言给我，我会补充进来。 《图床神器》 《小贱贱图床》 《SM.SMb》 《嗯，图片就交给它了》]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>Next</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[InnoDB并发如此高，原因竟然在这？]]></title>
    <url>%2F2018%2F08%2F14%2FInnoDB%E5%B9%B6%E5%8F%91%E5%A6%82%E6%AD%A4%E9%AB%98%EF%BC%8C%E5%8E%9F%E5%9B%A0%E7%AB%9F%E7%84%B6%E5%9C%A8%E8%BF%99%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[此文摘自微信公众号【架构师之路】 微信扫一扫关注该公众号 《InnoDB行锁，如何锁住一条不存在的记录？》埋了一个坑，没想到评论反响剧烈，大家都希望深挖下去。原计划写写InnoDB的锁结束这个case，既然呼声这么高，干脆全盘系统性的写写InnoDB的并发控制，锁，事务模型好了。 体系相对宏大，一篇肯定写不完，容我娓娓道来，通俗地说清楚来龙去脉。 一、并发控制 为啥要进行并发控制？ 并发的任务对同一个临界资源进行操作，如果不采取措施，可能导致不一致，故必须进行并发控制（Concurrency Control）。 技术上，通常如何进行并发控制？ 通过并发控制保证数据一致性的常见手段有： 锁（Locking） 数据多版本（Multi Versioning） 二、锁 如何使用普通锁保证一致性？ 普通锁，被使用最多： (1)操作数据前，锁住，实施互斥，不允许其他的并发任务操作； (2)操作完成后，释放锁，让其他任务执行； 如此这般，来保证一致性。 普通锁存在什么问题？ 简单的锁住太过粗暴，连“读任务”也无法并行，任务执行过程本质上是串行的。 于是出现了共享锁与排他锁： 共享锁（Share Locks，记为S锁），读取数据时加S锁 排他锁（eXclusive Locks，记为X锁），修改数据时加X锁 共享锁与排他锁的玩法是： 共享锁之间不互斥，简记为：读读可以并行 排他锁与任何锁互斥，简记为：写读，写写不可以并行 可以看到，一旦写数据的任务没有完成，数据是不能被其他任务读取的，这对并发度有较大的影响。 画外音：对应到数据库，可以理解为，写事务没有提交，读相关数据的select也会被阻塞。 有没有可能，进一步提高并发呢？ 即使写任务没有完成，其他读任务也可能并发，这就引出了数据多版本。 三、数据多版本 数据多版本是一种能够进一步提高并发的方法，它的核心原理是： （1）写任务发生时，将数据克隆一份，以版本号区分； （2）写任务操作新克隆的数据，直至提交； （3）并发读任务可以继续读取旧版本的数据，不至于阻塞； 如上图： 最开始数据的版本是V0； T1时刻发起了一个写任务，这是把数据clone了一份，进行修改，版本变为V1，但任务还未完成； T2时刻并发了一个读任务，依然可以读V0版本的数据； T3时刻又并发了一个读任务，依然不会阻塞； 可以看到，数据多版本，通过“读取旧版本数据”能够极大提高任务的并发度。 提高并发的演进思路，就在如此： 普通锁，本质是串行执行 读写锁，可以实现读读并发 数据多版本，可以实现读写并发 画外音：这个思路，比整篇文章的其他技术细节更重要，希望大家牢记。 好，对应到InnoDB上，具体是怎么玩的呢？ 四、redo, undo,**回滚段** 在进一步介绍InnoDB如何使用“读取旧版本数据”极大提高任务的并发度之前，有必要先介绍下redo日志，undo日志，回滚段（rollback segment）。 为什么要有redo**日志？** 数据库事务提交后，必须将更新后的数据刷到磁盘上，以保证ACID特性。磁盘随机写性能较低，如果每次都刷盘，会极大影响数据库的吞吐量。 优化方式是，将修改行为先写到redo日志里（此时变成了顺序写），再定期将数据刷到磁盘上，这样能极大提高性能。 画外音：这里的架构设计方法是，随机写优化为顺序写，思路更重要。 假如某一时刻，数据库崩溃，还没来得及刷盘的数据，在数据库重启后，会重做redo日志里的内容，以保证已提交事务对数据产生的影响都刷到磁盘上。 一句话，redo日志用于保障，已提交事务的ACID特性。 为什么要有undo**日志？** 数据库事务未提交时，会将事务修改数据的镜像（即修改前的旧版本）存放到undo日志里，当事务回滚时，或者数据库奔溃时，可以利用undo日志，即旧版本数据，撤销未提交事务对数据库产生的影响。 画外音：更细节的， 对于insert操作，undo日志记录新数据的PK(ROW_ID)，回滚时直接删除； 对于delete/update操作，undo日志记录旧数据row，回滚时直接恢复； 他们分别存放在不同的buffer里。 一句话，undo日志用于保障，未提交事务不会对数据库的ACID特性产生影响。 什么是回滚段？ 存储undo日志的地方，是回滚段。 undo日志和回滚段和InnoDB的MVCC密切相关，这里举个例子展开说明一下。 栗子：1t(id PK, name); 数据为：1231, shenjian2, zhangsan3, lisi 此时没有事务未提交，故回滚段是空的。 接着启动了一个事务：1234start trx;delete (1, shenjian);update set(3, lisi) to (3, xxx);insert (4, wangwu); 并且事务处于未提交的状态。 可以看到： (1)被删除前的(1, shenjian)作为旧版本数据，进入了回滚段； (2)被修改前的(3, lisi)作为旧版本数据，进入了回滚段； (3)被插入的数据，PK(4)进入了回滚段； 接下来，假如事务rollback，此时可以通过回滚段里的undo日志回滚。 画外音：假设事务提交，回滚段里的undo日志可以删除。 可以看到： (1)被删除的旧数据恢复了； (2)被修改的旧数据也恢复了； (3)被插入的数据，删除了； 事务回滚成功，一切如故。 四、InnoDB**是基于多版本并发控制的存储引擎** 《大数据量，高并发量的互联网业务，一定要使用InnoDB》提到，InnoDB是高并发互联网场景最为推荐的存储引擎，根本原因，就是其多版本并发控制（Multi Version Concurrency Control, MVCC）。行锁，并发，事务回滚等多种特性都和MVCC相关。 MVCC就是通过“读取旧版本数据”来降低并发事务的锁冲突，提高任务的并发度。 核心问题： 旧版本数据存储在哪里？ 存储旧版本数据，对MySQL**和InnoDB**原有架构是否有巨大冲击？ 通过上文undo日志和回滚段的铺垫，这两个问题就非常好回答了： (1)旧版本数据存储在回滚段里； (2)对MySQL和InnoDB原有架构体系冲击不大； InnoDB的内核，会对所有row数据增加三个内部属性： (1)DB_TRX_ID，6字节，记录每一行最近一次修改它的事务ID； (2)DB_ROLL_PTR，7字节，记录指向回滚段undo日志的指针； (3)DB_ROW_ID，6字节，单调递增的行ID； InnoDB**为何能够做到这么高的并发？** 回滚段里的数据，其实是历史数据的快照（snapshot），这些数据是不会被修改，select可以肆无忌惮的并发读取他们。 快照读（Snapshot Read），这种一致性不加锁的读（Consistent Nonlocking Read），就是InnoDB并发如此之高的核心原因之一。 这里的一致性是指，事务读取到的数据，要么是事务开始前就已经存在的数据（当然，是其他已提交事务产生的），要么是事务自身插入或者修改的数据。 什么样的select**是快照读？** 除非显示加锁，普通的select语句都是快照读，例如：1select * from t where id&gt;2; 这里的显示加锁，非快照读是指：12select * from t where id&gt;2 **lock in share mode**;select * from t where id&gt;2 **for update**; 问题来了，这些显示加锁的读，是什么读？会加什么锁？和事务的隔离级别又有什么关系？ 本节的内容已经够多了，且听下回分解。 总结 (1)常见并发控制保证数据一致性的方法有锁，数据多版本； (2)普通锁串行，读写锁读读并行，数据多版本读写并行； (3)redo日志保证已提交事务的，设计思路是，通过顺序写替代随机写，提高并发； (4)undo日志用来回滚未提交的事务，它存储在回滚段里； (5)InnoDB是基于MVCC的存储引擎，它利用了存储在回滚段里的undo日志，即数据的旧版本，提高并发； (6)InnoDB之所以并发高，快照读不加锁； (7)InnoDB所有普通select都是快照读； 画外音：本文的知识点均基于MySQL5.6。 希望大家有收获，下一篇继续深入InnoDB的锁。 希望通俗的技术文被更多人看到，求帮转。 相关文章： 《InnoDB，5项最佳实践，知其所以然？》]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>SQL</tag>
        <tag>InnoDB</tag>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java8 Lambda表达式]]></title>
    <url>%2F2018%2F08%2F13%2Fjava8-Lambda%E8%A1%A8%E8%BE%BE%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[Java8发布已经有一段时间了，这次发布的改动比较大，很多人将这次改动与Java5的升级相提并论。Java8其中一个很重要的新特性就是lambda表达式，允许我们将行为传到函数中。想想看，在Java8之前我们想要将行为传入函数，仅有的选择就是匿名内部类。Java8发布以后，lambda表达式将大量替代匿名内部类的使用，简化代码的同时，更突出了原来匿名内部类中最重要的那部分包含真正逻辑的代码。尤其是对于做数据的同学来说，当习惯使用类似scala之类的函数式编程语言以后，体会将更加深刻。现在我们就来看看Java8中lambda表达式的一些常见写法。 1.替代匿名内部类毫无疑问，lambda表达式用得最多的场合就是替代匿名内部类，而实现Runnable接口是匿名内部类的经典例子。lambda表达式的功能相当强大，用()-&gt;就可以代替整个匿名内部类！请看代码： 如果使用匿名内部类：123456789@Testpublic void oldRunable() &#123; new Thread(new Runnable() &#123; @Override public void run() &#123; System.out.println("The old runable now is using!"); &#125; &#125;).start();&#125; 而如果使用lambda表达式：1234@Testpublic void runable() &#123; new Thread(() -&gt; System.out.println("It's a lambda function!")).start();&#125; 最后的输出：12The old runable now is using!It's a lambda function! 是不是强大到可怕？是不是简单到可怕？是不是清晰明了重点突出到可怕？这就是lambda表达式的可怕之处，用极少的代码完成了之前一个类做的事情！ 2.使用lambda表达式对集合进行迭代Java的集合类是日常开发中经常用到的，甚至说没有哪个java代码中没有使用到集合类。。。而对集合类最常见的操作就是进行迭代遍历了。请看对比：1234567891011@Testpublic void iterTest() &#123; List&lt;String&gt; languages = Arrays.asList("java","scala","python"); //before java8 for(String each:languages) &#123; System.out.println(each); &#125; //after java8 languages.forEach(x -&gt; System.out.println(x)); languages.forEach(System.out::println);&#125; 如果熟悉scala的同学，肯定对forEach不陌生。它可以迭代集合中所有的对象，并且将lambda表达式带入其中。1languages.forEach(System.out::println); 这一行看起来有点像c++里面作用域解析的写法，在这里也是可以的。 3.用lambda表达式实现map一提到函数式编程，一提到lambda表达式，怎么能不提map。。。没错，java8肯定也是支持的。请看示例代码：12345@Testpublic void mapTest() &#123; List&lt;Double&gt; cost = Arrays.asList(10.0, 20.0,30.0); cost.stream().map(x -&gt; x + x*0.05).forEach(x -&gt; System.out.println(x));&#125; 最后的输出结果：12310.521.031.5 map函数可以说是函数式编程里最重要的一个方法了。map的作用是将一个对象变换为另外一个。在我们的例子中，就是通过map方法将cost增加了0,05倍的大小然后输出。 4.用lambda表达式实现map与reduce既然提到了map，又怎能不提到reduce。reduce与map一样，也是函数式编程里最重要的几个方法之一。。。map的作用是将一个对象变为另外一个，而reduce实现的则是将所有值合并为一个，请看：123456@Testpublic void mapReduceTest() &#123; List&lt;Double&gt; cost = Arrays.asList(10.0, 20.0,30.0); double allCost = cost.stream().map(x -&gt; x+x*0.05).reduce((sum,x) -&gt; sum + x).get(); System.out.println(allCost);&#125; 最终的结果为：163.0 如果我们用for循环来做这件事情：12345678910@Testpublic void sumTest() &#123; List&lt;Double&gt; cost = Arrays.asList(10.0, 20.0,30.0); double sum = 0; for(double each:cost) &#123; each += each * 0.05; sum += each; &#125; System.out.println(sum);&#125; 相信用map+reduce+lambda表达式的写法高出不止一个level。 5.filter操作filter也是我们经常使用的一个操作。在操作集合的时候，经常需要从原始的集合中过滤掉一部分元素。1234567@Testpublic void filterTest() &#123; List&lt;Double&gt; cost = Arrays.asList(10.0, 20.0,30.0,40.0); List&lt;Double&gt; filteredCost = cost.stream().filter(x -&gt; x &gt; 25.0).collect(Collectors.toList()); filteredCost.forEach(x -&gt; System.out.println(x)); &#125; 最后的结果：1230.040.0 将java写出了python或者scala的感觉有没有！是不是帅到爆！ 6.与函数式接口Predicate配合除了在语言层面支持函数式编程风格，Java 8也添加了一个包，叫做 java.util.function。它包含了很多类，用来支持Java的函数式编程。其中一个便是Predicate，使用 java.util.function.Predicate 函数式接口以及lambda表达式，可以向API方法添加逻辑，用更少的代码支持更多的动态行为。Predicate接口非常适用于做过滤。1234567891011121314151617public static void filterTest(List&lt;String&gt; languages, Predicate&lt;String&gt; condition) &#123; languages.stream().filter(x -&gt; condition.test(x)).forEach(x -&gt; System.out.println(x + " "));&#125; public static void main(String[] args) &#123; List&lt;String&gt; languages = Arrays.asList("Java","Python","scala","Shell","R"); System.out.println("Language starts with J: "); filterTest(languages,x -&gt; x.startsWith("J")); System.out.println("Language ends with a: "); filterTest(languages,x -&gt; x.endsWith("a")); System.out.println("All languages: "); filterTest(languages,x -&gt; true); System.out.println("No languages: "); filterTest(languages,x -&gt; false); System.out.println("Language length bigger three: "); filterTest(languages,x -&gt; x.length() &gt; 4);&#125; 最后的输出结果： 1234567891011121314151617181920Language starts with J: Java Language ends with a: Java scala All languages: Java Python scala Shell R No languages: Language length bigger three: Python scala Shell 可以看到，Stream API的过滤方法也接受一个Predicate，这意味着可以将我们定制的 filter() 方法替换成写在里面的内联代码，这也是lambda表达式的魔力！ 参考文档：1.《Java8 lambda表达式10个示例》]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>jdk1.8</tag>
        <tag>Lambda</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo-Next搭建个人博客（添加网页标题崩溃欺骗搞怪特效）]]></title>
    <url>%2F2018%2F08%2F13%2FHexo-Next%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%EF%BC%88%E6%B7%BB%E5%8A%A0%E7%BD%91%E9%A1%B5%E6%A0%87%E9%A2%98%E5%B4%A9%E6%BA%83%E6%AC%BA%E9%AA%97%E6%90%9E%E6%80%AA%E7%89%B9%E6%95%88%EF%BC%89%2F</url>
    <content type="text"><![CDATA[crash_cheat.js在next\source\js\src文件夹下创建crash_cheat.js，添加代码： 1234567891011121314151617&lt;!--崩溃欺骗--&gt;var OriginTitle = document.title;var titleTime;document.addEventListener('visibilitychange', function () &#123; if (document.hidden) &#123; $('[rel="icon"]').attr('href', "/img/TEP.ico"); document.title = '╭(°A°`)╮ 页面崩溃啦 ~'; clearTimeout(titleTime); &#125; else &#123; $('[rel="icon"]').attr('href', "/favicon.ico"); document.title = '(ฅ&gt;ω&lt;*ฅ) 噫又好了~' + OriginTitle; titleTime = setTimeout(function () &#123; document.title = OriginTitle; &#125;, 2000); &#125;&#125;); 引用在next\layout\_layout.swig文件中，添加引用（注：在swig末尾添加）： 12&lt;!--崩溃欺骗--&gt;&lt;script type="text/javascript" src="/js/src/crash_cheat.js"&gt;&lt;/script&gt;]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>Next</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[前端杂记]]></title>
    <url>%2F2018%2F08%2F13%2F%E5%89%8D%E7%AB%AF%E6%9D%82%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[正则验证 1234567891011121314let regIdNo = /(^\d&#123;15&#125;$)|(^\d&#123;18&#125;$)|(^\d&#123;17&#125;(\d|X|x)$)/; let regtel=/^[1][3,4,5,7,8][0-9]&#123;9&#125;$/;let reglicence = /^[京津沪渝冀豫云辽黑湘皖鲁新苏浙赣鄂桂甘晋蒙陕吉闽贵粤青藏川宁琼使领A-Z]&#123;1&#125;[A-Z]&#123;1&#125;[A-Z0-9]&#123;4&#125;[A-Z0-9挂学警港澳]&#123;1&#125;$/;if (!regtel.test('##############')) &#123; alert('手机号格式有误')&#125;if(!regIdNo.test('##############'))&#123; alert('身份证号填写有误')&#125;if(!reglicence.test('##########'))&#123; alert('身份证号填写有误')&#125; 集合快速去除（ES6） 1Array.from(new Set(a)) 获取当前项目的路径 12345var urlRootContext = (function () &#123; var strPath = window.document.location.pathname var postPath = strPath.substring(0, strPath.substr(1).indexOf('/') + 1) return postPath&#125;)()]]></content>
      <categories>
        <category>前端</category>
      </categories>
      <tags>
        <tag>HTML</tag>
        <tag>js</tag>
        <tag>前端</tag>
        <tag>css</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HTML5 file API加canvas实现图片前端JS压缩并上传]]></title>
    <url>%2F2018%2F08%2F10%2FHTML5%20file%20API%E5%8A%A0canvas%E5%AE%9E%E7%8E%B0%E5%9B%BE%E7%89%87%E5%89%8D%E7%AB%AFJS%E5%8E%8B%E7%BC%A9%E5%B9%B6%E4%B8%8A%E4%BC%A0%2F</url>
    <content type="text"><![CDATA[by zhangxinxu from http://www.zhangxinxu.com/wordpress/?p=6308本文可全文转载，但需得到原作者书面许可，同时保留原作者和出处，摘要引流则随意。 一、图片上传前端压缩的现实意义对于大尺寸图片的上传，在前端进行压缩除了省流量外，最大的意义是极大的提高了用户体验。 这种体验包括两方面： 由于上传图片尺寸比较小，因此上传速度会比较快，交互会更加流畅，同时大大降低了网络异常导致上传失败风险。 最最重要的体验改进点：省略了图片的再加工成本。很多网站的图片上传功能都会对图片的大小进行限制，尤其是头像上传，限制5M或者2M以内是非常常见的。然后现在的数码设备拍摄功能都非常出众，一张原始图片超过2M几乎是标配，此时如果用户想把手机或相机中的某个得意图片上传作为自己的头像，就会遇到因为图片大小限制而不能上传的窘境，不得不对图片进行再处理，而这种体验其实非常不好的。如果可以在前端进行压缩，则理论上对图片尺寸的限制是没有必要的。 二、图片前端JS压缩并上传功能体验特意制作了一个图片前端压缩并上传的完整demo，您可以狠狠的点击这里：使用canvas在前端压缩图片并上传demo 进入demo会看到一个相貌平平的文件输入框： 啊，不对，应该是这张图： 点击文件选择框，我们不妨选一张尺寸比较大的图片，例如下面这种2M多的钓鱼收获照： 于是图片歘歘歘地传上去了： 此时我们点击最终上传完毕的图片地址，会发现原来2M多3000多像素宽的图片被限制为400像素宽了： 保存到本地会发现图片尺寸已经变成只有70K了： 以上就是图片前端压缩并上传demo的完整演示。 三、HTML5 file API加canvas实现图片前端JS压缩要想使用JS实现图片的压缩效果，原理其实很简单，核心API就是使用canvas的drawImage()方法。 canvas的drawImage()方法API如下： 123context.drawImage(img, dx, dy);context.drawImage(img, dx, dy, dWidth, dHeight);context.drawImage(img, sx, sy, sWidth, sHeight, dx, dy, dWidth, dHeight); 后面最复杂的语法虽然看上去有9大参数，但不用慌，实际上可以看出就3个参数： img 就是图片对象，可以是页面上获取的DOM对象，也可以是虚拟DOM中的图片对象。 dx, dy, dWidth, dHeight 表示在canvas画布上规划处一片区域用来放置图片，dx, dy为canvas元素的左上角坐标，dWidth, dHeight指canvas元素上用在显示图片的区域大小。如果没有指定sx,sy,sWidth,sHeight这4个参数，则图片会被拉伸或缩放在这片区域内。 sx,sy,swidth,sheight 这4个坐标是针对图片元素的，表示图片在canvas画布上显示的大小和位置。sx,sy表示图片上sx,sy这个坐标作为左上角，然后往右下角的swidth,sheight尺寸范围图片作为最终在canvas上显示的图片内容。 drawImage()方法有一个非常怪异的地方，大家一定要注意，那就是5参数和9参数里面参数位置是不一样的，这个和一般的API有所不同。一般API可选参数是放在后面。但是，这里的drawImage()9个参数时候，可选参数sx,sy,swidth,sheight是在前面的。如果不注意这一点，有些表现会让你无法理解。 下图为MDN上原理示意： 对于本文的图片压缩，需要用的是是5个参数语法。举个例子，一张图片（假设图片对象是img）的原始尺寸是4000*3000，现在需要把尺寸限制为400*300大小，很简单，原理如下代码示意：123456var canvas = document.createElement('canvas');var context = canvas.getContext('2d');canvas.width = 400;canvas.height = 300;// 核心JS就这个context.drawImage(img,0,0,400,300); 把一张大的图片，直接画在一张小小的画布上。此时大图片就天然变成了小图片，压缩就这么实现了，是不是简单的有点超乎想象。 当然，若要落地于实际开发，我们还需要做些其他的工作，就是要解决图片来源和图片去向的问题。 1. 如何把系统中图片呈现在浏览器中？HTML5 file API可以让图片在上传之前直接在浏览器中显示，通常使用FileReader方法，代码示意如下：123456789var reader = new FileReader(), img = new Image();// 读文件成功的回调reader.onload = function(e) &#123; // e.target.result就是图片的base64地址信息 img.src = e.target.result;&#125;;eleFile.addEventListener('change', function (event) &#123; reader.readAsDataURL(event.target.files\[0\]);&#125;); 于是，包含图片信息的context.drawImage()方法中的img图片就有了。 2. 如果把canvas画布转换成img图像canvas天然提供了2个转图片的方法，一个是： canvas.toDataURL()方法 语法如下：1canvas.toDataURL(mimeType, qualityArgument) 可以把图片转换成base64格式信息，纯字符的图片表示法。 其中：mimeType表示canvas导出来的base64图片的类型，默认是png格式，也即是默认值是&#39;image/png&#39;，我们也可以指定为jpg格式&#39;image/jpeg&#39;或者webp等格式。file对象中的file.type就是文件的mimeType类型，在转换时候正好可以直接拿来用（如果有file对象）。qualityArgument表示导出的图片质量，只要导出为jpg和webp格式的时候此参数才有效果，默认值是0.92，是一个比较合理的图片质量输出参数，通常情况下，我们无需再设定。 canvas.toBlob()方法 语法如下： canvas.toBlob(callback, mimeType, qualityArgument) 可以把canvas转换成Blob文件，通常用在文件上传中，因为是二进制的，对后端更加友好。 和toDataURL()方法相比，toBlob()方法是异步的，因此多了个callback参数，这个callback回调方法默认的第一个参数就是转换好的blob文件信息，本文demo的文件上传就是将canvas图片转换成二进制的blob文件，然后再ajax上传的，代码如下：12345678// canvas转为blob并上传canvas.toBlob(function (blob) &#123; // 图片ajax上传 var xhr = new XMLHttpRequest(); // 开始上传 xhr.open("POST", 'upload.php', true); xhr.send(blob); &#125;); 于是，经过“图片→canvas压缩→图片”三步曲，我们完成了图片前端压缩并上传的功能。 更加完整的核心代码请参见demo页面的左侧，如果对其他交互代码也敢兴趣，请参考页面源代码。 下面贴出完整代码： HTML代码：1&lt;input id="file" type="file"&gt; JS代码：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667var eleFile = document.querySelector('#file');// 压缩图片需要的一些元素和对象var reader = new FileReader(), img = new Image();// 选择的文件对象var file = null;// 缩放图片需要的canvasvar canvas = document.createElement('canvas');var context = canvas.getContext('2d');// base64地址图片加载完毕后img.onload = function () &#123; // 图片原始尺寸 var originWidth = this.width; var originHeight = this.height; // 最大尺寸限制 var maxWidth = 400, maxHeight = 400; // 目标尺寸 var targetWidth = originWidth, targetHeight = originHeight; // 图片尺寸超过400x400的限制 if (originWidth &gt; maxWidth || originHeight &gt; maxHeight) &#123; if (originWidth / originHeight &gt; maxWidth / maxHeight) &#123; // 更宽，按照宽度限定尺寸 targetWidth = maxWidth; targetHeight = Math.round(maxWidth * (originHeight / originWidth)); &#125; else &#123; targetHeight = maxHeight; targetWidth = Math.round(maxHeight * (originWidth / originHeight)); &#125; &#125; // canvas对图片进行缩放 canvas.width = targetWidth; canvas.height = targetHeight; // 清除画布 context.clearRect(0, 0, targetWidth, targetHeight); // 图片压缩 context.drawImage(img, 0, 0, targetWidth, targetHeight); // canvas转为blob并上传 canvas.toBlob(function (blob) &#123; // 图片ajax上传 var xhr = new XMLHttpRequest(); // 文件上传成功 xhr.onreadystatechange = function() &#123; if (xhr.status == 200) &#123; // xhr.responseText就是返回的数据 &#125; &#125;; // 开始上传 xhr.open("POST", 'upload.php', true); xhr.send(blob); &#125;, file.type || 'image/png');&#125;;// 文件base64化，以便获知图片原始尺寸reader.onload = function(e) &#123; img.src = e.target.result;&#125;;eleFile.addEventListener('change', function (event) &#123; file = event.target.files[0]; // 选择的文件是图片 if (file.type.indexOf("image") == 0) &#123; reader.readAsDataURL(file); &#125;&#125;); 四、结束语就在几个月前刚写过一篇文章“使用canvas在前端实现图片水印合成”，实际上所使用的技术和套路和本文是如出一辙的，也是“图片→canvas水印→图片”三步曲，区别在于水印合成是连续执行两次context.drawImage()方法，一次是原图一次水印图片，以及最后转换成图片的时候什么是toDataURL()方法，其他代码逻辑和原理都是一样的。 由此及彼，利用同样的原理和代码逻辑，我们还可以实现其它很多以前前端不太好实现的功能，比方说图片的真剪裁效果，所谓“真剪裁”指不是使用个overflow:hidden或者clip这些CSS属性的“伪剪裁”，而是真正意义上就这么大区域图片信息。甚至配合一些前端算法，我们可以直接在前端进行人脸识别，图片自动美化等一系列功能再上传等等。 原理都是一样的，都是利用canvas作为中间媒介进行处理。 好，以上就是本文的全部内容，感谢阅读，欢迎纠错，欢迎交流！ 本文为原创文章，会经常更新知识点以及修正一些错误，因此转载请保留原出处，方便溯源，避免陈旧错误知识的误导，同时有更好的阅读体验。本文地址：http://www.zhangxinxu.com/wordpress/?p=6308 相关文章 XMLHttpRequest实现HTTP协议下文件上传断点续传 HTML input type=file文件选择表单元素二三事 纯前端实现可传图可字幕台词定制的GIF表情生成器 小tips: 纯前端JS读取与解析本地文本类文件 Ajax Upload多文件上传插件翻译及中文演示 理解DOMString、Document、FormData、Blob、File、ArrayBuffer数据类型 基于HTML5的可预览多图片Ajax上传 小tips:使用canvas在前端实现图片水印合成 原来浏览器原生支持JS Base64编码解码 小tip:JS前端创建html或json文件并浏览器导出下载 SVG 简介与截图等应用]]></content>
      <categories>
        <category>前端</category>
      </categories>
      <tags>
        <tag>HTML</tag>
        <tag>file</tag>
        <tag>canvas</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在HTML在线预览PDF、word、xls、ppt等文件]]></title>
    <url>%2F2018%2F08%2F09%2F%E5%9C%A8HTML%E5%9C%A8%E7%BA%BF%E9%A2%84%E8%A7%88PDF%E3%80%81word%E3%80%81xls%E3%80%81ppt%E7%AD%89%E6%96%87%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[html实现pdf文件在线预览功能方式一、&lt;a&gt;标签pdf文件理论上可以在浏览器直接打开预览但是需要打开新页面。在仅仅是预览pdf文件且UI要求不高的情况下可以直接通过a标签href属性实现预览1&lt;a href="文档地址"&gt;预览pdf&lt;/a&gt; 方式二、jquery.media.js通过jquery插件jquery.media.js实现 这个插件可以实现pdf预览功能（包括其他各种媒体文件）但是对word等类型的文件无能为力。 实现方式： js代码：12&lt;script type=”text/javascript” src=”jquery-1.7.1.min.js”&gt;&lt;/script&gt;&lt;script type=”text/javascript” src=”jquery.media.js”&gt;&lt;/script&gt; html结构：123&lt;body&gt; &lt;div id="handout_wrap_inner"&gt;&lt;/div&gt;&lt;/body&gt; 调用方式：123456$('#handout_wrap_inner').media(&#123; width: '100%', height: '100%', autoplay: true, src:'http://storage.xuetangx.com/public_assets/xuetangx/PDF/PlayerAPI_v1.0.6.pdf',&#125;); 方式三、&lt;iframe&gt;标签1$("&lt;iframe src='previewUrl' width='100%' height='100%'&gt;&lt;/iframe&gt;").append($('body')) 此外还可以在iframe标签之间提供一个提示类似这样12345&lt;iframe src="previewUrl" width="100%" height="100%"&gt; This browser does not support PDFs. Please download the PDF to view it: &lt;a href="previewUrl"&gt;Download PDF&lt;/a&gt;&lt;/iframe&gt; 方式四、&lt;embed&gt;标签1&lt;embed src="previewUrl" type="application/pdf" width="100%" height="100%"&gt; 此标签h5特性中包含四个属性：高、宽、类型、预览文件src！与&lt;iframe&gt;&lt;/iframe&gt;不同，这个标签是自闭合的的，也就是说如果浏览器不支持PDF的嵌入，那么这个标签的内容什么都看不到！ 方式五、&lt;object&gt;标签12345&lt;object src="previewUrl" width="100%" height="100%"&gt; This browser does not support PDFs. Please download the PDF to view it: &lt;a href="previewUrl"&gt;Download PDF&lt;/a&gt;&lt;/object&gt; 除方式二以外其他都是直接通过标签将内容引入页面实现预览 方式六、PDFObjectPDFObject实际上也是通过&lt;embed&gt;标签实现的直接上代码123456789101112131415161718192021222324252627&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;title&gt;Show PDF&lt;/title&gt; &lt;meta charset="utf-8" /&gt; &lt;script type="text/javascript" src='pdfobject.min.js'&gt;&lt;/script&gt; &lt;style type="text/css"&gt; html,body,#pdf_viewer&#123; width: 100%; height: 100%; margin: 0; padding: 0; &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt; &lt;div id="pdf_viewer"&gt;&lt;/div&gt;&lt;/body&gt;&lt;script type="text/javascript"&gt; if(PDFObject.supportsPDFs)&#123; // PDF嵌入到网页 PDFObject.embed("index.pdf", "#pdf_viewer" ); &#125; else &#123; location.href = "/canvas"; &#125;&lt;/script&gt;&lt;/html&gt; 还可以通过以下代码进行判断是否支持PDFObject预览12345if(PDFObject.supportsPDFs)&#123; console.log("Yay, this browser supports inline PDFs.");&#125; else &#123; console.log("Boo, inline PDFs are not supported by this browser");&#125; 方式七、PDF.jsPDF.js可以实现在html下直接浏览pdf文档，是一款开源的pdf文档读取解析插件，非常强大，能将PDF文件渲染成Canvas。PDF.js主要包含两个库文件，一个pdf.js和一个pdf.worker.js，一个负责API解析，一个负责核心解析。 word、xls、ppt文件在线预览功能word、ppt、xls文件实现在线预览的方式比较简单可以直接通过调用微软的在线预览功能实现 (预览前提：资源必须是公共可访问的)12&lt;iframe src='https://view.officeapps.live.com/op/view.aspx?src=http://storage.xuetangx.com/public_assets/xuetangx/PDF/1.xls' width='100%' height='100%' frameborder='1'&gt;&lt;/iframe src就是要实现预览的文件地址 具体文档看这微软接口文档 word文件XDOC可以实现预览以DataURI表示的DOC文档，此外XDOC还可以实现文本、带参数文本、html文本、json文本、公文等在线预览，具体实现方法请看官方文档 下面这种方式可以实现快速预览word但是对文件使用的编辑器可能会有一些限制1&lt;a href="http://www.xdocin.com/xdoc?_func=to&amp;amp;_format=html&amp;amp;_cache=1&amp;amp;_xdoc=http://www.xdocin.com/demo/demo.docx" target="_blank" rel="nofollow"&gt;XDOC&lt;/a&gt;]]></content>
      <categories>
        <category>前端</category>
      </categories>
      <tags>
        <tag>HTML</tag>
        <tag>js</tag>
        <tag>office</tag>
        <tag>pdf</tag>
        <tag>word</tag>
        <tag>xls</tag>
        <tag>ppt</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo-Next搭建个人博客（主题内加入动态背景）]]></title>
    <url>%2F2018%2F08%2F08%2FHexo-Next%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%EF%BC%88%E4%B8%BB%E9%A2%98%E5%86%85%E5%8A%A0%E5%85%A5%E5%8A%A8%E6%80%81%E8%83%8C%E6%99%AF%EF%BC%89%2F</url>
    <content type="text"><![CDATA[添加静态背景 打开博客根目录/themes/next/source/css/_custom/custom.styl文件，编辑如下： 1234567// Custom styles.body &#123; background-image: url(/images/background.png); background-attachment: fixed; // 不随屏幕滚动而滚动 background-repeat: no-repeat; // 如果背景图不够屏幕大小则重复铺，改为no-repeat则表示不重复铺 //background-size: contain; // 等比例铺满屏幕&#125; 将背景图命名为background.png并放入主题根目录/images下 添加动态背景_layout.swig找到themes\next\layout\_layout.swig文件，添加内容：在&lt;body&gt;里添加： 123&lt;div class="bg_content"&gt; &lt;canvas id="canvas"&gt;&lt;/canvas&gt;&lt;/div&gt; 仍是该文件，在末尾添加： 1&lt;script type="text/javascript" src="/js/src/dynamic_bg.js"&gt;&lt;/script&gt; dynamic_bg.js在themes\next\source\js\src中新建文件dynamic_bg.js，代码链接中可见：dynamic_bg.js custom.styl在themes\next\source\css\_custom\custom.styl文件末尾添加内容：1234567.bg_content &#123; position: fixed; top: 0; z-index: -1; width: 100%; height: 100%;&#125;]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>Next</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo-Next搭建个人博客（添加网页音乐播放器功能）]]></title>
    <url>%2F2018%2F08%2F08%2FHexo-Next%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%EF%BC%88%E6%B7%BB%E5%8A%A0%E7%BD%91%E9%A1%B5%E9%9F%B3%E4%B9%90%E6%92%AD%E6%94%BE%E5%99%A8%E5%8A%9F%E8%83%BD%EF%BC%89%2F</url>
    <content type="text"><![CDATA[效果图： download点击访问Aplayer源码：GitHub Aplayer。下载到本地，解压后将dist文件夹复制到themes\next\source文件夹下。 music.js新建themes\next\source\dist\music.js文件，添加内容： 12345678910111213141516171819202122232425262728293031const ap = new APlayer(&#123; container: document.getElementById('aplayer'), fixed: true, autoplay: false, audio: [ &#123; name: "canon in d", artist: 'Brian Crain', url: 'http://pd2tflnys.bkt.clouddn.com/Brian%20Crain%20-%20canon%20in%20d.mp3', cover: 'http://p1.music.126.net/QGb9Vtyw7qHS00uEvPfM6g==/843325418547559.jpg?param=130y130', &#125;, &#123; name: "Apologize", artist: 'Martin Ermen', url: 'http://pd2tflnys.bkt.clouddn.com/Martin%20Ermen%20-%20Apologize.mp3', cover: 'http://p1.music.126.net/-_6mcI4VV5IKaiwhUAytbg==/1791104441647901.jpg?param=130y130', &#125;, &#123; name: "River Flows in You", artist: 'Yiruma', url: 'http://pd2tflnys.bkt.clouddn.com/Yiruma%20-%20River%20Flows%20in%20You.flac', cover: 'http://p1.music.126.net/8ZRSyI0ZN_4ah8uzsNd1mA==/2324367581169008.jpg?param=130y130', &#125;, &#123; name: '惊蛰', artist: '音阙诗听/王梓钰', url: 'http://www.ytmp3.cn/down/48755.mp3', cover: 'http://p1.music.126.net/5MmXpaP9r88tNzExPGMI8Q==/109951163370350985.jpg?param=130y130', &#125; ]&#125;); 源码中对应的参数解释，这边都有： Aplayer 中文文档 audio对应的便是音频文件，所以音乐播放器需要播放的音乐是需要自己进行相关信息（如歌曲链接、歌词、封面等）的配置。这里放一个mp3音乐外链网站：http://up.mcyt.net/ ，搜索对应的音乐，然后复制url和右击封面图片链接粘贴到对应的位置上就行了。 注：由于该外链网站没有歌词链接，我这边没有进行配置，所以播放器在播放音乐时点击歌词是没有显示的。 _layout.swig打开themes\next\layout\_layout.swig文件，将1234&lt;link rel="stylesheet" href="/dist/APlayer.min.css"&gt;&lt;div id="aplayer"&gt;&lt;/div&gt;&lt;script type="text/javascript" src="/dist/APlayer.min.js"&gt;&lt;/script&gt;&lt;script type="text/javascript" src="/dist/music.js"&gt;&lt;/script&gt; 添加到&lt;body itemscope ...&gt;后面就行，即在&lt;body&gt;&lt;/body&gt;里面。 重新生成，访问页面，就能看到左下角的音乐播放器了。]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>Next</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo-Next搭建个人博客（添加统计访客量以及文章阅读量）]]></title>
    <url>%2F2018%2F08%2F08%2FHexo-Next%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%EF%BC%88%E6%B7%BB%E5%8A%A0%E7%BB%9F%E8%AE%A1%E8%AE%BF%E5%AE%A2%E9%87%8F%E4%BB%A5%E5%8F%8A%E6%96%87%E7%AB%A0%E9%98%85%E8%AF%BB%E9%87%8F%EF%BC%89%2F</url>
    <content type="text"><![CDATA[不蒜子统计功能NexT主题集成了不蒜子统计功能,以下为我的配置1234567891011121314151617# Show PV/UV of the website/page with busuanzi.# Get more information on http://ibruce.info/2015/04/04/busuanzi/busuanzi_count: # count values only if the other configs are false enable: true # custom uv span for the whole site site_uv: true site_uv_header: &lt;i class="fa fa-user"&gt;&lt;/i&gt; site_uv_footer: 人次 # custom pv span for the whole site site_pv: true site_pv_header: &lt;i class="fa fa-eye"&gt;&lt;/i&gt; site_pv_footer: 次 # custom pv span for one page only page_pv: true page_pv_header: &lt;i class="fa fa-file-o"&gt;&lt;/i&gt; page_pv_footer: 次 当enable: true时，代表开启全局开关。若site_uv、site_pv、page_pv的值均为false时，不蒜子仅作记录而不会在页面上显示。当site_uv: true时，代表在页面底部显示站点的UV值。当site_pv: true时，代表在页面底部显示站点的PV值。当page_pv: true时，代表在文章页面的标题下显示该页面的PV值（阅读数）。site_uv_header和site_uv_footer这几个为自定义样式配置，相关的值留空时将不显示，可以使用（带特效的）font-awesome。 [2018/9/19] 更新 Next主题已经更新至6.X版本,不蒜子统计插件配置有变化 文件：主题配置文件_config.yml12345678910# Show Views/Visitors of the website/page with busuanzi.# Get more information on http://ibruce.info/2015/04/04/busuanzi/busuanzi_count: enable: true total_visitors: true total_visitors_icon: user total_views: true total_views_icon: eye post_views: false post_views_icon: eye 效果图： LeanCloud阅读次数统计[2018/9/19] 更新 安装1$ npm install hexo-symbols-count-time --save 文件：站点配置文件_config.yml12345symbols_count_time: symbols: true time: true total_symbols: true total_time: true 文件：主题配置文件_config.yml123456symbols_count_time: separated_meta: true item_text_post: true item_text_total: true awl: 4 # 平均单词长度（单词的计数）。默认值:4。CN≈2 EN≈5 俄文≈6 wpm: 275 # 每分钟的单词。默认值:275。缓慢≈200 正常≈275 快≈350 更多请点击这里next升级6.X后，页面LeanCloud访问统计提示Counter not initialized! See more at console err msg.的问题，请查看《Leancloud访客统计插件重大安全漏洞修复指南》 首先一句话介绍Lean Cloud: LeanCloud（aka. AVOS Cloud）提供一站式后端云服务，从数据存储、实时聊天、消息推送到移动统计，涵盖应用开发的多方面后端需求。 相比不蒜子的统计，LeanCloud的文章阅读量统计更加稳定靠谱，所以本人也把网站的文章内统计改为LeanCloud的了。 配置LeanCloud在注册完成LeanCloud帐号并验证邮箱之后，我们就可以登录我们的LeanCloud帐号，进行一番配置之后拿到AppID以及AppKey这两个参数即可正常使用文章阅读量统计的功能了。 创建应用 我们新建一个应用来专门进行博客的访问统计的数据操作。首先，打开控制台，如下图所示： 在出现的界面点击创建应用： 在接下来的页面，新建的应用名称我们可以随意输入，即便是输入的不满意我们后续也是可以更改的: 这里为了演示的方便，我新创建一个取名为test的应用。创建完成之后我们点击新创建的应用的名字来进行该应用的参数配置： 在应用的数据配置界面，左侧下划线开头的都是系统预定义好的表，为了便于区分我们新建一张表来保存我们的数据。点击左侧右上角的齿轮图标，新建Class：在弹出的选项中选择创建Class来新建Class用来专门保存我们博客的文章访问量等数据:点击创建Class之后，理论上来说名字可以随意取名，只要你交互代码做相应的更改即可，但是为了保证我们前面对NexT主题的修改兼容，此处的新建Class名字必须为Counter: 由于LeanCloud升级了默认的ACL权限，如果你想避免后续因为权限的问题导致次数统计显示不正常，建议在此处选择无限制。 创建完成之后，左侧数据栏应该会多出一栏名为Counter的栏目，这个时候我们点击顶部的设置，切换到test应用的操作界面:在弹出的界面中，选择左侧的应用Key选项，即可发现我们创建应用的AppID以及AppKey，有了它，我们就有权限能够通过主题中配置好的Javascript代码与这个应用的Counter表进行数据存取操作了: 复制AppID以及AppKey并在NexT主题的_config.yml文件中我们相应的位置填入即可，正确配置之后文件内容像这个样子: 1234leancloud_visitors: enable: true app_id: joaeuuc4hsqudUUwx4gIvGF6-gzGzoHsz app_key: E9UJsJpw1omCHuS22PdSpKoh 这个时候重新生成部署Hexo博客，应该就可以正常使用文章阅读量统计的功能了。需要特别说明的是：记录文章访问量的唯一标识符是文章的发布日期以及文章的标题，因此请确保这两个数值组合的唯一性，如果你更改了这两个数值，会造成文章阅读数值的清零重计。 后台管理当你配置部分完成之后，初始的文章统计量显示为0，但是这个时候我们LeanCloud对应的应用的Counter表中并没有相应的记录，只是单纯的显示为0而已，当博客文章在配置好阅读量统计服务之后第一次打开时，便会自动向服务器发送数据来创建一条数据，该数据会被记录在对应的应用的Counter表中。 我们可以修改其中的time字段的数值来达到修改某一篇文章的访问量的目的（博客文章访问量快递提升人气的装逼利器）。双击具体的数值，修改之后回车即可保存。 url字段被当作唯一ID来使用，因此如果你不知道带来的后果的话请不要修改。 title字段显示的是博客文章的标题，用于后台管理的时候区分文章之用，没有什么实际作用。 其他字段皆为自动生成，具体作用请查阅LeanCloud官方文档，如果你不知道有什么作用请不要随意修改。 Web安全因为AppID以及AppKey是暴露在外的，因此如果一些别用用心之人知道了之后用于其它目的是得不偿失的，为了确保只用于我们自己的博客，建议开启Web安全选项，这样就只能通过我们自己的域名才有权访问后台的数据了，可以进一步提升安全性。 选择应用的设置的安全中心选项卡: 在Web 安全域名中填入我们自己的博客域名，来确保数据调用的安全: 如果你不知道怎么填写安全域名而或者填写完成之后发现博客文章访问量显示不正常，打开浏览器调试模式，发现如下图的输出: 这说明你的安全域名填写错误，导致服务器拒绝了数据交互的请求，你可以更改为正确的安全域名或者你不知道如何修改请在本博文中留言或者放弃设置Web安全域名。]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>Next</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo-Next搭建个人博客（集成DaoVoice在线联系功能）]]></title>
    <url>%2F2018%2F08%2F08%2FHexo-Next%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%EF%BC%88%E9%9B%86%E6%88%90DaoVoice%E5%9C%A8%E7%BA%BF%E8%81%94%E7%B3%BB%E5%8A%9F%E8%83%BD%EF%BC%89%2F</url>
    <content type="text"><![CDATA[注册首先在DaoVoice注册个账号，点击-&gt;邀请码是b3c7d22e。 完成后，会得到一个app_id，后面会用到： 修改head.swig修改/themes/next/layout/_partials/head.swig文件，添加内容如下： 123456789&#123;% if theme.daovoice %&#125; &lt;script&gt; (function(i,s,o,g,r,a,m)&#123;i["DaoVoiceObject"]=r;i[r]=i[r]||function()&#123;(i[r].q=i[r].q||[]).push(arguments)&#125;,i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;a.charset="utf-8";m.parentNode.insertBefore(a,m)&#125;)(window,document,"script",('https:' == document.location.protocol ? 'https:' : 'http:') + "//widget.daovoice.io/widget/0f81ff2f.js","daovoice") daovoice('init', &#123; app_id: "&#123;&#123;theme.daovoice_app_id&#125;&#125;" &#125;); daovoice('update'); &lt;/script&gt;&#123;% endif %&#125; 位置贴图： 主题配置文件在_config.yml文件中添加内容： 123# Online contact daovoice: truedaovoice_app_id: 这里输入前面获取的app_id 聊天窗口配置附上我的聊天窗口的颜色、位置等设置信息： 至此，网页的在线联系功能已经完成，重新hexo d -g上传GitHub后，页面上就能看到效果了。 效果图： 可以关注小程序接收回复消息，很方便 现在往右下角看看(～￣▽￣)～ ，欢迎撩我（滑稽）。]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>Next</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo-Next搭建个人博客（集成LiveRe来必力文章评论功能）]]></title>
    <url>%2F2018%2F08%2F08%2FHexo-Next%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%EF%BC%88%E9%9B%86%E6%88%90LiveRe%E6%9D%A5%E5%BF%85%E5%8A%9B%E6%96%87%E7%AB%A0%E8%AF%84%E8%AE%BA%E5%8A%9F%E8%83%BD%EF%BC%89%2F</url>
    <content type="text"><![CDATA[准备 去Livere官网注册Livere账号。 选择City版（免费），安装 进入管理页面-&gt;代码管理-&gt;一般网站，复制data-uid 在Hexo中添加Livere以下基于主题Next，其他主题做法类似 打开博客根目录/themes/next/_config.yml 将# Third Party Services Settings 栏目下其他评论系统如duoshuo、gentie、youyan、disqus用#注释掉，加入以下内容 12# Livere评论系统 livere_uid: 上一步中你获取的data-uid 在博客根目录/themes/layout/_scripts/third-party/comments/目录中新建txt文件，重命名为livere.swig，编辑内容如下： 1234567891011121314&#123;% if not (theme.duoshuo and theme.duoshuo.shortname) and not theme.duoshuo_shortname and not theme.disqus_shortname and not theme.hypercomments_id and not theme.gentie_productKey %&#125; &#123;% if theme.livere_uid %&#125; &lt;script type="text/javascript"&gt; (function(d, s) &#123; var j, e = d.getElementsByTagName(s)[0]; if (typeof LivereTower === 'function') &#123; return; &#125; j = d.createElement(s); j.src = 'https://cdn-city.livere.com/js/embed.dist.js'; j.async = true; e.parentNode.insertBefore(j, e); &#125;)(document, 'script'); &lt;/script&gt; &#123;% endif %&#125;&#123;% endif %&#125; 在博客根目录/themes/layout/_scripts/third-party/comments.swig文件中追加： 1&#123;% include './comments/livere.swig' %&#125; 在博客根目录/themes/layout/_partials/comments.swig文件中条件最后追加 LiveRe 插件是否引用的判断逻辑： 123&#123;% elseif theme.livere_uid %&#125; &lt;div id="lv-container" data-id="city" data-uid="&#123;&#123; theme.livere_uid &#125;&#125;"&gt;&lt;/div&gt;&#123;% endif %&#125; 重新hexo clean、hexo d -g，然后就可以看到来必力评论啦~ ps:因为是国外的，评论加载有点慢。 效果图]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>Next</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo-Next搭建个人博客（集成Algolia站内搜索引擎）]]></title>
    <url>%2F2018%2F08%2F08%2FHexo-Next%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%EF%BC%88%E9%9B%86%E6%88%90Algolia%E7%AB%99%E5%86%85%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%EF%BC%89%2F</url>
    <content type="text"><![CDATA[起因Swiftype现在收费了，也没有免费版本。Local Search搜索体验不好，微搜索Next官网上描述太少！所以选择Algolia。 注：Algolia搜索在版本 5.1.0 中引入，要使用此功能请确保所使用的 NexT 版本在此之后 首先注册Algolia账户Algolia 登陆页面https://www.algolia.com/users/sign_in ，可以使用 GitHub 或者 Google 账户直接登录，也可以注册一个新账户。我直接用谷歌账户登陆了，注册后的 14 天内拥有所有功能（包括收费类别的）。之后若未续费会自动降级为免费账户，免费账户 总共有 10,000 条记录，每月有 100,000 的可以操作数。 注册完成后，创建一个新的 Index，这个 index name 之后会用到 Index 创建完成后，此时这个 Index 里未包含任何数据。 接下来需要安装 Hexo Algolia 扩展， 这个扩展的功能是搜集站点的内容并通过 API 发送给 Algolia。前往站点根目录，执行命令安装：1npm install hexo-algolia --save # 目前最新版本是1.2.5，下面的操作都是基于这个版本的文档 获取 Key，更新站点根目录配置 前往站点根目录打开_config.yml添加以下代码1234567# Algolia Search API Keyalgolia: applicationID: '你的Application ID' apiKey: '你的Search-Only API Key' adminApiKey: '你的Admin API Key' indexName: '输入刚才创建index name' chunkSize: 5000 修改Algolia搜索ACL（访问控制列表） 选中后保存。 操作完成后去你的博客跟路径执行命令1set HEXO_ALGOLIA_INDEXING_KEY=你的Search-Only API Key 查看是否设置成功如果没有值就设置失败12hexo cleanhexo algolia 成功后修改Next主题配置文件更改Next主题配置文件，找到 Algolia Search 配置部分：123456789# Algolia Searchalgolia_search: enable: true hits: per_page: 10 labels: input_placeholder: 输入关键字 hits_empty: "没有找到与 $&#123;query&#125; 相关的内容" hits_stats: "$&#123;hits&#125;条相关记录，共耗时$&#123;time&#125; ms" 将 enable 改为 true 即可，根据需要你可以调整 labels 中的文本。这个是我修改的文本。 最终效果 总结一下集成遇到的BUGPlease set an HEXO_ALGOLIA_INDEXING_KEY environment variable to enable content indexing. 原因：Algolia Search API Key indexName 错了 解决方案：看下之前新建index的名字 Not enough rights to update an object near line:1 column:1635 原因：没有修改Algolia搜索ACL（访问控制列表） 解决方案： 按步骤3勾选上就可以 参考 《hexo+next添加algolia搜索》 帮助文档 《hexo-algolia》 更多教程可以来我yufeng.Zhou独立博客里面看到转载请注明出处http://yfzhou.coding.me/]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>Next</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo-Next搭建个人博客（托管到Github和Coding）]]></title>
    <url>%2F2018%2F08%2F07%2FHexo-Next%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%EF%BC%88%E6%89%98%E7%AE%A1%E5%88%B0Github%E5%92%8CCoding%EF%BC%89%2F</url>
    <content type="text"><![CDATA[前言这是一篇很详细的独立博客搭建教程，意在帮助小白们能快速入门，拥有自己的独立博客。作者已在window平台已搭建成功，博客效果请点链接查看。 为什么用Hexo搭建独立博客？Hexo 是一个快速、简洁且高效的博客框架。Hexo 使用 Markdown（或其他渲染引擎）解析文章，在几秒内，即可利用靓丽的主题生成静态网页。 Github和Coding又是什么？ Github是国外免费的Git代码托管平台。利用Github Page服务可以免费创建一个静态网站。 Coding则是国内Git代码托管平台。国内首个Git代码托管平台GitCafe已被Coding收购。也提供page服务。 为什么用两个代码托管平台？很多人都把hexo托管到github上，因为github大家都用的比较久了。但是，你的博客主要访问者肯定还是国内的用户，国内的用户访问coding比github是要快不少的。还可以利用域名解析实现国内的走coding，海外的走github，分流网站的访问。 步骤安装GitGit是什么？Git是目前世界上最先进的分布式版本控制系统（没有之一）。了解更多，参考git教程 点击下载，然后按默认选项安装即可。 安装完成后，在开始菜单里找到“Git”-&gt;“Git Bash”，蹦出一个类似命令行窗口的东西，就说明Git安装成功！ 安装完成后，还需要最后一步设置，在命令行输入12$ git config --global user.name "Your Name"$ git config --global user.email "email@example.com" 因为Git是分布式版本控制系统，所以，每个机器都必须自报家门：你的名字和Email地址。 注意git config命令的--global参数，用了这个参数，表示你这台机器上所有的Git仓库都会使用这个配置，当然也可以对某个仓库指定不同的用户名和Email地址。 将博客托管到Github和Coding上托管到github 注册github帐号访问官网注册,你的username和邮箱十分重要，GitHub上很多通知都是通过邮箱的。比如你的主页上传并构建成功会通过邮箱通知，更重要的是，如果构建失败的话也会在邮件中说明原因。 创建项目仓库注册并登陆Github官网成功后，点击页面右上角的+，选择New repository。在Repository name中填写Github账号名.github.io点击Create repository，完成创建。 托管到coding 注册coding帐号访问官网注册并登录 创建仓库点+创建项目填写项目名称描述创建即可, 配置SHH配置shh key是让本地git项目与远程的github建立联系 检查是否已经有SSH Key，打开Git Bash，输入 1cd ~/.ssh 如果没有.ssh这个目录，则生成一个新的SSH，输入 1ssh-keygen -t rsa -C "your e-mail" 注意1: 此处的邮箱地址，你可以输入自己的邮箱地址；注意2: 此处的「-C」的是大写的「C」 接下来几步都直接按回车键,然后系统会要你输入密码12Enter passphrase (empty for no passphrase):&lt;输入加密串&gt;Enter same passphrase again:&lt;再次输入加密串&gt; 这个密码会在你提交项目时使用，如果为空的话提交项目时则不用输入。这个设置是防止别人往你的项目里提交内容。注意：输入密码的时候没有*字样的，你直接输入就可以了。 最后看到这样的界面，就成功设置ssh key了 添加 SSH Key 到 GitHub和Coding 打开Git Bash，然后输入 1cd ~/.ssh 进入到.shh文件夹中再输入ls，查看是否有id_rsa.pub文件 输入cat命令，打开id_rsa.pub文件1cat id_rsa.pub 再鼠标全选中右击复制 再配置到GitHub和Coding的SSH中进入Github官网，点击+旁边的头像，再按settings进入设置在点击New SSH key创建title输入邮箱，key里面粘贴刚才右击复制的内容,再点Add SSH key同样进入coding,点击账户，在点SSH公钥设置即可 测试SSH是否配置成功 打开Git Bash，然后输入 1ssh -T git@github.com 如配置了密码则要输入密码,输完按回车如果显示以下内容，则说明Github中的ssh配置成功。 12Hi username! You've successfully authenticated, but GitHub does notprovide shell access. 再输入 1ssh -T git@git.coding.net 如果显示以下则说明coding中的ssh配置成功1Hello username You've connected to Coding.net by SSH successfully! 创建Github Pages和Coding Pages 服务 GitHub Pages分两种，一种是你的GitHub用户名建立的username.github.io这样的用户&amp;组织页（站），另一种是依附项目的pages。想建立个人博客是用的第一种，形如cnfeat.github.io这样的可访问的站，每个用户名下面只能建立一个。更多 官网点击代码再点击Coding Pages 服务开启。分支和github分支写一样，填写master 将博客网站上传到GitHub和Coding中 打开D:\blog文件夹中的_config.yml文件，找到如下位置，填写 1234567# Deployment## Docs: https://hexo.io/docs/deployment.htmldeploy: type: git repo: github: git@github.com:yourname/yourname.github.io.git,master coding: git@git.coding.net:yourname/yourname.git,master 注： (1) 其中yourname替换成你的Github账户名;(2)注意在yml文件中，:后面都是要带空格的。 在blog文件夹中空白处右击打开Git Bash输入123npm install hexo-deployer-git --savehexo g #生成静态网页hexo d #开始部署 此时，通过访问http://yourname.github.io和http://yourname.coding.me可以看到默认的Hexo首页面（与之前本地测试时一样）。 更换Hexo主题本网站使用的是Next主题。该主题简洁易用，在移动端也表现不错。 下载主题在blog文件夹中空白处右击打开Git Bash输入 1git clone https://github.com/iissnan/hexo-theme-next themes/next 修改网站的主题为Next打开D:\blog下的_config.yml文件，找到theme字段，将其修改为next 1234# Extensions## Plugins: http://hexo.io/plugins/## Themes: http://hexo.io/themes/theme: next 验证主题是否可用输入 1hexo s #启动服务，调试用 再在浏览器输入http://localhost:4000/确认网站主题是否切换为Next. 博客blog根目录下的_config.yml配置网站信息_config.yml配置请点参考 注册及绑定自己的域名地址域名注册推荐选择国内的万网或者国外的Goddady进行域名的注册，注册完还需改下绑定DNS服务商 域名解析如果你选择的是万网注册的域名，可以使用其自带的域名解析服务。也可以选择免费的DNSPod 域名解析填写如下图 打开blog文件夹下的source文件夹，新建CNAME文件,内容填写自己的域名CNAME文件设置的目的是，通过访问 yourname.github.io 可以跳转到你所注册的域名上。github是直接项目里面加CNAME文件。coding是直接在项目主页设置的，去coding项目主页添加CNAME，绑定域名。 总结只要按照上面步骤一步步设置，相信你也可以拥有自己的独立博客。希望此文对还在搭建hexo独立博客的小伙伴有所帮助。主题相关配置查看下面的，hexo和next帮助文档。 参考 《Hexo+Github: 搭建属于自己的静态博客》 《hexo你的博客》 《如何使用10个小时搭建出个人域名而又Geek的独立博客？》 《将hexo博客同时托管到github和coding》 《个人域名如何同时绑定 github 和 coding 上的博客》 《如何搭建一个独立博客——简明Github Pages与Hexo教程》 《「搭建Hexo博客」简明教程》 《使用 github Pages 服务建立 ixirong.com 独立博客全过程》 深山老猿 帮助文档 《Hexo文档》 《Next使用文档》 《Git教程》 《Github帮助文档》 《Coding帮助文档》 《Markdown 语法说明》 更多教程可以来我yufeng.Zhou独立博客里面看到转载请注明出处http://yfzhou.coding.me/]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>Next</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo-Next搭建个人博客（安装与部署）]]></title>
    <url>%2F2018%2F08%2F07%2FHexo-Next%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%EF%BC%88%E5%AE%89%E8%A3%85%E4%B8%8E%E9%83%A8%E7%BD%B2%EF%BC%89%2F</url>
    <content type="text"><![CDATA[简介Hexo 是一个快速、简洁且高效的静态站点生成框架，它基于 Node.js 。 它有以下特点： 超快速度Node.js 所带来的超快生成速度，让上百个页面在几秒内瞬间完成渲染。 支持Markdown Hexo 支持 GitHub Flavored Markdown 的所有功能，甚至可以整合 Octopress 的大多数插件。 一键部署 只需一条指令即可部署到Github Pages，或其他网站 丰富的插件 Hexo 拥有强大的插件系统，安装插件可以让 Hexo 支持 Jade, CoffeeScript。 通过 Hexo 你可以轻松地使用 Markdown 编写文章，除了 Markdown 本身的语法之外，还可以使用 Hexo 提供的 标签插件 来快速的插入特定形式的内容。 基于 Hexo 这个优秀的博客框架，很多优秀的开发者奉献出了它们基于 Hexo 开发的主题。NexT 因其 精于心，简于形 的风格，一直被广大用户所喜爱。 安装前提安装 Hexo 相当简单。然而在安装前，您必须检查电脑中是否已安装下列应用程序: Node.jsGit 如果您的电脑中已经安装上述必备程序，那么恭喜你！接下来只需要使用 npm 即可完成 Hexo 的安装。 1$ npm install -g hexo-cli 如果你的电脑中尚未安装所需要的程序，请根据以下安装指示完成安装。 Mac 用户您在编译时可能会遇到问题，请先到 App Store 安装 Xcode，Xcode 完成后，启动并进入 Preferences -&gt; Download -&gt; Command Line Tools -&gt; Install 安装命令行工具。 安装 Git Windows：下载安装 git 。 Download Now Mac：使用 Homebrew，MacPorts 或下载 安装程序 安装 Linux（Ubuntu，Debian）：sudo apt-get install git-core Linux（Fedora，Red Hat，CentOS）：sudo yum install git-core Windows 用户由于众所周知的原因，从上面的链接下载git for windows最好挂上一个代理，否则下载速度十分缓慢。也可以参考这个页面，收录了存储于百度云的下载地址。 安装 Node.js安装 Node.js 的最佳方式是使用 nvm。（nvm：Node Version Manager）windows 下使用 nvm 请看这里： nvm-windows ，首先需要下载安装 nvm 。 Download Nowwindows下安装完nvm以后，我们可以打开命令行中执行命令12$ nvm $ nvm install latest 执行完以后，重启命令行，执行命令 node -v ，如果出现版本号，那么 Node.js 就安装成功了。 如果没有安装成功，那可能就是墙的原因。建议下载 Node.js 直接安装。 Download Now 安装 Hexo有了 Node.js ，我们可以使用 npm 安装 Hexo。1$ npm install -g hexo-cli 安装 Hexo 完成后，我们首先需要为我们的项目创建一个指定文件夹（例如我在 D 盘目录下创建了一个文件夹 blog 。D:\blog ），在指定文件夹中执行下列命令， Hexo 将会在指定文件夹中新建所需要的文件。1$ hexo init 等待安装，安装完成后，指定文件夹 的目录如下：12345678. ├── _config.yml ├── package.json ├── scaffolds ├── source | ├── _drafts | └── _posts └── 我们继续执行命令 12$ hexo g $ hexo s --debug Hexo 将 source 文件夹中除 posts 文件夹之外，开头命名为 (下划线)的文件 / 文件夹和隐藏的文件将会被忽略。Markdown 和 HTML 文件会被解析并放到 public 文件夹，而其他文件夹会被拷贝过去。这个时候，我们在浏览器中访问 http://localhost:4000/ ，就可以看到基于 Hexo 的默认主题的原型 安装 NexT 主题下载 NexT 主题依旧是在当前目录下，使用 Git checkout 代码： 1$ git clone https://github.com/iissnan/hexo-theme-next themes/next 等待下载完成。 在 Hexo 中有两份主要的配置文件，其名称都是 _config.yml 。其中，一份位于站点根目录下，主要包含 Hexo 本身的配置；另一份位于主题目录下，这份配置由主题作者提供，主要用于配置主题相关的选项。我们约定，将前者称为 站点配置文件，后者称为 主题配置文件 启用 NexT 主题打开 站点配置文件文件 ，找到 theme 字段，并将其值更改为 next 。到此， NexT 主题安装完成。下一步我们将验证主题是否正确启用。在切换主题之后、验证之前，我们最好使用 hexo clean 来清除 Hexo 的缓存。 验证主题首先启动 Hexo 本地站点，并开启调试模式（即加上 --debug），整个命令是 hexo s --debug。在服务启动的过程，注意观察命令行输出是否有任何异常信息。当命令行输出中提示： INFO Hexo is running at http://0.0.0.0:4000/. Press Ctrl+C to stop. 此时即可使用浏览器访问 http://localhost:4000/ ，检查站点是否正确运行。 当你看到站点的外观与下图所示类似时即说明你已成功安装 NexT 主题。这是 NexT 默认的 Scheme —— Muse 现在，我们已经成功安装并启用了 NexT 主题。 关于更多基本操作和基础知识，请查阅 Hexo 与 NexT 官方文档. 总结本地调试步骤三部曲： $ hexo clean$ hexo g$ hexo s –debug 这种带 debug 的运行，如果出现错误，可以在命令行中看到错误提示信息。 部署步骤三部曲： $ hexo clean$ hexo g$ hexo d 当然在部署之前，需要先配置好配置文件中的 deploy。 常用命令$ hexo new “postName” #新建文章$ hexo new page “pageName” # 新建页面$ hexo generate # 生成静态页面至public目录$ hexo server # 开启预览访问端口(默认端口4000，’ctrl+c’关闭server)$ hexo deploy # 项目部署$ hexo help # 查看帮助$ hexo version # 查看Hexo的版本 简写命令$ hexo new == hexo n$ hexo generate == hexo g$ hexo server == hexo s$ hexo deploy == hexo d$ hexo generate + $ hexo server == $ hexo s -g$ hexo generate + $ hexo deploy == $ hexo d -g 常见问题1在 hexo 的配置和设置文件中，在冒号后面没留空格会导致出问题：错误的设置：123author:yufeng.Zhouemail:18851200889@163.comlanguage:zh-CN 正确的设置：123author: yufeng.Zhouemail: 18851200889@163.comlanguage: zh-CN 常见问题2关于 Git 提交中用户名和 Email 的设置12git config --global user.name "Your name" git config --global user.email "Your email" 常见问题3Hexo 中的图标使用的是 Font Awesome ，所以，我们的博客已经自带了 Font Awesome 中的所有图标，基本可以满足我们的所有需求，我们可以去 Font Awesome 中查找我们想要使用的图标。&lt;i class=&quot;fa fa-github&quot;&gt;&lt;/i&gt;&lt;i class=&quot;fa fa-github fa-lg&quot;&gt;&lt;/i&gt;&lt;i class=&quot;fa fa-github fa-2x&quot;&gt;&lt;/i&gt;]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>Next</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[FreeMarker笔记]]></title>
    <url>%2F2018%2F08%2F01%2FFreeMarker%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[FreeMarker是一个很值得去学习的模版引擎。它是基于模板文件生成其他文本的通用工具。本章内容通过如何使用FreeMarker生成Html web 页面 和 代码自动生成工具来快速了解FreeMarker。简介FreeMarker是一款用java语言编写的模版引擎，它虽然不是web应用框架，但它很合适作为web应用框架的一个组件。 特点 ==轻量级==模版引擎，不需要Servlet环境就可以很轻松的嵌入到应用程序中 能生成各种文本，如html，xml，java，等 入门==简单==，它是用java编写的，很多语法和java相似 FreeMarker 程序这里通过模拟简单的代码自动生产工具来感受第一个FreeMarker程序。 项目目录结构 eclipse安装freemarker插件Help –&gt; Install New Software点add，再出来的对话框中的Location中输入：http://download.jboss.org/jbosstools/updates/stable/indigo/name随便取一个即可。然后会列出来所有可用的插件，JBoss Application Development 下找到 FreeMarker IDE选中 点击Next安装好重启eclipse就可以了。过程可能有点慢，请==耐心等待==。 maven依赖12345&lt;dependency&gt; &lt;groupId&gt;org.freemarker&lt;/groupId&gt; &lt;artifactId&gt;freemarker&lt;/artifactId&gt; &lt;version&gt;2.3.23&lt;/version&gt;&lt;/dependency&gt; hello.ftl模板(部分) FreeMarkerDemo.java 核心方法，使用 FreeMarker 模版引擎。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788package com.freemarker.hello.templates;import java.io.BufferedWriter;import java.io.File;import java.io.FileOutputStream;import java.io.OutputStreamWriter;import java.io.Writer;import java.util.ArrayList;import java.util.Date;import java.util.HashMap;import java.util.List;import java.util.Map;import com.freemarker.hello.pojo.User;import freemarker.template.Configuration;import freemarker.template.Template;/** * 最常见的问题： java.io.FileNotFoundException: xxx does not exist. 解决方法：要有耐心 * FreeMarker jar 最新的版本（2.3.23）提示 Configuration 方法被弃用 代码自动生产基本原理： 数据填充 * freeMarker 占位符 */public class FreeMarkerDemo &#123; private static final String TEMPLATE_PATH = "src/main/java/com/freemarker/hello/templates"; private static final String CLASS_PATH = "src/main/java/com/freemarker/hello"; private static List&lt;User&gt; users = new ArrayList&lt;User&gt;(); static &#123; User u1 = new User("1", 22, "迟到峰"); User u2 = new User("2", 23, "要饭楚"); User u3 = new User("3", 27, "BUG李"); User u4 = new User("4", 25, "删库冬"); User u5 = new User("5", 29, "瓜子军"); User u6 = new User("6", 28, "老韩"); User u7 = new User(null, 25, null); users.add(u1); users.add(u2); users.add(u3); users.add(u4); users.add(u5);// users.add(null); users.add(u6);// users.add(u7);// users.clear(); &#125; public static void main(String[] args) &#123; // step1 创建freeMarker配置实例 Configuration configuration = new Configuration(Configuration.VERSION_2_3_23); Writer out = null; try &#123; // step2 获取模版路径 configuration.setDirectoryForTemplateLoading(new File(TEMPLATE_PATH)); // step3 创建数据模型 Map&lt;String, Object&gt; dataMap = new HashMap&lt;String, Object&gt;(); dataMap.put("classPath", "com.freemarker.hello"); dataMap.put("htmlName", "使用FreeMarker生成html模板"); dataMap.put("helloWorld", "通过简单的 &lt;代码自动生产程序&gt; 演示 FreeMarker的HelloWorld！"); dataMap.put("author", "周宇峰"); dataMap.put("github", "github.com/542869246"); dataMap.put("name", "abcdefg"); dataMap.put("dateTime",new Date()); dataMap.put("users", users); // step4 加载模版文件 Template template = configuration.getTemplate("hello.ftl"); // step5 生成数据 File docFile = new File(CLASS_PATH + "\\" + "hello.html"); out = new BufferedWriter(new OutputStreamWriter(new FileOutputStream(docFile))); // step6 输出文件 template.process(dataMap, out); System.out.println("文件创建成功 !"); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; try &#123; if (null != out) &#123; out.flush(); &#125; &#125; catch (Exception e2) &#123; e2.printStackTrace(); &#125; &#125; &#125;&#125; 运行程序后刷新项目，会发现多了一个hello.html文件。 语法详解数据类型和java不同，FreeMarker不需要定义变量的类型，直接赋值即可。字符串： value = “xxxx” 。 单引号和双引号是一样的。字符串中可以使用转义字符”\”。如果字符串内有大量的特殊字符，则可以在引号的前面加上一个字母r，则字符串内的所有字符都将直接输出。string = r”xxxx”。 数值：value = 1.2。数值可以直接等于，但是不能用科学计数法。 布尔值：true or false。 List集合：list = [1,2,3] ; list=[1..100] 表示 1 到 100 的集合，反之亦然。 Map集合：map = {“key” : “value” , “key2” : “value2”}，key 必须是字符串 时间对象: 12root.put("date1",new Date());$&#123;date1?string("yyyy-MM-dd HH:mm:ss")&#125; JaveBean：Freemarker中对于javabean的处理跟EL表达式一致，类型可自动转化！非常方便！ 注释：&lt;#– abcd –&gt;字符串操作声明变量和输出:1234&lt;#assign name="zyf"&gt; //声明一个变量值为zyf的变量name$&#123;name&#125; //输出name 结果为zyf&lt;#assign cname=r"特殊字符完成输出(https://github.com/542869246)"&gt;$&#123;cname&#125; 字符串连接：123//使用嵌套或者+ 进行字符串连接操作$&#123;"Hello $&#123;name&#125; !"&#125; / $&#123;"Hello " + name + " !"&#125;//输出：Hello zyf ! / Hello zyf ! 字符串截取：12345&lt;#assign a="abcdefg"&gt;$&#123;a[1]&#125; //b$&#123;a[1..5]&#125; //bcdef$&#123;a?substring(3)&#125; //efg$&#123;a?substring(3,2)&#125; //ef string[index]。index 可以是一个值，也可以是形如 0..2 表示下标从0开始，到下标为2结束。一共是三个数。substring（start,end）从一个字符串中截取子串。start:截取子串开始的索引，start必须大于等于0，小于等于end。end: 截取子串的长度，end必须大于等于0，小于等于字符串长度，如果省略该参数，默认为字符串长度。 算数运算：支持”+”、”－”、”*”、”/“、”%”运算符 123456&lt;#assign number1=10 number2=5 &gt;"+":$&#123;number1 + number2 &#125; //15"-":$&#123;number1 - number2 &#125; //5"*":$&#123;number1 * number2 &#125; //50"/":$&#123;number1 / number2 &#125; //2"%":$&#123;number1 % number2 &#125; //0 比较运算符表达式中支持的比较运算符有如下几种： =（或者==）：判断两个值是否相等； !=：判断两个值是否不相等；注： =和!=可以用作字符串、数值和日期的比较，但两边的数据类型必须相同。而且FreeMarker的比较是精确比较，==不会忽略大小写及空格==。 >（或者gt）：大于 >=（或者gte）：大于等于 &lt;（或者lt）：小于 &lt;=（或者lte）：小于等于 12345&lt;#if number1 + number2 gte 12 || number1 - number2 lt 6&gt;"*" : $&#123;number1 * number2&#125;&lt;#else&gt;"/" : $&#123;number1 / number2&#125;&lt;/#if&gt; 上面这些比较运算符可以用于数字和日期，但不能用于字符串。大部分时候，使用==gt比&gt;有更好的效果==，因为FreeMarker会把&gt;解释成标签的结束字符。可以使用括号来避免这种情况，如：&lt;#if (x&gt;y)&gt;。 逻辑运算符 &amp;&amp;：逻辑与； ||：逻辑或； !：逻辑非逻辑运算符只能用于布尔值。 内建函数FreeMarker还提供了一些内建函数来转换输出,可以在任何变量后紧跟?,?后紧跟内建函数,就可以通过内建函数来轮换输出变量.下面是常用的内建的字符串函数 字符串相关常用的内建函数：123456789101112131415161718192021222324252627282930&lt;#assign data = "abcd1234"&gt;html：对字符串进行HTML编码，将字符串中的&lt;、&gt;、&amp;和“替换为对应得&amp;lt;&amp;gt;&amp;quot:&amp;ampcap_first：使字符串第一个字母大写 $&#123;data?cap_first&#125;lower_case：将字符串转成小写 $&#123;data?lower_case&#125;upper_case：将字符串转成大写 $&#123;data?upper_case&#125;trim:去掉字符串前后的空白字符 $&#123;data?trim&#125;length:返回字符串的长度 $&#123;"string"?length&#125;index_of(substring,start)在字符串中查找某个子串，返回找到子串的第一个字符的索引，如果没有找到子串，则返回-1。Start参数用于指定从字符串的那个索引处开始搜索，start为数字值。如果start大于字符串长度，则start取值等于字符串长度，如果start小于0， 则start取值为0。$&#123;"string"?index_of("in") 结果为3$&#123;"string"?index_of("ab") 结果为-1replace用于将字符串中的一部分从左到右替换为另外的字符串。$&#123;"strabg"?replace("ab","in")&#125; 结果为stringsplit使用指定的分隔符将一个字符串拆分为一组字符串&lt;#list "This|is|split"?split("|"") as s&gt;$&#123;s&#125;&lt;/#list&gt;结果为:Thisissplit 集合相关常用的内建函数：1size：获得集合中元素的个数 $&#123;users?size&#125; 数字值相关常用的内建函数：12&lt;#assign floatData = 12.34&gt;int：取得数字的整数部分 $&#123;floatData?int&#125; 三元运算符12$&#123;(users?size gt 15)?string('a','b') &#125;&lt;#assign theValue = (temp == "default")?string('true','false') /&gt; 空值处理运算符FreeMarker对空值的处理非常严格,==FreeMarker的变量必须有值==,没有被赋值的变量就会抛出异常,因为FreeMarker未赋值的变量强制出错可以杜绝很多潜在的错误,如缺失潜在的变量命名,或者其他变量错误.这里所说的空值,实际上也包括那些并不存在的变量,对于一个Java的 null值而言,我们认为这个变量是存在的,只是它的值为null,但对于FreeMarker模板而言,它无法理解null值,null值和不存在的变量完全相同。 FreeMarker提供两个运算符来避免空值 !：指定缺失变量的默认值 ??：判断变量是否存在 !运算符有两种用法：variable!或variable!defaultValue。第一种用法不给变量指定默认值，表明默认值是空字符串、长度为0的集合、或长度为0的Map对象。==使用!运算符指定默认值并不要求默认值的类型和变量类型相同==。 ??运算符返回布尔值，如：variable??，如果变量存在，返回true，否则返回false。 12345&lt;#if user??&gt; $&#123;user.name!"变量为空则给一个默认值"&#125;&lt;#else&gt; users为空&lt;/#if&gt; 常用指令FreeMarker的FTL指令也是模板的重要组成部分,这些指令可实现对数据模型所包含数据的抚今迭代,分支控制.除此之外,还有一些重要的功能,也是通过FTL指令来实现的. if指令这是一个典型的分支控制指令,该指令的作用完全类似于Java语言中的if,if指令的语法格式如下123456&lt;#assign age=23&gt; &lt;#if (age&gt;60)&gt;老年人 &lt;#elseif (age&gt;40)&gt;中年人 &lt;#elseif (age&gt;20)&gt;青年人 &lt;#else&gt; 少年人 &lt;/#if&gt; switch case指令switch（expr）,其中expr只能是字符串、基本类型int或者包装类Integer，也包括不同的长度整型，例如short12345678910111213&lt;#switch being.size&gt; &lt;#case "small"&gt; This will be processed if it is small &lt;#break&gt; &lt;#case "medium"&gt; This will be processed if it is medium &lt;#break&gt; &lt;#case "large"&gt; This will be processed if it is large &lt;#break&gt; &lt;#default&gt; This will be processed if it is neither &lt;/#switch&gt; list指令list指令是一个迭代输出指令,用于迭代输出数据模型中的集合sequence是集合(collection)的表达式，item是循环变量的名字，不能是表达式。当在遍历sequence时，会将遍历变量的值保存到item中 123456789//格式&lt;#list sequence as item&gt; $&#123;item&#125; &lt;/#list&gt;//遍历users集合 获取user对象属性&lt;#list users as user&gt; $&#123;user.id&#125;--$&#123;user.age&#125;--&#123;user.name&#125;&lt;/#list&gt; List指令还隐含了两个循环变量：item_index:当前迭代项在所有迭代项中的位置，是数字值。 item_has_next:用于判断当前迭代项是否是所有迭代项中的最后一项。 123456&lt;#list users as user&gt; $&#123;user_index&#125;--$&#123;user.id&#125;--$&#123;user.age&#125;--&#123;user.name&#125; &lt;#if !user_has_next&gt; 共有$&#123;users?size&#125;最后一个用户是:$&#123;user.name&#125; &lt;/#if&gt;&lt;/#list&gt; 对List进行排序通常我们的排序操作都是通过DAO层来实现的，如果我们想随时更改我们的排序，那么就必须修改我们的DAO层代码，确实不方便。但Freemarker为我们提供了这样的排序方法，解决了这个问题。 sort升序排序函数sort对序列(sequence)进行排序，要求序列中的变量必须是：字符串（按首字母排序）,数字，日期值。 1&lt;#list list?sort as l&gt;…&lt;/#list&gt; sort_by函数sort_by有一个参数,该参数用于指定想要排序的子变量，排序是按照变量对应的值进行排序,如： 1&lt;#list users?sort_by("age") as user&gt;…&lt;/#list&gt; age是User对象的属性，排序是按age的值进行的。 reverse降序排序函数 1&lt;#list list? reverse as l&gt;…&lt;/#list&gt;。 reverse使用同sort相同。reverse还可以同sort_by一起使用如：想让用户按年龄降序排序，那么可以这个样写 1&lt;#list users?sort_by("age")?reverse as user&gt;…&lt;/#list&gt; 使用list指令遍历map1234567891011121314151617181920212223//创建一个map,注意在freemarker中,map的key只能是字符串来作为key&lt;#assign userMap=&#123;"1","刘德华","2":"张学友"&#125;/&gt;//获取map中的值$&#123;userMap["1"]&#125;//获取map的keys&lt;#assign keys=userMap?keys/&gt;//遍历map 首选获取key的集合&lt;#list keys as key&gt; key:$&#123;key&#125;-value:$&#123;userMap["$&#123;key&#125;"]&#125;&lt;/#list&gt;//直接遍历map的第二种方式&lt;#list userMap?keys as key&gt; key:$&#123;key&#125;--value:$&#123;userMap["$&#123;key&#125;"]&#125;&lt;/#list&gt;//直接遍历map的values&lt;#list userMap?values as value&gt; $&#123;value&#125;&lt;/#list&gt; include指令include指令的作用类似于JSP的包含指令,用于包含指定页 12345678910111213141516现在有hello.ftl、inc1.ftl与inc2.ftl 3个模板在inc1.ftl与inc2.ftl中的内容分别是:&lt;#assign username="刘德华"&gt;与&lt;#assign username="张学友"&gt;接着在hello.ftl模版中用include将inc1.ftl包含进来&lt;#include "/inc/inc1.ftl"&gt;$&#123;username&#125;此刻获取的结果是:刘德华接着我们在hello.ftl用include将inc1.ftl与inc2.ftl同时进行包含进来&lt;#include "/inc/inc1.ftl"&gt;&lt;#include "/inc/inc2.ftl"&gt;$&#123;username&#125;此刻获取的值是:张学友 总结：出现这种情况，在==两个模版中都分别存在变量名都相同的变量的时候，include包含进来，会进行覆盖==，include只时候将其公共的静态文件进行包含，而里面不涉及到内部函数以及变量声明之类的，当涉及到这种问题，我们就要用import进行导入import指令该指令用于导入FreeMarker模板中的所有变量,并将该变量放置在指定的Map对象中 1234567接着上面在hello.ftl用import指令将inc1.ftl与inc2.ftll模板文件中的所有变量,同时导入进来&lt;#import "/inc/inc1.ftl" as inc1&gt;&lt;#import "/inc/inc2.ftl" as inc2&gt;$&#123;inc1.username&#125; //刘德华$&#123;inc2.username&#125; //张学友 noparse指令noparse指令指定FreeMarker不处理该指定里包含的内容,该指令的语法格式如下 123456789101112&lt;#noparse&gt;...&lt;/#noparse&gt;看如下的例子: &lt;#noparse&gt; &lt;#list books as book&gt; &lt;tr&gt;&lt;td&gt;$&#123;book.name&#125;&lt;td&gt;作者:$&#123;book.author&#125; &lt;/#list&gt; &lt;/#noparse&gt; 输出如下: &lt;#list books as book&gt; &lt;tr&gt;&lt;td&gt;$&#123;book.name&#125;&lt;td&gt;作者:$&#123;book.author&#125; &lt;/#list&gt; assign指令assign指令在前面已经使用了多次,它用于为该模板页面创建或替换一个顶层变量,assign指令的用法有多种,包含创建或替换一个顶层变量, 或者创建或替换多个变量等,它的最简单的语法如下:&lt;#assign name=value [in namespacehash]&gt;,这个用法用于指定一个名为name的变量,该变量的值为value,此外,FreeMarker允许在使用 assign指令里增加in子句,in子句用于将创建的name变量放入namespacehash命名空间中assign指令还有如下用法: 1234567891011&lt;#assign name1=value1 name2=value2 ... nameN=valueN [in namespacehash]&gt;这个语法可以同时创建或替换多个顶层变量,此外,还有一种复杂的用法,如果需要创建或替换的变量值是一个复杂的表达式,则可以使用如下语法格式:&lt;#assign name [in namespacehash]&gt;capture this&lt;/#assign&gt;在这个语法中,是指将assign指令的内容赋值给name变量.如下例子:&lt;#assign x&gt; &lt;#list ["星期一", "星期二", "星期三", "星期四", "星期五", "星期六", "星期天"] as n&gt; $&#123;n&#125; &lt;/#list&gt; &lt;/#assign&gt; $&#123;x&#125; 上面的代码将产生如下输出:星期一 星期二 星期三 星期四 星期五 星期六 星期天 虽然assign指定了这种复杂变量值的用法,但是我们也不要滥用这种用法,如下例子:123&lt;#assign x&gt;Hello $&#123;user&#125;!&lt;/#assign&gt;以上代码改为如下写法更合适:&lt;#assign x="Hello $&#123;user&#125;!"&gt; setting指令该指令用于设置FreeMarker的运行环境,该指令的语法格式如下: 1&lt;#setting name=value&gt; 在这个格式中,name的取值范围包含如下几个： locale:该选项指定该模板所用的国家/语言选项 number_format:指定格式化输出数字的格式 boolean_format:指定两个布尔值的语法格式,默认值是true,false date_format,time_format,datetime_format:指定格式化输出日期的格式 time_zone:设置格式化输出日期时所使用的时区 自定义指令(macro指令)(宏)语法： 1234567&lt;#macro name param1 param2 ... paramN&gt;...&lt;#nested loopvar1, loopvar2, ..., loopvarN&gt;...&lt;#return&gt;...&lt;/#macro&gt; 用例： 12345678910//定义名为test的指令&lt;#macro test foo bar="Bar" baaz=-1&gt;这是自定义指令: $&#123;foo&#125;, $&#123;bar&#125;, $&#123;baaz&#125;&lt;/#macro&gt;//调用test指令&lt;@test foo="a" bar="b" baaz=5*5-2/&gt; //这是自定义指令: a, b, 23&lt;@test foo="a" bar="b"/&gt; //这是自定义指令: a, b, -1&lt;@test foo="a" baaz=5*5-2/&gt; //这是自定义指令: a, Bar, 23&lt;@test foo="a"/&gt; //这是自定义指令: a, Bar, -1 可以提前返回，比如&lt;#return/&gt; 但是不能&lt;#return 1&gt;， ==A macro cannot return a value====宏主要作用是拼接内容，把宏内部的字符串展示出来，return返回值没有意义==。 function指令(函数)用例： 123456789 &lt;#function buildPageUrl url pageNum data&gt; &lt;#assign pageUrl = "$&#123;url&#125;?pageNum=$&#123;pageNum&#125;&amp;pageSize=$&#123;data.pageSize&#125;"&gt; &lt;#return pageUrl/&gt;&lt;/#function&gt;$&#123;buildPageUrl(url2,page.pageNum+1,page)&#125; 与宏的调用方式不同，直接 ${buildPageUrl(url2,page.pageNum+1,page)}执行函数。==返回值才是最关键的结果，不是为了显示函数内部的字符串内容==。 参考文章（特别鸣谢）：https://blog.csdn.net/qq_34129814/article/details/76218863 https://segmentfault.com/a/1190000011768799 https://blog.csdn.net/fhx007/article/details/7902040/ https://www.cnblogs.com/qitian1/p/6463098.html 源码地址：github: https://github.com/542869246/myfreemarker码云: https://gitee.com/zyf542869246/myfreemarker Author:周宇峰Github:https://github.com/542869246码云:https://gitee.com/zyf542869246Time:2018/5/14 1:45:11]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>FreeMarker</tag>
        <tag>模板引擎</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[My New Project]]></title>
    <url>%2F2018%2F07%2F30%2FMy-New-Project%2F</url>
    <content type="text"><![CDATA[写啥好呢？]]></content>
      <categories>
        <category>其它</category>
      </categories>
      <tags>
        <tag>其它</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2018%2F07%2F29%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
      <categories>
        <category>其它</category>
      </categories>
      <tags>
        <tag>其它</tag>
      </tags>
  </entry>
  <entry>
    <title></title>
    <url>%2F404.html</url>
    <content type="text"><![CDATA[< !DOCTYPE html> 404页面]]></content>
  </entry>
  <entry>
    <title><![CDATA[About]]></title>
    <url>%2Fabout%2Findex.html</url>
    <content type="text"><![CDATA[🍀 基础信息 ☯微信☯：ZYF542869246 (网站底部扫描二维码) 🐧QQ🐧：542869246 📧E-mail📧：18851200889@163.com 😤GitHub😤: github.com/542869246 😔码云😔： gitee.com/zyf542869246 🚀坐标🚀：南京 行业：互联网💻 岗位：IT开发🐵 🍀 专业技能&amp;热衷方向 Java☕ SQL H5C3 Python 个人兴趣 吃🍗 喝🍹 拉💩 撒🙀 睡💤 博客简介 此Blog诞生纯属意外，完全是自己一时心血来潮的产物，若有瑕疵，请别bb，我会骂你。😎😎 更新日志 [2018/8/25] 更新 解决不蒜子插件不显示bug，查看解决方案 😎 [2018/10/08] 更新 原博客使用的Algolia站内搜索插件账户过期了，继续使用需要35美金。弃坑，使用自带的local_search。😒 [2018/9/20] 更新 修复TopX标签页title不显示的bug。😫 TopX标签增加排名序号，随机颜色。😖 [2018/9/18] 更新 hexo主题升级 v5-&gt;v6,升级一堆坑😭☠😠 提升性能，页面打开更快😎 增加TopX标签😎 全新的页面排版，大幅度修改页面样式😊 [2018/9/17] 更新 关闭博客右下角的萌妹子，十个看我Blog的有九个问我怎么弄的，喧宾夺主。😒 移除浏览器标签搞怪效果，理由同上。😒 新增Hexo-Next搭建个人博客（Hexo博客备份） Hexo-Next搭建个人博客（主题优化）更新11.好玩的写作样式 左侧站点概览新增我的知乎和网易云音乐入口，欢迎互关哦~😤 [2018/9/10] 更新 备份博客至Github，麻麻再也不怕我博客数据丢失啦。😏 [2018/8/31] 更新 修改主页文章列表显示方式，支持配图😍 优化文章内应用样式 优化文章标签样式 优化标签页 优化分类页 优化归档页 优化博客大部分字体 [2018/8/25] 更新 SEO优化，百度收录。☺ 优化主页顶部加载条样式 优化header部分背景颜色及选中效果 优化footer部分样式 回到顶部优化，显示百分比 [2018/8/15] 更新 添加各种功能😉 部分js使用CDN加速 footer添加时间 代码块复制 [2018/7/29] 更新 博客搭建😇]]></content>
  </entry>
  <entry>
    <title><![CDATA[标签]]></title>
    <url>%2Ftags%2Findex.html</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[分类]]></title>
    <url>%2Fcategories%2Findex.html</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[TopX]]></title>
    <url>%2Ftop%2Findex.html</url>
    <content type="text"><![CDATA[AV.initialize("4DkhRkq5CadWFisToA3zFrop-gzGzoHsz", "xihAA2ETGTdekJFloguCmmgq"); var time=0 var title="" var url="" var query = new AV.Query('Counter'); query.notEqualTo('id',0); query.descending('time'); query.limit(1000); query.find().then(function (todo) { for (var i=0;i]]></content>
  </entry>
</search>
